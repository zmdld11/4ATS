# model_comparison.py (æ–°æ–‡ä»¶)
import os
import torch
import matplotlib.pyplot as plt
import numpy as np
from config.config import Config
from src.model_manager import model_manager
from src.audio_preprocessor import AudioDataPreprocessor
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier

def compare_models():
    """æ¯”è¾ƒæ‰€æœ‰å¯ç”¨æ¨¡å‹çš„æ€§èƒ½"""
    print("=== æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ ===")
    
    # è·å–å¯ç”¨æ¨¡å‹
    available_models = model_manager.get_available_models()
    if not available_models:
        print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹")
        return
    
    print(f"æ‰¾åˆ°çš„æ¨¡å‹: {', '.join(available_models)}")
    
    # åŠ è½½æ•°æ®
    preprocessor = AudioDataPreprocessor(use_cache=True)
    train_loader, val_loader, test_loader, num_classes = preprocessor.create_data_loaders(
        use_cache=True, augment=False
    )
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    results = {}
    
    for model_type in available_models:
        print(f"\nè¯„ä¼° {model_type} æ¨¡å‹...")
        
        try:
            # åˆ›å»ºæ¨¡å‹
            if model_type == 'basic':
                model = create_improved_classifier((1, 128, 130), num_classes)
            elif model_type == 'advanced':
                model = create_advanced_classifier((1, 128, 130), num_classes)
            elif model_type == 'simplified':
                model = create_simplified_classifier((1, 128, 130), num_classes)
            else:
                continue
                
            model = model.to(device)
            
            # åŠ è½½æƒé‡
            load_success, _ = model_manager.load_model(model, model_type, device)
            if not load_success:
                continue
            
            # è¯„ä¼°æ¨¡å‹
            model.eval()
            correct = 0
            total = 0
            
            with torch.no_grad():
                for data, labels in test_loader:
                    data, labels = data.to(device), labels.to(device)
                    outputs = model(data)
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
            
            accuracy = correct / total
            results[model_type] = accuracy
            print(f"{model_type}æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")
            
        except Exception as e:
            print(f"è¯„ä¼°{model_type}æ¨¡å‹å¤±è´¥: {e}")
    
    # ç»˜åˆ¶æ¯”è¾ƒå›¾
    if results:
        plt.figure(figsize=(10, 6))
        models = list(results.keys())
        accuracies = [results[model] for model in models]
        
        bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'])
        plt.title('Model Performance Comparison', fontsize=14)
        plt.ylabel('Accuracy', fontsize=12)
        plt.ylim(0, 1.0)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, accuracy in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{accuracy:.4f}', ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        comparison_path = os.path.join(Config.OUTPUT_DIR, 'model_comparison.png')
        plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
        print(f"\næ¯”è¾ƒå›¾å·²ä¿å­˜åˆ°: {comparison_path}")
        plt.show()
        
        # è¾“å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(results, key=results.get)
        print(f"\nğŸ‰ æœ€ä½³æ¨¡å‹: {best_model} (å‡†ç¡®ç‡: {results[best_model]:.4f})")

if __name__ == "__main__":
    compare_models()