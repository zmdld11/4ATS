# audio_preprocessor.py (æ·»åŠ æ•°æ®å¢å¼º - ä¸­æ–‡æ³¨é‡Š)
import os
import librosa
import numpy as np
import pickle
import hashlib
import torch
import random
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from config.config import Config

class AudioDataset(Dataset):
    """å¸¦æœ‰æ•°æ®å¢å¼ºçš„PyTorchéŸ³é¢‘æ•°æ®é›†"""
    
    def __init__(self, features, labels, transform=None, augment=False):
        self.features = features
        self.labels = labels
        self.transform = transform
        self.augment = augment
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        feature = self.features[idx].copy()  # é‡è¦ï¼šåˆ›å»ºå‰¯æœ¬
        label = self.labels[idx]
        
        # è®­ç»ƒæœŸé—´çš„æ•°æ®å¢å¼º
        if self.augment:
            feature = self._apply_augmentation(feature)
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        feature = torch.FloatTensor(feature).unsqueeze(0)  # æ·»åŠ é€šé“ç»´åº¦
        label = torch.LongTensor([label])[0]
        
        if self.transform:
            feature = self.transform(feature)
            
        return feature, label
    
    def _apply_augmentation(self, feature):
        """å¯¹éŸ³é¢‘ç‰¹å¾åº”ç”¨æ•°æ®å¢å¼º"""
        # æ—¶é—´æ©ç ï¼ˆç±»ä¼¼äºSpecAugmentï¼‰
        if random.random() > 0.5:
            max_mask_width = feature.shape[1] // 4
            mask_width = random.randint(1, max_mask_width)
            mask_start = random.randint(0, feature.shape[1] - mask_width)
            feature[:, mask_start:mask_start + mask_width] = 0
        
        # é¢‘ç‡æ©ç 
        if random.random() > 0.5:
            max_mask_height = feature.shape[0] // 8
            mask_height = random.randint(1, max_mask_height)
            mask_start = random.randint(0, feature.shape[0] - mask_height)
            feature[mask_start:mask_start + mask_height, :] = 0
        
        # æ·»åŠ å°å™ªå£°
        if random.random() > 0.7:
            noise = np.random.normal(0, 0.01, feature.shape)
            feature = feature + noise
        
        return feature

class AudioDataPreprocessor:
    """å¸¦æœ‰å¢å¼ºåŠŸèƒ½å’Œæ•°æ®å¢å¼ºçš„éŸ³é¢‘æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, target_sr=Config.TARGET_SAMPLE_RATE, 
                 duration=Config.AUDIO_DURATION,
                 use_cache=True):
        self.target_sr = target_sr
        self.duration = duration
        self.label_encoder = LabelEncoder()
        self.use_cache = use_cache
        self.cache_dir = os.path.join(Config.DATA_DIR, "preprocessed_cache")
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    def extract_enhanced_features(self, audio_path):
        """æå–å¸¦æœ‰å¤šç§è¡¨ç¤ºçš„å¢å¼ºéŸ³é¢‘ç‰¹å¾"""
        try:
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(audio_path, sr=self.target_sr)
            
            # ç¡®ä¿éŸ³é¢‘é•¿åº¦ä¸€è‡´
            y = librosa.util.fix_length(y, size=self.target_sr * self.duration)
            
            # æå–å¤šç§ç‰¹å¾
            # 1. Melé¢‘è°±å›¾ï¼ˆä¸»è¦ç‰¹å¾ï¼‰
            mel_spec = librosa.feature.melspectrogram(
                y=y, sr=sr, n_mels=Config.N_MELS, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # 2. MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
            
            # 3. Chromaç‰¹å¾
            chroma = librosa.feature.chroma_stft(y=y, sr=sr)
            
            # ç»„åˆç‰¹å¾ï¼ˆå¯ä»¥é€‰æ‹©ä½¿ç”¨å“ªäº›ï¼‰
            # é€‰é¡¹1ï¼šä»…ä½¿ç”¨melé¢‘è°±å›¾ï¼ˆä¸å½“å‰æ¶æ„å…¼å®¹ï¼‰
            combined_features = log_mel
            
            # é€‰é¡¹2ï¼šå †å å¤šä¸ªç‰¹å¾ï¼ˆéœ€è¦æ›´æ”¹æ¨¡å‹æ¶æ„ï¼‰
            # combined_features = np.vstack([log_mel, mfcc, chroma])
            
            # æ ‡å‡†åŒ–
            combined_features = (combined_features - np.mean(combined_features)) / (np.std(combined_features) + 1e-8)
            
            return combined_features
            
        except Exception as e:
            print(f"å¤„ç†éŸ³é¢‘ {audio_path} æ—¶å‡ºé”™: {e}")
            return None
    
    def create_data_loaders(self, batch_size=Config.BATCH_SIZE, use_cache=True, augment=True):
        """åˆ›å»ºå¸¦æœ‰å¯é€‰æ•°æ®å¢å¼ºçš„PyTorchæ•°æ®åŠ è½½å™¨"""
        # åŠ è½½éŸ³é¢‘æ ·æœ¬
        audio_paths, labels = self.load_audio_samples(Config.DATA_DIR)
        
        # æå–ç‰¹å¾ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        X, y = self.prepare_dataset(audio_paths, labels, use_cache=use_cache)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=Config.VALIDATION_SPLIT, 
            random_state=42, stratify=y
        )
        
        # è¿›ä¸€æ­¥åˆ’åˆ†è®­ç»ƒé›†ç”¨äºéªŒè¯
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, 
            random_state=42, stratify=y_train
        )
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = AudioDataset(X_train, y_train, augment=augment)
        val_dataset = AudioDataset(X_val, y_val, augment=False)
        test_dataset = AudioDataset(X_test, y_test, augment=False)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
        
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_dataset)}")
        
        return train_loader, val_loader, test_loader, len(self.label_encoder.classes_)

    def _get_cache_filename(self, data_dir):
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶å"""
        config_str = f"{data_dir}_{self.target_sr}_{self.duration}_{Config.N_MELS}"
        hash_obj = hashlib.md5(config_str.encode())
        return os.path.join(self.cache_dir, f"preprocessed_{hash_obj.hexdigest()}.pkl")
    
    def _save_to_cache(self, cache_file, X, y, label_encoder):
        """ä¿å­˜é¢„å¤„ç†ç»“æœåˆ°ç¼“å­˜"""
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'features': X,
                    'labels': y,
                    'label_encoder': label_encoder,
                    'config': {
                        'target_sr': self.target_sr,
                        'duration': self.duration,
                        'n_mels': Config.N_MELS
                    }
                }, f)
            print(f"âœ… é¢„å¤„ç†æ•°æ®å·²ç¼“å­˜åˆ°: {cache_file}")
            return True
        except Exception as e:
            print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _load_from_cache(self, cache_file):
        """ä»ç¼“å­˜åŠ è½½é¢„å¤„ç†ç»“æœ"""
        try:
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # éªŒè¯é…ç½®æ˜¯å¦åŒ¹é…
            config = cache_data['config']
            if (config['target_sr'] == self.target_sr and 
                config['duration'] == self.duration and 
                config['n_mels'] == Config.N_MELS):
                
                print("âœ… ä»ç¼“å­˜åŠ è½½é¢„å¤„ç†æ•°æ®")
                return cache_data['features'], cache_data['labels'], cache_data['label_encoder']
            else:
                print("âš ï¸ ç¼“å­˜é…ç½®ä¸åŒ¹é…ï¼Œé‡æ–°é¢„å¤„ç†")
                return None, None, None
                
        except Exception as e:
            print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None, None, None
    
    def load_audio_samples(self, data_dir):
        """åŠ è½½éŸ³é¢‘æ ·æœ¬å’Œæ ‡ç­¾"""
        samples = []
        labels = []
        
        dataset_path = os.path.join(data_dir, Config.DATASET_NAME)
        print(f"ä» {dataset_path} åŠ è½½æ•°æ®...")
        
        # éå†æ¯ä¸ªä¹å™¨æ–‡ä»¶å¤¹
        for instrument in os.listdir(dataset_path):
            instrument_path = os.path.join(dataset_path, instrument)
            if os.path.isdir(instrument_path):
                for audio_file in os.listdir(instrument_path):
                    if audio_file.endswith('.wav'):
                        audio_path = os.path.join(instrument_path, audio_file)
                        samples.append(audio_path)
                        labels.append(instrument)
        
        print(f"æ‰¾åˆ° {len(samples)} ä¸ªéŸ³é¢‘æ ·æœ¬")
        print(f"ä¹å™¨ç±»åˆ«: {set(labels)}")
        
        return samples, labels
    
    def prepare_dataset(self, audio_paths, labels, use_cache=True):
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›† - å¸¦ç¼“å­˜åŠŸèƒ½"""
        cache_file = self._get_cache_filename(Config.DATA_DIR)
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache and os.path.exists(cache_file):
            X, y, label_encoder = self._load_from_cache(cache_file)
            if X is not None and y is not None:
                self.label_encoder = label_encoder
                return X, y
        
        # ç¼“å­˜ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨ï¼Œé‡æ–°å¤„ç†
        print("ğŸ”„ é¢„å¤„ç†éŸ³é¢‘æ•°æ®ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
        features = []
        valid_labels = []
        
        for i, (path, label) in enumerate(zip(audio_paths, labels)):
            if i % 100 == 0:  # æ¯100ä¸ªæ ·æœ¬æ˜¾ç¤ºè¿›åº¦
                print(f"å·²å¤„ç† {i}/{len(audio_paths)} ä¸ªæ ·æœ¬ ({i/len(audio_paths)*100:.1f}%)")
                
            feature = self.extract_enhanced_features(path)
            if feature is not None:
                features.append(feature)
                valid_labels.append(label)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        X = np.array(features)
        y = self.label_encoder.fit_transform(valid_labels)
        
        print(f"æœ€ç»ˆæ•°æ®é›†å½¢çŠ¶: {X.shape}")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if use_cache:
            self._save_to_cache(cache_file, X, y, self.label_encoder)
        
        return X, y