import os
import torch
import librosa
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import joblib

class TimeSeriesAnalyzer:
    """æ—¶é—´åºåˆ—ä¹å™¨åˆ†æå™¨"""
    
    def __init__(self, model, label_encoder, device):
        self.model = model
        self.label_encoder = label_encoder
        self.device = device
        self.model.eval()
    
    def analyze_audio_timeline(self, audio_path, window_size=3.0, hop_size=1.0, threshold=0.3):
        """
        åˆ†æéŸ³é¢‘æ—¶é—´çº¿ï¼Œæ£€æµ‹ä¹å™¨å‡ºç°çš„æ—¶é—´æ®µ
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            window_size: åˆ†æçª—å£å¤§å°ï¼ˆç§’ï¼‰
            hop_size: çª—å£è·³è·ƒå¤§å°ï¼ˆç§’ï¼‰
            threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        print(f"å¼€å§‹åˆ†æéŸ³é¢‘æ—¶é—´çº¿: {audio_path}")
        
        # åŠ è½½å®Œæ•´éŸ³é¢‘
        y, sr = librosa.load(audio_path, sr=22050)
        duration = len(y) / sr
        print(f"éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’, é‡‡æ ·ç‡: {sr}Hz")
        
        # è®¡ç®—çª—å£å‚æ•°
        window_samples = int(window_size * sr)
        hop_samples = int(hop_size * sr)
        
        # æ»‘åŠ¨çª—å£åˆ†æ
        predictions = []
        timestamps = []
        
        for start in range(0, len(y) - window_samples + 1, hop_samples):
            # æå–å½“å‰çª—å£
            end = start + window_samples
            window_audio = y[start:end]
            timestamp = start / sr  # å½“å‰çª—å£å¼€å§‹æ—¶é—´
            
            # æå–ç‰¹å¾
            features = self._extract_features(window_audio, sr)
            if features is not None:
                # é¢„æµ‹
                prediction = self._predict_single_window(features)
                predictions.append(prediction)
                timestamps.append(timestamp)
            
            # æ˜¾ç¤ºè¿›åº¦
            if len(predictions) % 10 == 0:
                progress = min(100, (end / len(y)) * 100)
                print(f"åˆ†æè¿›åº¦: {progress:.1f}%")
        
        print("æ—¶é—´åºåˆ—åˆ†æå®Œæˆ!")
        return self._process_timeline_results(predictions, timestamps, window_size, threshold)
    
    def _extract_features(self, audio, sr):
        """æå–éŸ³é¢‘ç‰¹å¾"""
        try:
            # æå–Melé¢‘è°±å›¾
            mel_spec = librosa.feature.melspectrogram(
                y=audio, sr=sr, n_mels=128, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # æ ‡å‡†åŒ–
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            # ç¡®ä¿ç‰¹å¾å°ºå¯¸ä¸€è‡´
            target_frames = 130  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
            if log_mel.shape[1] < target_frames:
                # å¡«å……
                pad_width = target_frames - log_mel.shape[1]
                log_mel = np.pad(log_mel, ((0, 0), (0, pad_width)), mode='constant')
            elif log_mel.shape[1] > target_frames:
                # æˆªæ–­
                log_mel = log_mel[:, :target_frames]
            
            return log_mel
            
        except Exception as e:
            print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
            return None
    
    def _predict_single_window(self, features):
        """é¢„æµ‹å•ä¸ªçª—å£"""
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)  # (1, 1, 128, 130)
        input_tensor = input_tensor.to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
        
        # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        probs = probabilities.cpu().numpy()[0]
        results = {}
        
        for idx, prob in enumerate(probs):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            results[instrument] = float(prob)
        
        return results
    
    def _process_timeline_results(self, predictions, timestamps, window_size, threshold):
        """å¤„ç†æ—¶é—´çº¿ç»“æœï¼Œåˆå¹¶è¿ç»­çš„æ—¶é—´æ®µ"""
        timeline = {}
        
        # ä¸ºæ¯ä¸ªä¹å™¨åˆå§‹åŒ–æ—¶é—´çº¿
        for instrument in self.label_encoder.classes_:
            timeline[instrument] = {
                'segments': [],
                'total_duration': 0.0,
                'max_confidence': 0.0,
                'average_confidence': 0.0
            }
        
        # åˆ†ææ¯ä¸ªæ—¶é—´ç‚¹çš„é¢„æµ‹
        for i, (timestamp, pred_dict) in enumerate(zip(timestamps, predictions)):
            # æ‰¾åˆ°å½“å‰çª—å£æœ€å¯èƒ½çš„ä¹å™¨
            if pred_dict:
                best_instrument = max(pred_dict.items(), key=lambda x: x[1])
                instrument, confidence = best_instrument
                
                # åªè®°å½•ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼çš„é¢„æµ‹
                if confidence >= threshold:
                    timeline[instrument]['segments'].append({
                        'start': timestamp,
                        'end': timestamp + window_size,
                        'confidence': confidence
                    })
        
        # åˆå¹¶è¿ç»­çš„æ—¶é—´æ®µå¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        for instrument in timeline.keys():
            segments = timeline[instrument]['segments']
            if segments:
                # æŒ‰å¼€å§‹æ—¶é—´æ’åº
                segments.sort(key=lambda x: x['start'])
                
                # åˆå¹¶è¿ç»­çš„æ—¶é—´æ®µ
                merged_segments = []
                current_segment = segments[0]
                
                for segment in segments[1:]:
                    if segment['start'] <= current_segment['end'] + 0.5:  # å…è®¸0.5ç§’é—´éš™
                        # æ—¶é—´æ®µè¿ç»­ï¼Œåˆå¹¶
                        current_segment['end'] = max(current_segment['end'], segment['end'])
                        current_segment['confidence'] = max(current_segment['confidence'], segment['confidence'])
                    else:
                        # æ—¶é—´æ®µä¸è¿ç»­ï¼Œä¿å­˜å½“å‰æ®µå¹¶å¼€å§‹æ–°æ®µ
                        merged_segments.append(current_segment)
                        current_segment = segment
                
                merged_segments.append(current_segment)
                
                # æ›´æ–°timeline
                timeline[instrument]['segments'] = merged_segments
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                total_duration = sum(seg['end'] - seg['start'] for seg in merged_segments)
                confidences = [seg['confidence'] for seg in merged_segments]
                
                timeline[instrument]['total_duration'] = total_duration
                timeline[instrument]['max_confidence'] = max(confidences) if confidences else 0
                timeline[instrument]['average_confidence'] = np.mean(confidences) if confidences else 0
        
        return timeline
    
    def visualize_timeline(self, timeline, audio_duration, save_path=None):
        """å¯è§†åŒ–æ—¶é—´çº¿ç»“æœ"""
        # ä¹å™¨åç§°æ˜ å°„å­—å…¸
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        fig, ax = plt.subplots(figsize=(14, 8))
        
        # ä½¿ç”¨è‹±æ–‡åç§°
        english_instruments = [instrument_names.get(instr, instr) for instr in timeline.keys()]
        colors = plt.cm.Set3(np.linspace(0, 1, len(english_instruments)))
        
        # ä¸ºæ¯ä¸ªä¹å™¨åˆ›å»ºæ—¶é—´çº¿
        for i, (instrument, data) in enumerate(timeline.items()):
            english_name = instrument_names.get(instrument, instrument)
            segments = data['segments']
            if segments:
                for segment in segments:
                    # ç»˜åˆ¶æ—¶é—´æ®µçŸ©å½¢
                    rect = Rectangle(
                        (segment['start'], i - 0.4),
                        segment['end'] - segment['start'],
                        0.8,
                        facecolor=colors[i],
                        alpha=0.7,
                        edgecolor='black',
                        linewidth=1
                    )
                    ax.add_patch(rect)
                    
                    # æ·»åŠ ç½®ä¿¡åº¦æ–‡æœ¬
                    if segment['end'] - segment['start'] > 2:  # åªåœ¨ä¸å°çš„æ®µä¸Šæ·»åŠ æ–‡æœ¬
                        ax.text(
                            (segment['start'] + segment['end']) / 2,
                            i,
                            f'{segment["confidence"]:.2f}',
                            ha='center',
                            va='center',
                            fontsize=8,
                            fontweight='bold'
                        )
        
        # è®¾ç½®åæ ‡è½´
        ax.set_xlim(0, audio_duration)
        ax.set_ylim(-0.5, len(english_instruments) - 0.5)
        ax.set_xlabel('Time (seconds)')
        ax.set_ylabel('Instrument')
        ax.set_title('Instrument Timeline Analysis')
        
        # è®¾ç½®yè½´åˆ»åº¦ï¼ˆä½¿ç”¨è‹±æ–‡åç§°ï¼‰
        ax.set_yticks(range(len(english_instruments)))
        ax.set_yticklabels(english_instruments)
        
        # æ·»åŠ ç½‘æ ¼
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ å›¾ä¾‹
        legend_elements = []
        for i, (instrument, data) in enumerate(timeline.items()):
            english_name = instrument_names.get(instrument, instrument)
            if data['segments']:
                legend_elements.append(
                    plt.Rectangle((0, 0), 1, 1, facecolor=colors[i], alpha=0.7, 
                                label=f"{english_name} ({data['total_duration']:.1f}s)")
                )
        
        if legend_elements:
            ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.15, 1))
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Timeline chart saved: {save_path}")
        
        plt.show()
        
        return fig

    def generate_report(self, timeline, audio_duration):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        # ä¹å™¨åç§°æ˜ å°„å­—å…¸
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        print("\n" + "="*60)
        print("               Instrument Timeline Analysis Report")
        print("="*60)
        
        # ç»Ÿè®¡æ´»è·ƒä¹å™¨
        active_instruments = []
        for instrument, data in timeline.items():
            if data['segments']:
                english_name = instrument_names.get(instrument, instrument)
                active_instruments.append((english_name, data['total_duration'], instrument))
        
        # æŒ‰æŒç»­æ—¶é—´æ’åº
        active_instruments.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ“Š Active Instruments Statistics ({len(active_instruments)} instruments):")
        print("-" * 60)
        
        for english_name, duration, original_name in active_instruments:
            percentage = (duration / audio_duration) * 100
            max_conf = timeline[original_name]['max_confidence']
            avg_conf = timeline[original_name]['average_confidence']
            segment_count = len(timeline[original_name]['segments'])
            
            print(f"ğŸµ {english_name:15s} | {duration:6.1f}s ({percentage:5.1f}%) | "
                f"Max Confidence: {max_conf:.3f} | Segments: {segment_count}")
        
        print(f"\nğŸ“ˆ Detailed Time Segments:")
        print("-" * 50)
        
        for english_name, duration, original_name in active_instruments:
            segments = timeline[original_name]['segments']
            
            print(f"\n{english_name}:")
            for i, segment in enumerate(segments, 1):
                print(f"  Segment {i}: {segment['start']:6.1f}s - {segment['end']:6.1f}s "
                    f"(Confidence: {segment['confidence']:.3f})")
        
        return active_instruments