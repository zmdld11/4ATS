import os
import sys
import torch
import joblib
import librosa
import numpy as np
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.config import Config
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier
from src.time_series_analyzer import TimeSeriesAnalyzer

def load_model_with_fix(model_type, device):
    """åŠ è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹å¹¶ä¿®å¤è¾“å…¥å½¢çŠ¶é—®é¢˜"""
    # åŠ è½½æ ‡ç­¾ç¼–ç å™¨
    label_encoder_path = os.path.join(project_root, "model", f"model_{model_type}_label_encoder.pkl")
    if not os.path.exists(label_encoder_path):
        print(f"é”™è¯¯: æ ‡ç­¾ç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨ - {label_encoder_path}")
        return None, None
    
    label_encoder = joblib.load(label_encoder_path)
    
    # åŠ è½½æ¨¡å‹
    model_path = os.path.join(project_root, "model", f"model_{model_type}.pth")
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ - {model_path}")
        return None, None
    
    checkpoint = torch.load(model_path, map_location=device)
    
    # ä¿®å¤è¾“å…¥å½¢çŠ¶
    input_shape = (1, 128, 130)  # (channels, height, width)
    num_classes = len(label_encoder.classes_)
    
    print(f"ä½¿ç”¨è¾“å…¥å½¢çŠ¶: {input_shape}")
    print(f"ç±»åˆ«æ•°é‡: {num_classes}")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºå¯¹åº”çš„æ¨¡å‹
    if model_type == 'basic':
        model = create_improved_classifier(input_shape, num_classes)
    elif model_type == 'advanced':
        model = create_advanced_classifier(input_shape, num_classes)
    elif model_type == 'simplified':
        model = create_simplified_classifier(input_shape, num_classes)
    else:
        print(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
        return None, None
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    
    return model, label_encoder

def ensure_feature_shape(features, target_shape=(128, 130)):
    """ç¡®ä¿ç‰¹å¾å½¢çŠ¶ä¸€è‡´"""
    if features.shape != target_shape:
        # è°ƒæ•´åˆ°ç›®æ ‡å½¢çŠ¶
        if features.shape[1] < target_shape[1]:
            # å¡«å……
            pad_width = target_shape[1] - features.shape[1]
            features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')
        elif features.shape[1] > target_shape[1]:
            # æˆªæ–­
            features = features[:, :target_shape[1]]
    
    return features

class FixedTimeSeriesAnalyzer(TimeSeriesAnalyzer):
    """ä¿®å¤çš„æ—¶é—´åºåˆ—åˆ†æå™¨"""
    
    def __init__(self, model, label_encoder, device, model_type):
        super().__init__(model, label_encoder, device)
        self.model_type = model_type
    
    def _extract_features(self, audio, sr):
        """æå–éŸ³é¢‘ç‰¹å¾"""
        try:
            # æå–Melé¢‘è°±å›¾
            mel_spec = librosa.feature.melspectrogram(
                y=audio, sr=sr, n_mels=128, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # æ ‡å‡†åŒ–
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            # ç¡®ä¿ç‰¹å¾å°ºå¯¸ä¸€è‡´
            log_mel = ensure_feature_shape(log_mel, (128, 130))
            
            return log_mel
            
        except Exception as e:
            print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
            return None
    
    def _predict_single_window(self, features):
        """é¢„æµ‹å•ä¸ªçª—å£"""
        # ç¡®ä¿ç‰¹å¾å½¢çŠ¶æ­£ç¡®
        features = ensure_feature_shape(features, (128, 130))
        
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼: (1, 1, 128, 130)
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)
        input_tensor = input_tensor.to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
        
        # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        probs = probabilities.cpu().numpy()[0]
        results = {}
        
        for idx, prob in enumerate(probs):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            results[instrument] = float(prob)
        
        return results
    
    def generate_report(self, timeline, audio_duration):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        from src.instrument_mapper import InstrumentMapper
        
        # ä¹å™¨åç§°æ˜ å°„å­—å…¸
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        print(f"\n{'='*60}")
        print(f"      {self.model_type}æ¨¡å‹ - Instrument Timeline Analysis Report")
        print(f"{'='*60}")
        
        # ç»Ÿè®¡æ´»è·ƒä¹å™¨
        active_instruments = []
        for instrument, data in timeline.items():
            if data['segments']:
                english_name = instrument_names.get(instrument, instrument)
                active_instruments.append((english_name, data['total_duration'], instrument))
        
        # æŒ‰æŒç»­æ—¶é—´æ’åº
        active_instruments.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ“Š Active Instruments Statistics ({len(active_instruments)} instruments):")
        print("-" * 60)
        
        for english_name, duration, original_name in active_instruments:
            percentage = (duration / audio_duration) * 100
            max_conf = timeline[original_name]['max_confidence']
            avg_conf = timeline[original_name]['average_confidence']
            segment_count = len(timeline[original_name]['segments'])
            
            print(f"ğŸµ {english_name:15s} | {duration:6.1f}s ({percentage:5.1f}%) | "
                f"Max Confidence: {max_conf:.3f} | Segments: {segment_count}")
        
        return active_instruments

def test_timeline_with_model(model_type, audio_path):
    """ä½¿ç”¨æŒ‡å®šæ¨¡å‹æµ‹è¯•æ—¶é—´çº¿åˆ†æ"""
    print(f"\n{'='*50}")
    print(f"ä½¿ç”¨ {model_type} æ¨¡å‹è¿›è¡Œæ—¶é—´çº¿åˆ†æ")
    print(f"{'='*50}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åŠ è½½æ¨¡å‹å’Œæ ‡ç­¾ç¼–ç å™¨
    model, label_encoder = load_model_with_fix(model_type, device)
    if model is None or label_encoder is None:
        return None
    
    print(f"ä¹å™¨ç±»åˆ«: {list(label_encoder.classes_)}")
    print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
    
    # åˆ›å»ºæ—¶é—´åºåˆ—åˆ†æå™¨
    analyzer = FixedTimeSeriesAnalyzer(model, label_encoder, device, model_type)
    
    # åˆ†æéŸ³é¢‘æ—¶é—´çº¿
    print("\nå¼€å§‹æ—¶é—´çº¿åˆ†æ...")
    timeline = analyzer.analyze_audio_timeline(
        audio_path, 
        window_size=1.5,      # å‡å°çª—å£å¤§å°ï¼Œæé«˜æ—¶é—´åˆ†è¾¨ç‡
        hop_size=0.3,         # å‡å°è·³è·ƒæ­¥é•¿ï¼Œå¢åŠ é‡‡æ ·å¯†åº¦
        threshold=0.15         # é™ä½é˜ˆå€¼ï¼Œæ•æ‰æ›´å¤šå¼±ä¿¡å·
    )
    
    # è·å–éŸ³é¢‘æ—¶é•¿ç”¨äºå¯è§†åŒ–
    y, sr = librosa.load(audio_path, sr=22050)
    audio_duration = len(y) / sr
    
    # ç”ŸæˆæŠ¥å‘Š
    active_instruments = analyzer.generate_report(timeline, audio_duration)
    
    # å¯è§†åŒ–æ—¶é—´çº¿
    output_path = os.path.join(Config.OUTPUT_DIR, f"instrument_timeline_{model_type}.png")
    analyzer.visualize_timeline(timeline, audio_duration, output_path)
    
    print(f"\nâœ… {model_type}æ¨¡å‹åˆ†æå®Œæˆ!")
    print(f"ğŸ“Š å‘ç°äº† {len(active_instruments)} ç§æ´»è·ƒä¹å™¨")
    print(f"ğŸ“ æ—¶é—´çº¿å›¾å·²ä¿å­˜: {output_path}")
    
    return active_instruments

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ¨¡å‹æ—¶é—´çº¿åˆ†æ')
    parser.add_argument('--model-type', type=str, default='all', 
                       choices=['basic', 'simplified', 'advanced', 'all'],
                       help='è¦æµ‹è¯•çš„æ¨¡å‹ç±»å‹')
    parser.add_argument('--audio-path', type=str, 
                       default=os.path.join(project_root, "music", "3.flac"),
                       help='æµ‹è¯•éŸ³é¢‘è·¯å¾„')
    
    args = parser.parse_args()
    
    print("=== AIéŸ³é¢‘åˆ†æä¸è‡ªåŠ¨æ‰’è°±ç³»ç»Ÿ - å¤šæ¨¡å‹æ—¶é—´çº¿åˆ†æ ===")
    
    # 1. åˆå§‹åŒ–é…ç½®
    Config.create_directories()
    
    # 2. æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.audio_path):
        print(f"é”™è¯¯: æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ - {args.audio_path}")
        print("è¯·ç¡®ä¿éŸ³é¢‘æ–‡ä»¶å­˜åœ¨")
        return
    
    if args.model_type == 'all':
        # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
        model_types = ['basic', 'simplified', 'advanced']
        all_results = {}
        
        for model_type in model_types:
            result = test_timeline_with_model(model_type, args.audio_path)
            if result:
                all_results[model_type] = result
        
        # è¾“å‡ºæ¯”è¾ƒç»“æœ
        print(f"\n{'='*60}")
        print("æ—¶é—´çº¿åˆ†ææ¨¡å‹æ¯”è¾ƒç»“æœ")
        print(f"{'='*60}")
        
        for model_type, instruments in all_results.items():
            print(f"\n{model_type:>10}æ¨¡å‹: æ£€æµ‹åˆ° {len(instruments)} ç§ä¹å™¨")
            for i, (name, duration, _) in enumerate(instruments[:3]):  # æ˜¾ç¤ºå‰3ç§
                print(f"            {i+1}. {name}: {duration:.1f}s")
    
    else:
        # æµ‹è¯•å•ä¸ªæ¨¡å‹
        test_timeline_with_model(args.model_type, args.audio_path)

if __name__ == "__main__":
    main()