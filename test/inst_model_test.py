import os
import sys
import torch
import librosa
import numpy as np
import joblib
import matplotlib.pyplot as plt
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.config import Config
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier
from src.instrument_mapper import InstrumentMapper

class MultiModelTester:
    """å¤šæ¨¡å‹æµ‹è¯•ç±»"""
    
    def __init__(self, model_type='simplified'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_type = model_type
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æµ‹è¯•æ¨¡å‹: {model_type}")
        
        # åŠ è½½æ ‡ç­¾ç¼–ç å™¨
        label_encoder_path = os.path.join(project_root, "model", f"model_{model_type}_label_encoder.pkl")
        if not os.path.exists(label_encoder_path):
            print(f"é”™è¯¯: æ ‡ç­¾ç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨ - {label_encoder_path}")
            return
        
        self.label_encoder = joblib.load(label_encoder_path)
        print(f"æ ‡ç­¾ç¼–ç å™¨åŠ è½½æˆåŠŸï¼Œç±»åˆ«: {list(self.label_encoder.classes_)}")
        
        # åŠ è½½æ¨¡å‹
        self.model = self.load_model(model_type)
        if self.model:
            self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            print(f"{model_type}æ¨¡å‹åŠ è½½æˆåŠŸ!")
    
    def load_model(self, model_type):
        """åŠ è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
        model_path = os.path.join(project_root, "model", f"model_{model_type}.pth")
        
        if not os.path.exists(model_path):
            print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ - {model_path}")
            return None
        
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # è·å–æ¨¡å‹å‚æ•°
            input_shape = checkpoint.get('input_shape', (1, 128, 130))
            num_classes = checkpoint.get('num_classes', len(self.label_encoder.classes_))
            
            print(f"æ¨¡å‹å‚æ•° - è¾“å…¥å½¢çŠ¶: {input_shape}, ç±»åˆ«æ•°: {num_classes}")
            
            # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºå¯¹åº”çš„æ¨¡å‹å®ä¾‹
            if model_type == 'basic':
                model = create_improved_classifier(input_shape, num_classes)
            elif model_type == 'advanced':
                model = create_advanced_classifier(input_shape, num_classes)
            elif model_type == 'simplified':
                model = create_simplified_classifier(input_shape, num_classes)
            else:
                print(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
                return None
            
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(self.device)
            
            return model
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None
    
    def preprocess_audio(self, audio_path):
        """é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶"""
        try:
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(audio_path, sr=Config.TARGET_SAMPLE_RATE)
            
            # ç¡®ä¿éŸ³é¢‘é•¿åº¦ä¸€è‡´
            y = librosa.util.fix_length(y, size=Config.TARGET_SAMPLE_RATE * Config.AUDIO_DURATION)
            
            # æå–Melé¢‘è°±å›¾
            mel_spec = librosa.feature.melspectrogram(
                y=y, sr=sr, n_mels=Config.N_MELS, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # æ ‡å‡†åŒ–
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            return log_mel
            
        except Exception as e:
            print(f"å¤„ç†éŸ³é¢‘ {audio_path} æ—¶å‡ºé”™: {e}")
            return None
    
    def predict_single_audio(self, audio_path):
        """é¢„æµ‹å•ä¸ªéŸ³é¢‘æ–‡ä»¶çš„ä¹å™¨ç§ç±»"""
        if self.model is None:
            print("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
            return None, None
            
        print(f"\nåˆ†æéŸ³é¢‘æ–‡ä»¶: {audio_path}")
        
        # é¢„å¤„ç†éŸ³é¢‘
        features = self.preprocess_audio(audio_path)
        if features is None:
            return None, None
        
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)  # (1, 1, 128, 130)
        input_tensor = input_tensor.to(self.device)
        print(f"è¾“å…¥å¼ é‡å½¢çŠ¶: {input_tensor.shape}")
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            top_probs, top_indices = torch.topk(probabilities, k=3)  # è·å–å‰3ä¸ªé¢„æµ‹
        
        # è½¬æ¢ä¸ºnumpy
        top_probs = top_probs.cpu().numpy()[0]
        top_indices = top_indices.cpu().numpy()[0]
        
        # è§£ç é¢„æµ‹ç»“æœ
        predictions = []
        for i, (idx, prob) in enumerate(zip(top_indices, top_probs)):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            predictions.append({
                'rank': i+1,
                'instrument': instrument,
                'probability': prob
            })
        
        return predictions, features
    
    def visualize_prediction(self, predictions, features, audio_name):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        # ä¹å™¨åç§°æ˜ å°„å­—å…¸
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        plt.figure(figsize=(12, 5))
        
        # ç»˜åˆ¶Melé¢‘è°±å›¾
        plt.subplot(1, 2, 1)
        librosa.display.specshow(features, sr=Config.TARGET_SAMPLE_RATE, 
                                hop_length=512, x_axis='time', y_axis='mel')
        plt.colorbar(format='%+2.0f dB')
        plt.title(f'Mel Spectrogram - {audio_name}\n({self.model_type}æ¨¡å‹)')
        
        # ç»˜åˆ¶é¢„æµ‹æ¦‚ç‡
        plt.subplot(1, 2, 2)
        # å°†ç¼©å†™è½¬æ¢ä¸ºè‹±æ–‡åç§°
        instruments = [instrument_names.get(p['instrument'], p['instrument']) for p in predictions]
        probabilities = [p['probability'] for p in predictions]
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(predictions)))
        bars = plt.bar(instruments, probabilities, color=colors)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, prob in zip(bars, probabilities):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{prob:.3f}', ha='center', va='bottom')
        
        plt.title(f'{self.model_type}æ¨¡å‹é¢„æµ‹ç»“æœ')
        plt.xlabel('Instrument')
        plt.ylabel('Probability')
        plt.ylim(0, 1)
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # ä¿å­˜ç»“æœ
        output_path = os.path.join(Config.OUTPUT_DIR, f"prediction_{audio_name}_{self.model_type}.png")
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"é¢„æµ‹ç»“æœå›¾å·²ä¿å­˜: {output_path}")
        
        plt.show()

def test_all_models(audio_path):
    """æµ‹è¯•æ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹"""
    model_types = ['basic', 'simplified', 'advanced']
    all_results = {}
    
    for model_type in model_types:
        print(f"\n{'='*50}")
        print(f"æµ‹è¯• {model_type} æ¨¡å‹")
        print(f"{'='*50}")
        
        tester = MultiModelTester(model_type)
        if tester.model is None:
            print(f"è·³è¿‡ {model_type} æ¨¡å‹ï¼ˆåŠ è½½å¤±è´¥ï¼‰")
            continue
            
        predictions, features = tester.predict_single_audio(audio_path)
        
        if predictions:
            print(f"\n=== {model_type}æ¨¡å‹é¢„æµ‹ç»“æœ ===")
            for pred in predictions:
                instrument_name = InstrumentMapper.get_english_name(pred['instrument'])
                print(f"{pred['rank']}. {instrument_name}: {pred['probability']:.3f}")
            
            # å¯è§†åŒ–ç»“æœ
            audio_name = os.path.splitext(os.path.basename(audio_path))[0]
            tester.visualize_prediction(predictions, features, audio_name)
            
            # å­˜å‚¨ç»“æœ
            all_results[model_type] = {
                'top_prediction': predictions[0],
                'all_predictions': predictions
            }
    
    return all_results

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ¨¡å‹æµ‹è¯•')
    parser.add_argument('--model-type', type=str, default='all', 
                       choices=['basic', 'simplified', 'advanced', 'all'],
                       help='è¦æµ‹è¯•çš„æ¨¡å‹ç±»å‹')
    parser.add_argument('--audio-path', type=str, 
                       default=os.path.join(project_root, "music", "3.flac"),
                       help='æµ‹è¯•éŸ³é¢‘è·¯å¾„')
    
    args = parser.parse_args()
    
    print("=== AIéŸ³é¢‘åˆ†æä¸è‡ªåŠ¨æ‰’è°±ç³»ç»Ÿ - å¤šæ¨¡å‹æµ‹è¯• ===")
    
    # 1. åˆå§‹åŒ–é…ç½®
    Config.create_directories()
    
    # 2. æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.audio_path):
        print(f"é”™è¯¯: æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ - {args.audio_path}")
        print("è¯·ç¡®ä¿éŸ³é¢‘æ–‡ä»¶å­˜åœ¨")
        return
    
    if args.model_type == 'all':
        # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
        results = test_all_models(args.audio_path)
        
        # è¾“å‡ºæ¯”è¾ƒç»“æœ
        print(f"\n{'='*60}")
        print("æ¨¡å‹æ¯”è¾ƒç»“æœ")
        print(f"{'='*60}")
        
        for model_type, result in results.items():
            top_pred = result['top_prediction']
            instrument_name = InstrumentMapper.get_english_name(top_pred['instrument'])
            print(f"{model_type:>10}æ¨¡å‹: {instrument_name:15s} (ç½®ä¿¡åº¦: {top_pred['probability']:.3f})")
    
    else:
        # æµ‹è¯•å•ä¸ªæ¨¡å‹
        tester = MultiModelTester(args.model_type)
        if tester.model is None:
            return
            
        predictions, features = tester.predict_single_audio(args.audio_path)
        
        if predictions:
            print(f"\n=== {args.model_type}æ¨¡å‹é¢„æµ‹ç»“æœ ===")
            for pred in predictions:
                instrument_name = InstrumentMapper.get_english_name(pred['instrument'])
                print(f"{pred['rank']}. {instrument_name}: {pred['probability']:.3f}")
            
            # å¯è§†åŒ–ç»“æœ
            audio_name = os.path.splitext(os.path.basename(args.audio_path))[0]
            tester.visualize_prediction(predictions, features, audio_name)
            
            # è¾“å‡ºæœ€å¯èƒ½çš„ä¹å™¨
            top_prediction = predictions[0]
            instrument_name = InstrumentMapper.get_english_name(top_prediction['instrument'])
            print(f"\nğŸµ æœ€å¯èƒ½çš„ä¹å™¨: {instrument_name} (ç½®ä¿¡åº¦: {top_prediction['probability']:.3f})")

if __name__ == "__main__":
    main()