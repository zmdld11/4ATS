# config.py

```py
# 环境配置
import os

class Config:
    """项目配置类"""
    
    # 路径配置
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    DATA_DIR = os.path.join(BASE_DIR, "data")
    MODEL_DIR = os.path.join(BASE_DIR, "model")
    OUTPUT_DIR = os.path.join(BASE_DIR, "output")
    
    # 数据集配置
    DATASET_NAME = "IRMAS-TrainingData"
    DATASET_URL = "https://zenodo.org/record/1290750/files/IRMAS-TrainingData.zip"
    
    # 音频处理配置
    TARGET_SAMPLE_RATE = 22050
    AUDIO_DURATION = 3  # 秒
    N_MELS = 128
    
    # 训练配置
    BATCH_SIZE = 32
    EPOCHS = 100
    LEARNING_RATE = 0.0005
    VALIDATION_SPLIT = 0.2
    
    # 模型配置
    INPUT_SHAPE = (128, 130, 1)  # Mel频谱图形状
    
    @classmethod
    def create_directories(cls):
        """创建必要的目录"""
        directories = [cls.DATA_DIR, cls.MODEL_DIR, cls.OUTPUT_DIR]
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            print(f"目录已创建: {directory}")
```



# utils.py

```py
import os
import urllib.request
import zipfile
import numpy as np
import matplotlib.pyplot as plt
import torch
from config.config import Config

def download_dataset():
    """下载数据集"""
    dataset_path = os.path.join(Config.DATA_DIR, "irmas.zip")
    extract_path = Config.DATA_DIR
    
    if not os.path.exists(os.path.join(extract_path, Config.DATASET_NAME)):
        print("下载数据集中...")
        urllib.request.urlretrieve(Config.DATASET_URL, dataset_path)
        
        # 解压文件
        print("解压数据集中...")
        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        
        print("数据集下载完成!")
    else:
        print("数据集已存在!")

def plot_training_history(train_losses, val_losses, train_accs, val_accs, save_path=None):
    """绘制训练历史图表"""
    # 设置中文字体
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']  # 用来正常显示中文标签
    plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
    
    plt.figure(figsize=(12, 4))
    
    # 准确率图表
    plt.subplot(1, 2, 1)
    plt.plot(train_accs, label='训练准确率')
    plt.plot(val_accs, label='验证准确率')
    plt.title('模型准确率')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    
    # 损失图表
    plt.subplot(1, 2, 2)
    plt.plot(train_losses, label='训练损失')
    plt.plot(val_losses, label='验证损失')
    plt.title('模型损失')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, bbox_inches='tight', dpi=300, 
                   facecolor='white', edgecolor='none')
        print(f"训练图表已保存到: {save_path}")
    
    plt.show()

def analyze_model_performance(model, test_loader, label_encoder, device, save_dir):
    """分析模型性能"""
    from sklearn.metrics import classification_report, confusion_matrix
    import seaborn as sns
    
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for data, labels in test_loader:
            data, labels = data.to(device), labels.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # 分类报告
    print("分类报告:")
    print(classification_report(all_labels, all_preds, 
                              target_names=label_encoder.classes_))
    
    # 混淆矩阵
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(all_labels, all_preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.title('混淆矩阵')
    plt.xlabel('预测标签')
    plt.ylabel('真实标签')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    confusion_matrix_path = os.path.join(save_dir, 'confusion_matrix.png')
    plt.savefig(confusion_matrix_path)
    print(f"混淆矩阵已保存到: {confusion_matrix_path}")
    plt.show()
    
    # 计算每个类别的准确率
    print("\n各类别准确率:")
    class_accuracy = {}
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)
    
    for i, class_name in enumerate(label_encoder.classes_):
        mask = all_labels == i
        if np.sum(mask) > 0:
            acc = np.mean(all_preds[mask] == all_labels[mask])
            class_accuracy[class_name] = acc
            print(f"  {class_name}: {acc:.3f}")
    
    return class_accuracy
```

# audio_preprocessor.py

```py
import os
import librosa
import numpy as np
import pickle
import hashlib
import torch
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from config.config import Config

class AudioDataset(Dataset):
    """PyTorch音频数据集"""
    
    def __init__(self, features, labels, transform=None):
        self.features = features
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        feature = self.features[idx]
        label = self.labels[idx]
        
        # 转换为PyTorch tensor
        feature = torch.FloatTensor(feature).unsqueeze(0)  # 添加channel维度
        label = torch.LongTensor([label])[0]
        
        if self.transform:
            feature = self.transform(feature)
            
        return feature, label

class AudioDataPreprocessor:
    """音频数据预处理器 - 带缓存功能"""
    
    def __init__(self, target_sr=Config.TARGET_SAMPLE_RATE, 
                 duration=Config.AUDIO_DURATION,
                 use_cache=True):
        self.target_sr = target_sr
        self.duration = duration
        self.label_encoder = LabelEncoder()
        self.use_cache = use_cache
        self.cache_dir = os.path.join(Config.DATA_DIR, "preprocessed_cache")
        
        # 创建缓存目录
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    def _get_cache_filename(self, data_dir):
        """生成缓存文件名（基于数据目录和配置参数的哈希）"""
        config_str = f"{data_dir}_{self.target_sr}_{self.duration}_{Config.N_MELS}"
        hash_obj = hashlib.md5(config_str.encode())
        return os.path.join(self.cache_dir, f"preprocessed_{hash_obj.hexdigest()}.pkl")
    
    def _save_to_cache(self, cache_file, X, y, label_encoder):
        """保存预处理结果到缓存"""
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'features': X,
                    'labels': y,
                    'label_encoder': label_encoder,
                    'config': {
                        'target_sr': self.target_sr,
                        'duration': self.duration,
                        'n_mels': Config.N_MELS
                    }
                }, f)
            print(f"✅ 预处理数据已缓存到: {cache_file}")
            return True
        except Exception as e:
            print(f"❌ 缓存保存失败: {e}")
            return False
    
    def _load_from_cache(self, cache_file):
        """从缓存加载预处理结果"""
        try:
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # 验证配置是否匹配
            config = cache_data['config']
            if (config['target_sr'] == self.target_sr and 
                config['duration'] == self.duration and 
                config['n_mels'] == Config.N_MELS):
                
                print("✅ 从缓存加载预处理数据")
                return cache_data['features'], cache_data['labels'], cache_data['label_encoder']
            else:
                print("⚠️ 缓存配置不匹配，重新预处理")
                return None, None, None
                
        except Exception as e:
            print(f"❌ 缓存加载失败: {e}")
            return None, None, None
    
    def load_audio_samples(self, data_dir):
        """加载音频样本和标签"""
        samples = []
        labels = []
        
        dataset_path = os.path.join(data_dir, Config.DATASET_NAME)
        print(f"从 {dataset_path} 加载数据...")
        
        # 遍历每个乐器文件夹
        for instrument in os.listdir(dataset_path):
            instrument_path = os.path.join(dataset_path, instrument)
            if os.path.isdir(instrument_path):
                for audio_file in os.listdir(instrument_path):
                    if audio_file.endswith('.wav'):
                        audio_path = os.path.join(instrument_path, audio_file)
                        samples.append(audio_path)
                        labels.append(instrument)
        
        print(f"找到 {len(samples)} 个音频样本")
        print(f"乐器类别: {set(labels)}")
        
        return samples, labels
    
    def extract_enhanced_features(self, audio_path):
        """提取增强的音频特征"""
        try:
            # 加载音频
            y, sr = librosa.load(audio_path, sr=self.target_sr)
            
            # 确保音频长度一致
            y = librosa.util.fix_length(y, size=self.target_sr * self.duration)
            
            # 提取Mel频谱图
            mel_spec = librosa.feature.melspectrogram(
                y=y, sr=sr, n_mels=Config.N_MELS, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # 标准化
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            return log_mel
            
        except Exception as e:
            print(f"处理音频 {audio_path} 时出错: {e}")
            return None
    
    def prepare_dataset(self, audio_paths, labels, use_cache=True):
        """准备训练数据集 - 带缓存功能"""
        cache_file = self._get_cache_filename(Config.DATA_DIR)
        
        # 尝试从缓存加载
        if use_cache and os.path.exists(cache_file):
            X, y, label_encoder = self._load_from_cache(cache_file)
            if X is not None and y is not None:
                self.label_encoder = label_encoder
                return X, y
        
        # 缓存不存在或不可用，重新处理
        print("🔄 预处理音频数据（这可能需要一些时间）...")
        features = []
        valid_labels = []
        
        for i, (path, label) in enumerate(zip(audio_paths, labels)):
            if i % 100 == 0:  # 每100个样本显示进度
                print(f"已处理 {i}/{len(audio_paths)} 个样本 ({i/len(audio_paths)*100:.1f}%)")
                
            feature = self.extract_enhanced_features(path)
            if feature is not None:
                features.append(feature)
                valid_labels.append(label)
        
        # 转换为numpy数组
        X = np.array(features)
        y = self.label_encoder.fit_transform(valid_labels)
        
        print(f"最终数据集形状: {X.shape}")
        
        # 保存到缓存
        if use_cache:
            self._save_to_cache(cache_file, X, y, self.label_encoder)
        
        return X, y
    
    def create_data_loaders(self, batch_size=Config.BATCH_SIZE, use_cache=True):
        """创建PyTorch数据加载器 - 带缓存功能"""
        # 加载音频样本
        audio_paths, labels = self.load_audio_samples(Config.DATA_DIR)
        
        # 提取特征（使用缓存）
        X, y = self.prepare_dataset(audio_paths, labels, use_cache=use_cache)
        
        # 划分训练集和测试集
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=Config.VALIDATION_SPLIT, 
            random_state=42, stratify=y
        )
        
        # 创建数据集
        train_dataset = AudioDataset(X_train, y_train)
        test_dataset = AudioDataset(X_test, y_test)
        
        # 创建数据加载器
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, test_loader, len(self.label_encoder.classes_)
```

# model_builder.py

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
from config.config import Config

class ImprovedInstrumentClassifier(nn.Module):
    """改进的乐器分类CNN模型 - PyTorch版本"""
    
    def __init__(self, input_shape, num_classes):
        super(ImprovedInstrumentClassifier, self).__init__()
        
        # 确保输入形状是 (channels, height, width) 格式
        if len(input_shape) == 3:
            self.input_shape = input_shape  # (channels, height, width)
        elif len(input_shape) == 2:
            self.input_shape = (1, input_shape[0], input_shape[1])  # 添加channel维度
        else:
            raise ValueError(f"不支持的输入形状: {input_shape}")
            
        self.num_classes = num_classes
        
        print(f"模型输入形状: {self.input_shape}")
        
        # 第一个卷积块
        self.conv1 = nn.Sequential(
            nn.Conv2d(self.input_shape[0], 64, 3, padding=1),  # 使用正确的输入通道数
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # 第二个卷积块
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # 第三个卷积块
        self.conv3 = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # 计算卷积后的特征图尺寸
        with torch.no_grad():
            self.feature_size = self._get_conv_output(self.input_shape)
        
        # 全连接层
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_size, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def _get_conv_output(self, shape):
        """计算卷积层输出尺寸"""
        batch_size = 1
        # 创建测试输入 (batch_size, channels, height, width)
        input_tensor = torch.rand(batch_size, *shape)
        output = self.conv1(input_tensor)
        output = self.conv2(output)
        output = self.conv3(output)
        return output.view(batch_size, -1).size(1)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = x.view(x.size(0), -1)  # 展平
        x = self.classifier(x)
        return x

def create_improved_classifier(input_shape, num_classes):
    """创建改进的乐器分类器"""
    return ImprovedInstrumentClassifier(input_shape, num_classes)
```

# model_trainer.py

```py
import os
import torch
import torch.nn as nn
import joblib
import numpy as np
from config.config import Config

class ModelTrainer:
    """PyTorch模型训练器"""
    
    def __init__(self, model, preprocessor, device):
        self.model = model
        self.preprocessor = preprocessor
        self.device = device
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
    
    def train_epoch(self, train_loader, criterion, optimizer):
        """训练一个epoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for data, labels in train_loader:
            data, labels = data.to(self.device), labels.to(self.device)
            
            # 前向传播
            outputs = self.model(data)
            loss = criterion(outputs, labels)
            
            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # 统计
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = correct / total
        
        return epoch_loss, epoch_acc
    
    def validate(self, val_loader, criterion):
        """验证模型"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, labels in val_loader:
                data, labels = data.to(self.device), labels.to(self.device)
                outputs = self.model(data)
                loss = criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = correct / total
        
        return epoch_loss, epoch_acc
    
    def train(self, train_loader, val_loader, epochs=Config.EPOCHS, patience=15):
        """训练模型"""
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=Config.LEARNING_RATE)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=8, min_lr=1e-7
        )
        
        print("开始训练模型...")
        best_val_acc = 0.0
        patience_counter = 0
        
        for epoch in range(epochs):
            # 训练
            train_loss, train_acc = self.train_epoch(train_loader, criterion, optimizer)
            
            # 验证
            val_loss, val_acc = self.validate(val_loader, criterion)
            
            # 学习率调整
            scheduler.step(val_loss)
            
            # 记录历史
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_acc'].append(val_acc)
            
            print(f'Epoch [{epoch+1}/{epochs}]')
            print(f'  训练损失: {train_loss:.4f}, 训练准确率: {train_acc:.4f}')
            print(f'  验证损失: {val_loss:.4f}, 验证准确率: {val_acc:.4f}')
            print(f'  学习率: {optimizer.param_groups[0]["lr"]:.2e}')
            
            # 早停和保存最佳模型
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                self.save_model("best_model")
                print(f'  ✅ 保存最佳模型，验证准确率: {val_acc:.4f}')
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                print(f'早停: {patience}个epoch验证准确率未提升')
                break
        
        return self.history
    
    def evaluate(self, test_loader):
        """评估模型"""
        test_loss, test_acc = self.validate(test_loader, nn.CrossEntropyLoss())
        print(f"最终测试准确率: {test_acc:.4f}")
        return test_loss, test_acc
    
    def save_model(self, model_name="best_model"):
        """保存模型和相关文件"""
        # 确保使用Config.MODEL_DIR作为基础路径
        model_save_path = os.path.join(Config.MODEL_DIR, f"{model_name}.pth")
        
        # 保存完整的模型信息
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_architecture': self.model.__class__.__name__,
            'input_shape': getattr(self.model, 'input_shape', None),
            'num_classes': getattr(self.model, 'num_classes', None),
            'history': self.history,
            'epoch': len(self.history['train_loss'])
        }, model_save_path)
        
        # 保存标签编码器
        label_encoder_save_path = os.path.join(Config.MODEL_DIR, f"{model_name}_label_encoder.pkl")
        joblib.dump(self.preprocessor.label_encoder, label_encoder_save_path)
        
        print(f"模型已保存到: {model_save_path}")
        print(f"标签编码器已保存到: {label_encoder_save_path}")
        
        return model_save_path, label_encoder_save_path
```

# main.py

```py
import os
import torch
import sys
import argparse
# 添加项目根目录到Python路径
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.config import Config
from src.audio_preprocessor import AudioDataPreprocessor
from src.model_builder import create_improved_classifier
from src.model_trainer import ModelTrainer
from src.utils import download_dataset, plot_training_history, analyze_model_performance

def setup_device():
    """设置训练设备"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"🎉 使用GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device('cpu')
        print("❌ 使用CPU进行训练")
    return device

def parse_arguments():
    """解析命令行参数"""
    parser = argparse.ArgumentParser(description='AI音频分析与自动扒谱系统')
    parser.add_argument('--no-cache', action='store_true', 
                       help='不使用缓存，重新预处理数据')
    parser.add_argument('--resume', type=str, default=None,
                       help='从指定模型继续训练')
    parser.add_argument('--epochs', type=int, default=Config.EPOCHS,
                       help='训练轮数')
    parser.add_argument('--no-resume', action='store_true',
                       help='强制从头开始训练，忽略现有模型')
    return parser.parse_args()

def get_model_path(model_name="best_model.pth"):
    """获取模型文件的正确路径"""
    # 如果已经是绝对路径，直接返回
    if os.path.isabs(model_name):
        return model_name
    
    # 如果是相对路径，基于项目根目录构建
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    model_path = os.path.join(project_root, "model", model_name)
    return model_path

def main():
    """主函数"""
    args = parse_arguments()
    
    print("=== AI音频分析与自动扒谱系统 - PyTorch版本 ===")
    print(f"使用缓存: {not args.no_cache}")
    
    # 1. 初始化配置
    print("1. 初始化配置...")
    Config.create_directories()
    
    # 2. 设置设备
    print("2. 检查硬件配置...")
    device = setup_device()
    print(f"PyTorch版本: {torch.__version__}")
    
    # 3. 数据预处理
    print("3. 数据预处理...")
    preprocessor = AudioDataPreprocessor(use_cache=not args.no_cache)
    train_loader, test_loader, num_classes = preprocessor.create_data_loaders(use_cache=not args.no_cache)
    
    # 获取输入形状（从第一个batch）
    for data, _ in train_loader:
        input_shape = data.shape[1:]  # (1, 128, 130)
        break
    
    print(f"输入形状: {input_shape}")
    print(f"类别数量: {num_classes}")
    
    # 4. 构建或加载模型
    print("4. 构建模型...")
    model = create_improved_classifier(input_shape, num_classes)
    model = model.to(device)
    
    # 初始化训练器
    trainer = ModelTrainer(model, preprocessor, device)
    
    # 检查是否需要恢复训练
    resume_path = args.resume
    if args.no_resume:
        resume_path = None
        print("⚠️ 强制从头开始训练...")
    elif resume_path is None:
        # 自动检测现有模型
        default_model_path = get_model_path("best_model.pth")
        if os.path.exists(default_model_path):
            resume_path = default_model_path
            print(f"发现现有模型: {resume_path}")
    
    if resume_path and os.path.exists(resume_path):
        print(f"从 {resume_path} 加载预训练模型...")
        try:
            checkpoint = torch.load(resume_path, map_location=device)
            
            # 加载模型状态
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # 加载训练历史（如果存在）
            if 'history' in checkpoint:
                trainer.history = checkpoint['history']
                print(f"恢复训练历史，已训练 {len(trainer.history['train_loss'])} 个epoch")
            
            print("✅ 模型加载成功，继续训练...")
            
        except Exception as e:
            print(f"❌ 模型加载失败: {e}")
            print("⚠️ 将从头开始训练...")
    else:
        if resume_path:
            print(f"⚠️ 模型文件不存在: {resume_path}，将从头开始训练")
        else:
            print("✅ 从头开始训练新模型...")
    
    # 5. 训练模型
    print("5. 开始训练...")
    history = trainer.train(train_loader, test_loader, epochs=args.epochs)
    
    # 6. 评估模型
    print("6. 模型评估...")
    test_loss, test_acc = trainer.evaluate(test_loader)
    
    # 7. 保存最终模型
    print("7. 保存模型...")
    model_path, encoder_path = trainer.save_model()
    
    # 8. 可视化结果
    print("8. 生成可视化结果...")
    plot_training_history(
        history['train_loss'], history['val_loss'],
        history['train_acc'], history['val_acc'],
        os.path.join(Config.OUTPUT_DIR, 'training_curves.png')
    )
    
    # 性能分析
    analyze_model_performance(
        model, test_loader, preprocessor.label_encoder, device, Config.OUTPUT_DIR
    )
    
    print("\n=== 训练完成! ===")
    print(f"测试准确率: {test_acc:.4f}")
    print(f"模型保存位置: {Config.MODEL_DIR}")
    print(f"输出文件位置: {Config.OUTPUT_DIR}")

if __name__ == "__main__":
    main()
```

