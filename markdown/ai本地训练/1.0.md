# config.py

```py
# ç¯å¢ƒé…ç½®
import os

class Config:
    """é¡¹ç›®é…ç½®ç±»"""
    
    # è·¯å¾„é…ç½®
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    DATA_DIR = os.path.join(BASE_DIR, "data")
    MODEL_DIR = os.path.join(BASE_DIR, "model")
    OUTPUT_DIR = os.path.join(BASE_DIR, "output")
    
    # æ•°æ®é›†é…ç½®
    DATASET_NAME = "IRMAS-TrainingData"
    DATASET_URL = "https://zenodo.org/record/1290750/files/IRMAS-TrainingData.zip"
    
    # éŸ³é¢‘å¤„ç†é…ç½®
    TARGET_SAMPLE_RATE = 22050
    AUDIO_DURATION = 3  # ç§’
    N_MELS = 128
    
    # è®­ç»ƒé…ç½®
    BATCH_SIZE = 32
    EPOCHS = 100
    LEARNING_RATE = 0.0005
    VALIDATION_SPLIT = 0.2
    
    # æ¨¡å‹é…ç½®
    INPUT_SHAPE = (128, 130, 1)  # Melé¢‘è°±å›¾å½¢çŠ¶
    
    @classmethod
    def create_directories(cls):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [cls.DATA_DIR, cls.MODEL_DIR, cls.OUTPUT_DIR]
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            print(f"ç›®å½•å·²åˆ›å»º: {directory}")
```



# utils.py

```py
import os
import urllib.request
import zipfile
import numpy as np
import matplotlib.pyplot as plt
import torch
from config.config import Config

def download_dataset():
    """ä¸‹è½½æ•°æ®é›†"""
    dataset_path = os.path.join(Config.DATA_DIR, "irmas.zip")
    extract_path = Config.DATA_DIR
    
    if not os.path.exists(os.path.join(extract_path, Config.DATASET_NAME)):
        print("ä¸‹è½½æ•°æ®é›†ä¸­...")
        urllib.request.urlretrieve(Config.DATASET_URL, dataset_path)
        
        # è§£å‹æ–‡ä»¶
        print("è§£å‹æ•°æ®é›†ä¸­...")
        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        
        print("æ•°æ®é›†ä¸‹è½½å®Œæˆ!")
    else:
        print("æ•°æ®é›†å·²å­˜åœ¨!")

def plot_training_history(train_losses, val_losses, train_accs, val_accs, save_path=None):
    """ç»˜åˆ¶è®­ç»ƒå†å²å›¾è¡¨"""
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
    plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    
    plt.figure(figsize=(12, 4))
    
    # å‡†ç¡®ç‡å›¾è¡¨
    plt.subplot(1, 2, 1)
    plt.plot(train_accs, label='è®­ç»ƒå‡†ç¡®ç‡')
    plt.plot(val_accs, label='éªŒè¯å‡†ç¡®ç‡')
    plt.title('æ¨¡å‹å‡†ç¡®ç‡')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    
    # æŸå¤±å›¾è¡¨
    plt.subplot(1, 2, 2)
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±')
    plt.plot(val_losses, label='éªŒè¯æŸå¤±')
    plt.title('æ¨¡å‹æŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, bbox_inches='tight', dpi=300, 
                   facecolor='white', edgecolor='none')
        print(f"è®­ç»ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()

def analyze_model_performance(model, test_loader, label_encoder, device, save_dir):
    """åˆ†ææ¨¡å‹æ€§èƒ½"""
    from sklearn.metrics import classification_report, confusion_matrix
    import seaborn as sns
    
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for data, labels in test_loader:
            data, labels = data.to(device), labels.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # åˆ†ç±»æŠ¥å‘Š
    print("åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_labels, all_preds, 
                              target_names=label_encoder.classes_))
    
    # æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(all_labels, all_preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    confusion_matrix_path = os.path.join(save_dir, 'confusion_matrix.png')
    plt.savefig(confusion_matrix_path)
    print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {confusion_matrix_path}")
    plt.show()
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    print("\nå„ç±»åˆ«å‡†ç¡®ç‡:")
    class_accuracy = {}
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)
    
    for i, class_name in enumerate(label_encoder.classes_):
        mask = all_labels == i
        if np.sum(mask) > 0:
            acc = np.mean(all_preds[mask] == all_labels[mask])
            class_accuracy[class_name] = acc
            print(f"  {class_name}: {acc:.3f}")
    
    return class_accuracy
```

# audio_preprocessor.py

```py
import os
import librosa
import numpy as np
import pickle
import hashlib
import torch
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from config.config import Config

class AudioDataset(Dataset):
    """PyTorchéŸ³é¢‘æ•°æ®é›†"""
    
    def __init__(self, features, labels, transform=None):
        self.features = features
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        feature = self.features[idx]
        label = self.labels[idx]
        
        # è½¬æ¢ä¸ºPyTorch tensor
        feature = torch.FloatTensor(feature).unsqueeze(0)  # æ·»åŠ channelç»´åº¦
        label = torch.LongTensor([label])[0]
        
        if self.transform:
            feature = self.transform(feature)
            
        return feature, label

class AudioDataPreprocessor:
    """éŸ³é¢‘æ•°æ®é¢„å¤„ç†å™¨ - å¸¦ç¼“å­˜åŠŸèƒ½"""
    
    def __init__(self, target_sr=Config.TARGET_SAMPLE_RATE, 
                 duration=Config.AUDIO_DURATION,
                 use_cache=True):
        self.target_sr = target_sr
        self.duration = duration
        self.label_encoder = LabelEncoder()
        self.use_cache = use_cache
        self.cache_dir = os.path.join(Config.DATA_DIR, "preprocessed_cache")
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    def _get_cache_filename(self, data_dir):
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶åï¼ˆåŸºäºæ•°æ®ç›®å½•å’Œé…ç½®å‚æ•°çš„å“ˆå¸Œï¼‰"""
        config_str = f"{data_dir}_{self.target_sr}_{self.duration}_{Config.N_MELS}"
        hash_obj = hashlib.md5(config_str.encode())
        return os.path.join(self.cache_dir, f"preprocessed_{hash_obj.hexdigest()}.pkl")
    
    def _save_to_cache(self, cache_file, X, y, label_encoder):
        """ä¿å­˜é¢„å¤„ç†ç»“æœåˆ°ç¼“å­˜"""
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'features': X,
                    'labels': y,
                    'label_encoder': label_encoder,
                    'config': {
                        'target_sr': self.target_sr,
                        'duration': self.duration,
                        'n_mels': Config.N_MELS
                    }
                }, f)
            print(f"âœ… é¢„å¤„ç†æ•°æ®å·²ç¼“å­˜åˆ°: {cache_file}")
            return True
        except Exception as e:
            print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _load_from_cache(self, cache_file):
        """ä»ç¼“å­˜åŠ è½½é¢„å¤„ç†ç»“æœ"""
        try:
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # éªŒè¯é…ç½®æ˜¯å¦åŒ¹é…
            config = cache_data['config']
            if (config['target_sr'] == self.target_sr and 
                config['duration'] == self.duration and 
                config['n_mels'] == Config.N_MELS):
                
                print("âœ… ä»ç¼“å­˜åŠ è½½é¢„å¤„ç†æ•°æ®")
                return cache_data['features'], cache_data['labels'], cache_data['label_encoder']
            else:
                print("âš ï¸ ç¼“å­˜é…ç½®ä¸åŒ¹é…ï¼Œé‡æ–°é¢„å¤„ç†")
                return None, None, None
                
        except Exception as e:
            print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None, None, None
    
    def load_audio_samples(self, data_dir):
        """åŠ è½½éŸ³é¢‘æ ·æœ¬å’Œæ ‡ç­¾"""
        samples = []
        labels = []
        
        dataset_path = os.path.join(data_dir, Config.DATASET_NAME)
        print(f"ä» {dataset_path} åŠ è½½æ•°æ®...")
        
        # éå†æ¯ä¸ªä¹å™¨æ–‡ä»¶å¤¹
        for instrument in os.listdir(dataset_path):
            instrument_path = os.path.join(dataset_path, instrument)
            if os.path.isdir(instrument_path):
                for audio_file in os.listdir(instrument_path):
                    if audio_file.endswith('.wav'):
                        audio_path = os.path.join(instrument_path, audio_file)
                        samples.append(audio_path)
                        labels.append(instrument)
        
        print(f"æ‰¾åˆ° {len(samples)} ä¸ªéŸ³é¢‘æ ·æœ¬")
        print(f"ä¹å™¨ç±»åˆ«: {set(labels)}")
        
        return samples, labels
    
    def extract_enhanced_features(self, audio_path):
        """æå–å¢å¼ºçš„éŸ³é¢‘ç‰¹å¾"""
        try:
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(audio_path, sr=self.target_sr)
            
            # ç¡®ä¿éŸ³é¢‘é•¿åº¦ä¸€è‡´
            y = librosa.util.fix_length(y, size=self.target_sr * self.duration)
            
            # æå–Melé¢‘è°±å›¾
            mel_spec = librosa.feature.melspectrogram(
                y=y, sr=sr, n_mels=Config.N_MELS, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # æ ‡å‡†åŒ–
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            return log_mel
            
        except Exception as e:
            print(f"å¤„ç†éŸ³é¢‘ {audio_path} æ—¶å‡ºé”™: {e}")
            return None
    
    def prepare_dataset(self, audio_paths, labels, use_cache=True):
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›† - å¸¦ç¼“å­˜åŠŸèƒ½"""
        cache_file = self._get_cache_filename(Config.DATA_DIR)
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache and os.path.exists(cache_file):
            X, y, label_encoder = self._load_from_cache(cache_file)
            if X is not None and y is not None:
                self.label_encoder = label_encoder
                return X, y
        
        # ç¼“å­˜ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨ï¼Œé‡æ–°å¤„ç†
        print("ğŸ”„ é¢„å¤„ç†éŸ³é¢‘æ•°æ®ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
        features = []
        valid_labels = []
        
        for i, (path, label) in enumerate(zip(audio_paths, labels)):
            if i % 100 == 0:  # æ¯100ä¸ªæ ·æœ¬æ˜¾ç¤ºè¿›åº¦
                print(f"å·²å¤„ç† {i}/{len(audio_paths)} ä¸ªæ ·æœ¬ ({i/len(audio_paths)*100:.1f}%)")
                
            feature = self.extract_enhanced_features(path)
            if feature is not None:
                features.append(feature)
                valid_labels.append(label)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        X = np.array(features)
        y = self.label_encoder.fit_transform(valid_labels)
        
        print(f"æœ€ç»ˆæ•°æ®é›†å½¢çŠ¶: {X.shape}")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if use_cache:
            self._save_to_cache(cache_file, X, y, self.label_encoder)
        
        return X, y
    
    def create_data_loaders(self, batch_size=Config.BATCH_SIZE, use_cache=True):
        """åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨ - å¸¦ç¼“å­˜åŠŸèƒ½"""
        # åŠ è½½éŸ³é¢‘æ ·æœ¬
        audio_paths, labels = self.load_audio_samples(Config.DATA_DIR)
        
        # æå–ç‰¹å¾ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        X, y = self.prepare_dataset(audio_paths, labels, use_cache=use_cache)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=Config.VALIDATION_SPLIT, 
            random_state=42, stratify=y
        )
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = AudioDataset(X_train, y_train)
        test_dataset = AudioDataset(X_test, y_test)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, test_loader, len(self.label_encoder.classes_)
```

# model_builder.py

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
from config.config import Config

class ImprovedInstrumentClassifier(nn.Module):
    """æ”¹è¿›çš„ä¹å™¨åˆ†ç±»CNNæ¨¡å‹ - PyTorchç‰ˆæœ¬"""
    
    def __init__(self, input_shape, num_classes):
        super(ImprovedInstrumentClassifier, self).__init__()
        
        # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ˜¯ (channels, height, width) æ ¼å¼
        if len(input_shape) == 3:
            self.input_shape = input_shape  # (channels, height, width)
        elif len(input_shape) == 2:
            self.input_shape = (1, input_shape[0], input_shape[1])  # æ·»åŠ channelç»´åº¦
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥å½¢çŠ¶: {input_shape}")
            
        self.num_classes = num_classes
        
        print(f"æ¨¡å‹è¾“å…¥å½¢çŠ¶: {self.input_shape}")
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        self.conv1 = nn.Sequential(
            nn.Conv2d(self.input_shape[0], 64, 3, padding=1),  # ä½¿ç”¨æ­£ç¡®çš„è¾“å…¥é€šé“æ•°
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        self.conv3 = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # è®¡ç®—å·ç§¯åçš„ç‰¹å¾å›¾å°ºå¯¸
        with torch.no_grad():
            self.feature_size = self._get_conv_output(self.input_shape)
        
        # å…¨è¿æ¥å±‚
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_size, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def _get_conv_output(self, shape):
        """è®¡ç®—å·ç§¯å±‚è¾“å‡ºå°ºå¯¸"""
        batch_size = 1
        # åˆ›å»ºæµ‹è¯•è¾“å…¥ (batch_size, channels, height, width)
        input_tensor = torch.rand(batch_size, *shape)
        output = self.conv1(input_tensor)
        output = self.conv2(output)
        output = self.conv3(output)
        return output.view(batch_size, -1).size(1)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = x.view(x.size(0), -1)  # å±•å¹³
        x = self.classifier(x)
        return x

def create_improved_classifier(input_shape, num_classes):
    """åˆ›å»ºæ”¹è¿›çš„ä¹å™¨åˆ†ç±»å™¨"""
    return ImprovedInstrumentClassifier(input_shape, num_classes)
```

# model_trainer.py

```py
import os
import torch
import torch.nn as nn
import joblib
import numpy as np
from config.config import Config

class ModelTrainer:
    """PyTorchæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model, preprocessor, device):
        self.model = model
        self.preprocessor = preprocessor
        self.device = device
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
    
    def train_epoch(self, train_loader, criterion, optimizer):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for data, labels in train_loader:
            data, labels = data.to(self.device), labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(data)
            loss = criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = correct / total
        
        return epoch_loss, epoch_acc
    
    def validate(self, val_loader, criterion):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, labels in val_loader:
                data, labels = data.to(self.device), labels.to(self.device)
                outputs = self.model(data)
                loss = criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = correct / total
        
        return epoch_loss, epoch_acc
    
    def train(self, train_loader, val_loader, epochs=Config.EPOCHS, patience=15):
        """è®­ç»ƒæ¨¡å‹"""
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=Config.LEARNING_RATE)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=8, min_lr=1e-7
        )
        
        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        best_val_acc = 0.0
        patience_counter = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader, criterion, optimizer)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader, criterion)
            
            # å­¦ä¹ ç‡è°ƒæ•´
            scheduler.step(val_loss)
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_acc'].append(val_acc)
            
            print(f'Epoch [{epoch+1}/{epochs}]')
            print(f'  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}')
            print(f'  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}')
            print(f'  å­¦ä¹ ç‡: {optimizer.param_groups[0]["lr"]:.2e}')
            
            # æ—©åœå’Œä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                self.save_model("best_model")
                print(f'  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}')
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                print(f'æ—©åœ: {patience}ä¸ªepochéªŒè¯å‡†ç¡®ç‡æœªæå‡')
                break
        
        return self.history
    
    def evaluate(self, test_loader):
        """è¯„ä¼°æ¨¡å‹"""
        test_loss, test_acc = self.validate(test_loader, nn.CrossEntropyLoss())
        print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        return test_loss, test_acc
    
    def save_model(self, model_name="best_model"):
        """ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶"""
        # ç¡®ä¿ä½¿ç”¨Config.MODEL_DIRä½œä¸ºåŸºç¡€è·¯å¾„
        model_save_path = os.path.join(Config.MODEL_DIR, f"{model_name}.pth")
        
        # ä¿å­˜å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_architecture': self.model.__class__.__name__,
            'input_shape': getattr(self.model, 'input_shape', None),
            'num_classes': getattr(self.model, 'num_classes', None),
            'history': self.history,
            'epoch': len(self.history['train_loss'])
        }, model_save_path)
        
        # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
        label_encoder_save_path = os.path.join(Config.MODEL_DIR, f"{model_name}_label_encoder.pkl")
        joblib.dump(self.preprocessor.label_encoder, label_encoder_save_path)
        
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")
        print(f"æ ‡ç­¾ç¼–ç å™¨å·²ä¿å­˜åˆ°: {label_encoder_save_path}")
        
        return model_save_path, label_encoder_save_path
```

# main.py

```py
import os
import torch
import sys
import argparse
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.config import Config
from src.audio_preprocessor import AudioDataPreprocessor
from src.model_builder import create_improved_classifier
from src.model_trainer import ModelTrainer
from src.utils import download_dataset, plot_training_history, analyze_model_performance

def setup_device():
    """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"ğŸ‰ ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device('cpu')
        print("âŒ ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")
    return device

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='AIéŸ³é¢‘åˆ†æä¸è‡ªåŠ¨æ‰’è°±ç³»ç»Ÿ')
    parser.add_argument('--no-cache', action='store_true', 
                       help='ä¸ä½¿ç”¨ç¼“å­˜ï¼Œé‡æ–°é¢„å¤„ç†æ•°æ®')
    parser.add_argument('--resume', type=str, default=None,
                       help='ä»æŒ‡å®šæ¨¡å‹ç»§ç»­è®­ç»ƒ')
    parser.add_argument('--epochs', type=int, default=Config.EPOCHS,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--no-resume', action='store_true',
                       help='å¼ºåˆ¶ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¿½ç•¥ç°æœ‰æ¨¡å‹')
    return parser.parse_args()

def get_model_path(model_name="best_model.pth"):
    """è·å–æ¨¡å‹æ–‡ä»¶çš„æ­£ç¡®è·¯å¾„"""
    # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
    if os.path.isabs(model_name):
        return model_name
    
    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºé¡¹ç›®æ ¹ç›®å½•æ„å»º
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    model_path = os.path.join(project_root, "model", model_name)
    return model_path

def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    print("=== AIéŸ³é¢‘åˆ†æä¸è‡ªåŠ¨æ‰’è°±ç³»ç»Ÿ - PyTorchç‰ˆæœ¬ ===")
    print(f"ä½¿ç”¨ç¼“å­˜: {not args.no_cache}")
    
    # 1. åˆå§‹åŒ–é…ç½®
    print("1. åˆå§‹åŒ–é…ç½®...")
    Config.create_directories()
    
    # 2. è®¾ç½®è®¾å¤‡
    print("2. æ£€æŸ¥ç¡¬ä»¶é…ç½®...")
    device = setup_device()
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # 3. æ•°æ®é¢„å¤„ç†
    print("3. æ•°æ®é¢„å¤„ç†...")
    preprocessor = AudioDataPreprocessor(use_cache=not args.no_cache)
    train_loader, test_loader, num_classes = preprocessor.create_data_loaders(use_cache=not args.no_cache)
    
    # è·å–è¾“å…¥å½¢çŠ¶ï¼ˆä»ç¬¬ä¸€ä¸ªbatchï¼‰
    for data, _ in train_loader:
        input_shape = data.shape[1:]  # (1, 128, 130)
        break
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_shape}")
    print(f"ç±»åˆ«æ•°é‡: {num_classes}")
    
    # 4. æ„å»ºæˆ–åŠ è½½æ¨¡å‹
    print("4. æ„å»ºæ¨¡å‹...")
    model = create_improved_classifier(input_shape, num_classes)
    model = model.to(device)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = ModelTrainer(model, preprocessor, device)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤è®­ç»ƒ
    resume_path = args.resume
    if args.no_resume:
        resume_path = None
        print("âš ï¸ å¼ºåˆ¶ä»å¤´å¼€å§‹è®­ç»ƒ...")
    elif resume_path is None:
        # è‡ªåŠ¨æ£€æµ‹ç°æœ‰æ¨¡å‹
        default_model_path = get_model_path("best_model.pth")
        if os.path.exists(default_model_path):
            resume_path = default_model_path
            print(f"å‘ç°ç°æœ‰æ¨¡å‹: {resume_path}")
    
    if resume_path and os.path.exists(resume_path):
        print(f"ä» {resume_path} åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        try:
            checkpoint = torch.load(resume_path, map_location=device)
            
            # åŠ è½½æ¨¡å‹çŠ¶æ€
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # åŠ è½½è®­ç»ƒå†å²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'history' in checkpoint:
                trainer.history = checkpoint['history']
                print(f"æ¢å¤è®­ç»ƒå†å²ï¼Œå·²è®­ç»ƒ {len(trainer.history['train_loss'])} ä¸ªepoch")
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»§ç»­è®­ç»ƒ...")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("âš ï¸ å°†ä»å¤´å¼€å§‹è®­ç»ƒ...")
    else:
        if resume_path:
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {resume_path}ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
        else:
            print("âœ… ä»å¤´å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹...")
    
    # 5. è®­ç»ƒæ¨¡å‹
    print("5. å¼€å§‹è®­ç»ƒ...")
    history = trainer.train(train_loader, test_loader, epochs=args.epochs)
    
    # 6. è¯„ä¼°æ¨¡å‹
    print("6. æ¨¡å‹è¯„ä¼°...")
    test_loss, test_acc = trainer.evaluate(test_loader)
    
    # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("7. ä¿å­˜æ¨¡å‹...")
    model_path, encoder_path = trainer.save_model()
    
    # 8. å¯è§†åŒ–ç»“æœ
    print("8. ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    plot_training_history(
        history['train_loss'], history['val_loss'],
        history['train_acc'], history['val_acc'],
        os.path.join(Config.OUTPUT_DIR, 'training_curves.png')
    )
    
    # æ€§èƒ½åˆ†æ
    analyze_model_performance(
        model, test_loader, preprocessor.label_encoder, device, Config.OUTPUT_DIR
    )
    
    print("\n=== è®­ç»ƒå®Œæˆ! ===")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    print(f"æ¨¡å‹ä¿å­˜ä½ç½®: {Config.MODEL_DIR}")
    print(f"è¾“å‡ºæ–‡ä»¶ä½ç½®: {Config.OUTPUT_DIR}")

if __name__ == "__main__":
    main()
```

