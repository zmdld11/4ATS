# 代码文档 

生成时间: 2025/10/06 周一  8:56:28.27 

主要更改了test的测试方法，由原来的单文件测试变为多文件测试，适配了三种模型

# src 

## advanced_models.py 
文件路径: D:\program_project\4ATS\src\advanced_models.py 

```py 
# advanced_models.py (修复权重初始化)
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from config.config import Config

class ResidualBlock(nn.Module):
    """带有批归一化的残差块"""
    
    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.3):
        super(ResidualBlock, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.dropout = nn.Dropout2d(dropout_rate)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        out = self.dropout(out)
        return out

class AttentionModule(nn.Module):
    """通道注意力模块"""
    
    def __init__(self, channels, reduction=16):
        super(AttentionModule, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class AdvancedInstrumentClassifier(nn.Module):
    """带有残差连接和注意力机制的高级乐器分类器"""
    
    def __init__(self, input_shape, num_classes):
        super(AdvancedInstrumentClassifier, self).__init__()
        
        if len(input_shape) == 3:
            self.input_shape = input_shape
        elif len(input_shape) == 2:
            self.input_shape = (1, input_shape[0], input_shape[1])
        else:
            raise ValueError(f"不支持的输入形状: {input_shape}")
            
        self.num_classes = num_classes
        
        print(f"模型输入形状: {self.input_shape}")
        
        # 初始卷积层 - 修复：使用正确的输入通道数
        self.conv1 = nn.Sequential(
            nn.Conv2d(self.input_shape[0], 64, 7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1)
        )
        
        # 带有注意力的残差块
        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.attention1 = AttentionModule(64)
        
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.attention2 = AttentionModule(128)
        
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.attention3 = AttentionModule(256)
        
        self.layer4 = self._make_layer(256, 512, 2, stride=2)
        self.attention4 = AttentionModule(512)
        
        # 全局池化和分类器
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )
        
        # 延迟权重初始化，在模型移动到设备后进行
        self.weights_initialized = False
    
    def _make_layer(self, in_channels, out_channels, blocks, stride):
        """创建残差层"""
        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride))
        for _ in range(1, blocks):
            layers.append(ResidualBlock(out_channels, out_channels))
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        """正确初始化权重 - 修复版本"""
        print("初始化高级模型权重...")
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:  # 只有在偏置存在时才初始化
                    nn.init.constant_(m.bias, 0)
                    print(f"初始化卷积层偏置: {m.bias.shape}")
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
                print(f"初始化BN层: weight={m.weight.shape}, bias={m.bias.shape}")
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:  # 只有在偏置存在时才初始化
                    nn.init.constant_(m.bias, 0)
                    print(f"初始化全连接层偏置: {m.bias.shape}")
        
        self.weights_initialized = True
        print("? 高级模型权重初始化完成")
    
    def forward(self, x):
        """前向传播"""
        # 延迟初始化权重（第一次前向传播时）
        if not self.weights_initialized:
            self._initialize_weights()
        
        x = self.conv1(x)
        
        x = self.layer1(x)
        x = self.attention1(x)
        
        x = self.layer2(x)
        x = self.attention2(x)
        
        x = self.layer3(x)
        x = self.attention3(x)
        
        x = self.layer4(x)
        x = self.attention4(x)
        
        x = self.global_avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        
        return x

def create_advanced_classifier(input_shape, num_classes):
    """创建高级乐器分类器"""
    return AdvancedInstrumentClassifier(input_shape, num_classes)

# 添加一个更稳定的简化高级模型
class StableAdvancedClassifier(nn.Module):
    """稳定的高级分类器 - 避免复杂的初始化问题"""
    
    def __init__(self, input_shape, num_classes):
        super(StableAdvancedClassifier, self).__init__()
        
        if len(input_shape) == 3:
            self.input_shape = input_shape
        elif len(input_shape) == 2:
            self.input_shape = (1, input_shape[0], input_shape[1])
        else:
            raise ValueError(f"不支持的输入形状: {input_shape}")
            
        self.num_classes = num_classes
        
        print(f"模型输入形状: {self.input_shape}")
        
        # 使用更简单的结构
        self.features = nn.Sequential(
            # 块1
            nn.Conv2d(self.input_shape[0], 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # 块2 - 残差块
            self._residual_block(64, 128, stride=2),
            
            # 块3 - 残差块
            self._residual_block(128, 256, stride=2),
            
            # 块4 - 残差块
            self._residual_block(256, 512, stride=2),
            
            # 全局平均池化
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # 分类器
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def _residual_block(self, in_channels, out_channels, stride=1):
        """简化的残差块"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.25)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

def create_stable_advanced_classifier(input_shape, num_classes):
    """创建稳定的高级分类器"""
    return StableAdvancedClassifier(input_shape, num_classes)

class SimplifiedAdvancedClassifier(nn.Module):
    """简化但有效的分类器"""
    
    def __init__(self, input_shape, num_classes):
        super(SimplifiedAdvancedClassifier, self).__init__()
        
        if len(input_shape) == 3:
            self.input_shape = input_shape
        elif len(input_shape) == 2:
            self.input_shape = (1, input_shape[0], input_shape[1])
        else:
            raise ValueError(f"不支持的输入形状: {input_shape}")
            
        self.num_classes = num_classes
        
        print(f"模型输入形状: {self.input_shape}")
        
        # 增强的CNN主干网络
        self.features = nn.Sequential(
            # 块1
            nn.Conv2d(self.input_shape[0], 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # 块2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # 块3
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # 块4
            nn.Conv2d(256, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # 分类器
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """初始化权重"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """前向传播"""
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

def create_simplified_classifier(input_shape, num_classes):
    """创建简化但有效的分类器"""
    return SimplifiedAdvancedClassifier(input_shape, num_classes)
 
```

## audio_preprocessor.py 
文件路径: D:\program_project\4ATS\src\audio_preprocessor.py 

```py 
# audio_preprocessor.py (添加数据增强 - 中文注释)
import os
import librosa
import numpy as np
import pickle
import hashlib
import torch
import random
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from config.config import Config

class AudioDataset(Dataset):
    """带有数据增强的PyTorch音频数据集"""
    
    def __init__(self, features, labels, transform=None, augment=False):
        self.features = features
        self.labels = labels
        self.transform = transform
        self.augment = augment
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        feature = self.features[idx].copy()  # 重要：创建副本
        label = self.labels[idx]
        
        # 训练期间的数据增强
        if self.augment:
            feature = self._apply_augmentation(feature)
        
        # 转换为PyTorch张量
        feature = torch.FloatTensor(feature).unsqueeze(0)  # 添加通道维度
        label = torch.LongTensor([label])[0]
        
        if self.transform:
            feature = self.transform(feature)
            
        return feature, label
    
    def _apply_augmentation(self, feature):
        """对音频特征应用数据增强"""
        # 时间掩码（类似于SpecAugment）
        if random.random() > 0.5:
            max_mask_width = feature.shape[1] // 4
            mask_width = random.randint(1, max_mask_width)
            mask_start = random.randint(0, feature.shape[1] - mask_width)
            feature[:, mask_start:mask_start + mask_width] = 0
        
        # 频率掩码
        if random.random() > 0.5:
            max_mask_height = feature.shape[0] // 8
            mask_height = random.randint(1, max_mask_height)
            mask_start = random.randint(0, feature.shape[0] - mask_height)
            feature[mask_start:mask_start + mask_height, :] = 0
        
        # 添加小噪声
        if random.random() > 0.7:
            noise = np.random.normal(0, 0.01, feature.shape)
            feature = feature + noise
        
        return feature

class AudioDataPreprocessor:
    """带有增强功能和数据增强的音频数据预处理器"""
    
    def __init__(self, target_sr=Config.TARGET_SAMPLE_RATE, 
                 duration=Config.AUDIO_DURATION,
                 use_cache=True):
        self.target_sr = target_sr
        self.duration = duration
        self.label_encoder = LabelEncoder()
        self.use_cache = use_cache
        self.cache_dir = os.path.join(Config.DATA_DIR, "preprocessed_cache")
        
        # 创建缓存目录
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    def extract_enhanced_features(self, audio_path):
        """提取带有多种表示的增强音频特征"""
        try:
            # 加载音频
            y, sr = librosa.load(audio_path, sr=self.target_sr)
            
            # 确保音频长度一致
            y = librosa.util.fix_length(y, size=self.target_sr * self.duration)
            
            # 提取多种特征
            # 1. Mel频谱图（主要特征）
            mel_spec = librosa.feature.melspectrogram(
                y=y, sr=sr, n_mels=Config.N_MELS, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # 2. MFCC特征
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
            
            # 3. Chroma特征
            chroma = librosa.feature.chroma_stft(y=y, sr=sr)
            
            # 组合特征（可以选择使用哪些）
            # 选项1：仅使用mel频谱图（与当前架构兼容）
            combined_features = log_mel
            
            # 选项2：堆叠多个特征（需要更改模型架构）
            # combined_features = np.vstack([log_mel, mfcc, chroma])
            
            # 标准化
            combined_features = (combined_features - np.mean(combined_features)) / (np.std(combined_features) + 1e-8)
            
            return combined_features
            
        except Exception as e:
            print(f"处理音频 {audio_path} 时出错: {e}")
            return None
    
    def create_data_loaders(self, batch_size=Config.BATCH_SIZE, use_cache=True, augment=True):
        """创建带有可选数据增强的PyTorch数据加载器"""
        # 加载音频样本
        audio_paths, labels = self.load_audio_samples(Config.DATA_DIR)
        
        # 提取特征（使用缓存）
        X, y = self.prepare_dataset(audio_paths, labels, use_cache=use_cache)
        
        # 划分训练集和测试集
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=Config.VALIDATION_SPLIT, 
            random_state=42, stratify=y
        )
        
        # 进一步划分训练集用于验证
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, 
            random_state=42, stratify=y_train
        )
        
        # 创建数据集
        train_dataset = AudioDataset(X_train, y_train, augment=augment)
        val_dataset = AudioDataset(X_val, y_val, augment=False)
        test_dataset = AudioDataset(X_test, y_test, augment=False)
        
        # 创建数据加载器
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
        
        print(f"训练样本数: {len(train_dataset)}")
        print(f"验证样本数: {len(val_dataset)}")
        print(f"测试样本数: {len(test_dataset)}")
        
        return train_loader, val_loader, test_loader, len(self.label_encoder.classes_)

    def _get_cache_filename(self, data_dir):
        """生成缓存文件名"""
        config_str = f"{data_dir}_{self.target_sr}_{self.duration}_{Config.N_MELS}"
        hash_obj = hashlib.md5(config_str.encode())
        return os.path.join(self.cache_dir, f"preprocessed_{hash_obj.hexdigest()}.pkl")
    
    def _save_to_cache(self, cache_file, X, y, label_encoder):
        """保存预处理结果到缓存"""
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'features': X,
                    'labels': y,
                    'label_encoder': label_encoder,
                    'config': {
                        'target_sr': self.target_sr,
                        'duration': self.duration,
                        'n_mels': Config.N_MELS
                    }
                }, f)
            print(f"? 预处理数据已缓存到: {cache_file}")
            return True
        except Exception as e:
            print(f"? 缓存保存失败: {e}")
            return False
    
    def _load_from_cache(self, cache_file):
        """从缓存加载预处理结果"""
        try:
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # 验证配置是否匹配
            config = cache_data['config']
            if (config['target_sr'] == self.target_sr and 
                config['duration'] == self.duration and 
                config['n_mels'] == Config.N_MELS):
                
                print("? 从缓存加载预处理数据")
                return cache_data['features'], cache_data['labels'], cache_data['label_encoder']
            else:
                print("?? 缓存配置不匹配，重新预处理")
                return None, None, None
                
        except Exception as e:
            print(f"? 缓存加载失败: {e}")
            return None, None, None
    
    def load_audio_samples(self, data_dir):
        """加载音频样本和标签"""
        samples = []
        labels = []
        
        dataset_path = os.path.join(data_dir, Config.DATASET_NAME)
        print(f"从 {dataset_path} 加载数据...")
        
        # 遍历每个乐器文件夹
        for instrument in os.listdir(dataset_path):
            instrument_path = os.path.join(dataset_path, instrument)
            if os.path.isdir(instrument_path):
                for audio_file in os.listdir(instrument_path):
                    if audio_file.endswith('.wav'):
                        audio_path = os.path.join(instrument_path, audio_file)
                        samples.append(audio_path)
                        labels.append(instrument)
        
        print(f"找到 {len(samples)} 个音频样本")
        print(f"乐器类别: {set(labels)}")
        
        return samples, labels
    
    def prepare_dataset(self, audio_paths, labels, use_cache=True):
        """准备训练数据集 - 带缓存功能"""
        cache_file = self._get_cache_filename(Config.DATA_DIR)
        
        # 尝试从缓存加载
        if use_cache and os.path.exists(cache_file):
            X, y, label_encoder = self._load_from_cache(cache_file)
            if X is not None and y is not None:
                self.label_encoder = label_encoder
                return X, y
        
        # 缓存不存在或不可用，重新处理
        print("?? 预处理音频数据（这可能需要一些时间）...")
        features = []
        valid_labels = []
        
        for i, (path, label) in enumerate(zip(audio_paths, labels)):
            if i % 100 == 0:  # 每100个样本显示进度
                print(f"已处理 {i}/{len(audio_paths)} 个样本 ({i/len(audio_paths)*100:.1f}%)")
                
            feature = self.extract_enhanced_features(path)
            if feature is not None:
                features.append(feature)
                valid_labels.append(label)
        
        # 转换为numpy数组
        X = np.array(features)
        y = self.label_encoder.fit_transform(valid_labels)
        
        print(f"最终数据集形状: {X.shape}")
        
        # 保存到缓存
        if use_cache:
            self._save_to_cache(cache_file, X, y, self.label_encoder)
        
        return X, y
 
```

## instrument_mapper.py 
文件路径: D:\program_project\4ATS\src\instrument_mapper.py 

```py 
"""
乐器名称映射工具
"""

class InstrumentMapper:
    """乐器名称映射类"""
    
    # IRMAS数据集乐器映射
    IRMAS_MAPPING = {
        'cel': 'Cello',
        'cla': 'Clarinet', 
        'flu': 'Flute',
        'gac': 'Acoustic Guitar',
        'gel': 'Electric Guitar',
        'org': 'Organ',
        'pia': 'Piano',
        'sax': 'Saxophone',
        'tru': 'Trumpet',
        'vio': 'Violin',
        'voi': 'Voice'
    }
    
    # 反向映射（英文到缩写）
    REVERSE_MAPPING = {v: k for k, v in IRMAS_MAPPING.items()}
    
    # 乐器类别描述
    INSTRUMENT_DESCRIPTIONS = {
        'Cello': 'Bowed string instrument, bass voice of the violin family',
        'Clarinet': 'Woodwind instrument with a single-reed mouthpiece',
        'Flute': 'Woodwind instrument that produces sound from the flow of air',
        'Acoustic Guitar': 'String instrument that produces sound acoustically',
        'Electric Guitar': 'Guitar that uses pickups to convert string vibration into electrical signals',
        'Organ': 'Keyboard instrument of one or more pipe divisions',
        'Piano': 'Acoustic stringed keyboard instrument',
        'Saxophone': 'Woodwind instrument made of brass',
        'Trumpet': 'Brass instrument commonly used in classical and jazz ensembles',
        'Violin': 'Bowed string instrument, smallest and highest-pitched in the violin family',
        'Voice': 'Human singing voice'
    }
    
    @classmethod
    def get_english_name(cls, abbreviation):
        """获取英文名称"""
        return cls.IRMAS_MAPPING.get(abbreviation, abbreviation)
    
    @classmethod
    def get_abbreviation(cls, english_name):
        """获取缩写"""
        return cls.REVERSE_MAPPING.get(english_name, english_name)
    
    @classmethod
    def get_description(cls, name):
        """获取乐器描述"""
        # 先尝试作为缩写查找
        english_name = cls.get_english_name(name)
        return cls.INSTRUMENT_DESCRIPTIONS.get(english_name, "No description available")
    
    @classmethod
    def translate_labels(cls, labels):
        """翻译标签列表"""
        if isinstance(labels, list):
            return [cls.get_english_name(label) for label in labels]
        elif isinstance(labels, np.ndarray):
            return np.array([cls.get_english_name(label) for label in labels])
        else:
            return labels
    
    @classmethod
    def print_instrument_info(cls):
        """打印所有乐器信息"""
        print("IRMAS Dataset Instrument Information:")
        print("=" * 50)
        for abbrev, english_name in cls.IRMAS_MAPPING.items():
            description = cls.INSTRUMENT_DESCRIPTIONS.get(english_name, "No description")
            print(f"{abbrev:4s} -> {english_name:15s} : {description}")

    @staticmethod
    def get_english_name_static(abbreviation):
        """静态方法获取英文名称"""
        return InstrumentMapper.get_english_name(abbreviation)
 
```

## main.py 
文件路径: D:\program_project\4ATS\src\main.py 

```py 
# main.py (更新版本 - 中文输出)
import os
import torch
import sys
import argparse

# 添加项目根目录到Python路径
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.config import Config
from src.audio_preprocessor import AudioDataPreprocessor
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier
from src.model_trainer import AdvancedModelTrainer
from src.model_manager import model_manager
from src.utils import download_dataset, plot_training_history, analyze_model_performance

def setup_device():
    """设置训练设备"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"?? 使用GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device('cpu')
        print("? 使用CPU进行训练")
    return device

def parse_arguments():
    """解析命令行参数"""
    parser = argparse.ArgumentParser(description='AI音频分析与自动扒谱系统')
    parser.add_argument('--no-cache', action='store_true', 
                       help='不使用缓存，重新预处理数据')
    parser.add_argument('--resume', type=str, default=None,
                       help='从指定模型继续训练')
    parser.add_argument('--epochs', type=int, default=Config.EPOCHS,
                       help='训练轮数')
    parser.add_argument('--no-resume', action='store_true',
                       help='强制从头开始训练，忽略现有模型')
    parser.add_argument('--basic-model', action='store_true',
                       help='使用基础模型而不是高级模型')
    parser.add_argument('--no-augmentation', action='store_true',
                       help='禁用数据增强')
    parser.add_argument('--model-type', type=str, default='simplified', 
                       choices=['basic', 'advanced', 'simplified', 'all'],
                       help='使用的模型类型 (basic, advanced, simplified, all)')
    parser.add_argument('--train-all', action='store_true',
                       help='训练所有模型类型')
    return parser.parse_args()

def create_model(model_type, input_shape, num_classes, device):
    """根据类型创建模型并移动到设备"""
    if model_type == 'basic':
        model = create_improved_classifier(input_shape, num_classes)
    elif model_type == 'advanced':
        try:
            model = create_advanced_classifier(input_shape, num_classes)
        except Exception as e:
            print(f"高级模型创建失败: {e}")
            print("回退到稳定高级模型")
            model = create_stable_advanced_classifier(input_shape, num_classes)
    elif model_type == 'simplified':
        model = create_simplified_classifier(input_shape, num_classes)
    elif model_type == 'stable_advanced':
        model = create_stable_advanced_classifier(input_shape, num_classes)
    else:
        raise ValueError(f"未知的模型类型: {model_type}")
    
    # 立即移动到设备
    model = model.to(device)
    print(f"? {model_type}模型已创建并移动到{device}")
    
    return model


def train_single_model(model_type, train_loader, val_loader, test_loader, preprocessor, device, args):
    """训练单个模型"""
    print(f"\n{'='*50}")
    print(f"训练 {model_type} 模型")
    print(f"{'='*50}")
    
    # 获取输入形状
    for data, _ in train_loader:
        input_shape = data.shape[1:]  # (1, 128, 130)
        break
    
    print(f"输入形状: {input_shape}")
    print(f"使用设备: {device}")
    
    # 创建模型并立即移动到设备
    model = create_model(model_type, input_shape, len(preprocessor.label_encoder.classes_), device)
    
    # 统计参数数量
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"总参数数量: {total_params:,}")
    print(f"可训练参数数量: {trainable_params:,}")
    
    # 检查模型是否在正确的设备上
    print(f"模型设备: {next(model.parameters()).device}")
    
    # 初始化训练器
    trainer = AdvancedModelTrainer(model, preprocessor, device, model_type=model_type)
    
    # 检查是否需要恢复训练
    resume_model = not args.no_resume
    if resume_model:
        load_success, checkpoint = model_manager.load_model(model, model_type, device)
        if load_success and checkpoint and 'history' in checkpoint:
            trainer.history = checkpoint['history']
            print(f"恢复训练历史，已训练 {len(trainer.history['train_loss'])} 个周期")
            print(f"? {model_type}模型加载成功，继续训练...")
        else:
            print(f"? 从头开始训练{model_type}模型...")
    else:
        print(f"? 强制从头开始训练{model_type}模型...")
    
    # 训练模型
    history = trainer.train(train_loader, val_loader, epochs=args.epochs, 
                           patience=Config.EARLY_STOPPING_PATIENCE)
    
    # 评估模型
    test_loss, test_acc = trainer.evaluate(test_loader)
    
    # 保存最终模型
    trainer.save_model()
    
    return test_acc, history

def setup_device():
    """设置训练设备 - 增加内存监控"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"?? 使用GPU: {torch.cuda.get_device_name(0)}")
        # 打印GPU内存信息
        print(f"GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        device = torch.device('cpu')
        print("? 使用CPU进行训练")
    return device

def main():
    """主函数"""
    args = parse_arguments()
    
    print("=== AI音频分析与自动扒谱系统 - 多版本模型 ===")
    print(f"使用缓存: {not args.no_cache}")
    print(f"模型类型: {args.model_type}")
    print(f"使用数据增强: {not args.no_augmentation}")
    print(f"同时保存多个版本: {Config.SAVE_MULTIPLE_VERSIONS}")
    
    # 显示可用模型
    available_models = model_manager.get_available_models()
    if available_models:
        print(f"已存在的模型: {', '.join(available_models)}")
    else:
        print("没有找到已训练的模型")
    
    # 1. 初始化配置
    print("\n1. 初始化配置...")
    Config.create_directories()
    
    # 2. 设置设备
    print("2. 检查硬件配置...")
    device = setup_device()
    print(f"PyTorch版本: {torch.__version__}")
    
    # 3. 数据预处理
    print("3. 数据预处理...")
    preprocessor = AudioDataPreprocessor(use_cache=not args.no_cache)
    
    # 创建数据加载器，可选数据增强
    use_augmentation = not args.no_augmentation and Config.USE_DATA_AUGMENTATION
    train_loader, val_loader, test_loader, num_classes = preprocessor.create_data_loaders(
        use_cache=not args.no_cache, 
        augment=use_augmentation
    )
    
    print(f"类别数量: {num_classes}")
    
    # 确定要训练的模型类型
    if args.model_type == 'all' or args.train_all:
        model_types = ['basic', 'simplified', 'advanced']
    else:
        model_types = [args.model_type]
    
    # 训练结果统计
    results = {}
    
    # 4. 训练模型
    for model_type in model_types:
        try:
            test_acc, history = train_single_model(
                model_type, train_loader, val_loader, test_loader, 
                preprocessor, device, args
            )
            results[model_type] = {
                'test_accuracy': test_acc,
                'history': history
            }
            
            # 可视化结果
            print(f"8. 生成{model_type}模型可视化结果...")
            plot_training_history(
                history['train_loss'], history['val_loss'],
                history['train_acc'], history['val_acc'],
                os.path.join(Config.OUTPUT_DIR, f'training_curves_{model_type}.png'),
                show_plot=False  # 添加这个参数
            )
            
            # 性能分析（只在最后一个模型上执行，避免重复）
            if model_type == model_types[-1]:
                print("9. 性能分析...")
                # 重新加载最佳模型进行评估
                best_model = create_model(model_type, train_loader.dataset[0][0].shape, num_classes)
                best_model = best_model.to(device)
                load_success, _ = model_manager.load_model(best_model, model_type, device)
                
                if load_success:
                    analyze_model_performance(
                        best_model, test_loader, preprocessor.label_encoder, device, Config.OUTPUT_DIR
                    )
            
        except Exception as e:
            print(f"? 训练{model_type}模型时出错: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # 5. 输出训练总结
    print("\n" + "="*60)
    print("训练总结")
    print("="*60)
    
    for model_type, result in results.items():
        print(f"{model_type:>10}模型: 测试准确率 = {result['test_accuracy']:.4f}")
    
    print(f"\n模型保存位置: {Config.MODEL_DIR}")
    print(f"输出文件位置: {Config.OUTPUT_DIR}")
    
    # 显示最终可用的模型
    final_models = model_manager.get_available_models()
    print(f"最终可用模型: {', '.join(final_models)}")

if __name__ == "__main__":
    main()
 
```

## model_builder.py 
文件路径: D:\program_project\4ATS\src\model_builder.py 

```py 
import torch
import torch.nn as nn
import torch.nn.functional as F
from config.config import Config

class ImprovedInstrumentClassifier(nn.Module):
    """改进的乐器分类CNN模型 - PyTorch版本"""
    
    def __init__(self, input_shape, num_classes):
        super(ImprovedInstrumentClassifier, self).__init__()
        
        # 确保输入形状是 (channels, height, width) 格式
        if len(input_shape) == 3:
            self.input_shape = input_shape  # (channels, height, width)
        elif len(input_shape) == 2:
            self.input_shape = (1, input_shape[0], input_shape[1])  # 添加channel维度
        else:
            raise ValueError(f"不支持的输入形状: {input_shape}")
            
        self.num_classes = num_classes
        
        print(f"模型输入形状: {self.input_shape}")
        
        # 第一个卷积块
        self.conv1 = nn.Sequential(
            nn.Conv2d(self.input_shape[0], 64, 3, padding=1),  # 使用正确的输入通道数
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # 第二个卷积块
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # 第三个卷积块
        self.conv3 = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # 计算卷积后的特征图尺寸
        with torch.no_grad():
            self.feature_size = self._get_conv_output(self.input_shape)
        
        # 全连接层
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_size, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def _get_conv_output(self, shape):
        """计算卷积层输出尺寸"""
        batch_size = 1
        # 创建测试输入 (batch_size, channels, height, width)
        input_tensor = torch.rand(batch_size, *shape)
        output = self.conv1(input_tensor)
        output = self.conv2(output)
        output = self.conv3(output)
        return output.view(batch_size, -1).size(1)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = x.view(x.size(0), -1)  # 展平
        x = self.classifier(x)
        return x

def create_improved_classifier(input_shape, num_classes):
    """创建改进的乐器分类器"""
    return ImprovedInstrumentClassifier(input_shape, num_classes)
 
```

## model_comparison.py 
文件路径: D:\program_project\4ATS\src\model_comparison.py 

```py 
# model_comparison.py (新文件)
import os
import torch
import matplotlib.pyplot as plt
import numpy as np
from config.config import Config
from src.model_manager import model_manager
from src.audio_preprocessor import AudioDataPreprocessor
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier

def compare_models():
    """比较所有可用模型的性能"""
    print("=== 模型性能比较 ===")
    
    # 获取可用模型
    available_models = model_manager.get_available_models()
    if not available_models:
        print("没有找到可用的模型")
        return
    
    print(f"找到的模型: {', '.join(available_models)}")
    
    # 加载数据
    preprocessor = AudioDataPreprocessor(use_cache=True)
    train_loader, val_loader, test_loader, num_classes = preprocessor.create_data_loaders(
        use_cache=True, augment=False
    )
    
    # 设置设备
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    results = {}
    
    for model_type in available_models:
        print(f"\n评估 {model_type} 模型...")
        
        try:
            # 创建模型
            if model_type == 'basic':
                model = create_improved_classifier((1, 128, 130), num_classes)
            elif model_type == 'advanced':
                model = create_advanced_classifier((1, 128, 130), num_classes)
            elif model_type == 'simplified':
                model = create_simplified_classifier((1, 128, 130), num_classes)
            else:
                continue
                
            model = model.to(device)
            
            # 加载权重
            load_success, _ = model_manager.load_model(model, model_type, device)
            if not load_success:
                continue
            
            # 评估模型
            model.eval()
            correct = 0
            total = 0
            
            with torch.no_grad():
                for data, labels in test_loader:
                    data, labels = data.to(device), labels.to(device)
                    outputs = model(data)
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
            
            accuracy = correct / total
            results[model_type] = accuracy
            print(f"{model_type}模型准确率: {accuracy:.4f}")
            
        except Exception as e:
            print(f"评估{model_type}模型失败: {e}")
    
    # 绘制比较图
    if results:
        plt.figure(figsize=(10, 6))
        models = list(results.keys())
        accuracies = [results[model] for model in models]
        
        bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'])
        plt.title('Model Performance Comparison', fontsize=14)
        plt.ylabel('Accuracy', fontsize=12)
        plt.ylim(0, 1.0)
        
        # 在柱子上添加数值
        for bar, accuracy in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{accuracy:.4f}', ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        comparison_path = os.path.join(Config.OUTPUT_DIR, 'model_comparison.png')
        plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
        print(f"\n比较图已保存到: {comparison_path}")
        plt.show()
        
        # 输出最佳模型
        best_model = max(results, key=results.get)
        print(f"\n?? 最佳模型: {best_model} (准确率: {results[best_model]:.4f})")

if __name__ == "__main__":
    compare_models()
 
```

## model_manager.py 
文件路径: D:\program_project\4ATS\src\model_manager.py 

```py 
# model_manager.py (新文件)
import os
import torch
import joblib
from config.config import Config

class ModelManager:
    """模型管理器 - 处理多版本模型的保存和加载"""
    
    def __init__(self):
        self.model_dir = Config.MODEL_DIR
    
    def get_model_filename(self, model_type, suffix="pth"):
        """获取模型文件名"""
        return f"model_{model_type}.{suffix}"
    
    def get_label_encoder_filename(self, model_type):
        """获取标签编码器文件名"""
        return f"model_{model_type}_label_encoder.pkl"
    
    def save_model(self, model, preprocessor, model_type, history=None, epoch=0):
        """保存指定类型的模型"""
        # 模型文件路径
        model_filename = self.get_model_filename(model_type)
        model_path = os.path.join(self.model_dir, model_filename)
        
        # 标签编码器文件路径
        encoder_filename = self.get_label_encoder_filename(model_type)
        encoder_path = os.path.join(self.model_dir, encoder_filename)
        
        # 保存模型
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_architecture': model.__class__.__name__,
            'model_type': model_type,
            'input_shape': getattr(model, 'input_shape', None),
            'num_classes': getattr(model, 'num_classes', None),
            'history': history or {},
            'epoch': epoch
        }, model_path)
        
        # 保存标签编码器
        joblib.dump(preprocessor.label_encoder, encoder_path)
        
        print(f"? {model_type}模型已保存到: {model_path}")
        print(f"? {model_type}标签编码器已保存到: {encoder_path}")
        
        return model_path, encoder_path
    
    def load_model(self, model, model_type, device):
        """加载指定类型的模型"""
        model_filename = self.get_model_filename(model_type)
        model_path = os.path.join(self.model_dir, model_filename)
        
        if not os.path.exists(model_path):
            print(f"?? {model_type}模型文件不存在: {model_path}")
            return False, None
        
        try:
            checkpoint = torch.load(model_path, map_location=device)
            
            # 检查模型类型是否匹配
            saved_model_type = checkpoint.get('model_type', 'unknown')
            if saved_model_type != model_type:
                print(f"?? 模型类型不匹配: 保存的是 '{saved_model_type}'，当前请求的是 '{model_type}'")
                return False, checkpoint
            
            # 加载模型权重
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"? {model_type}模型加载成功")
            return True, checkpoint
            
        except Exception as e:
            print(f"? {model_type}模型加载失败: {e}")
            return False, None
    
    def load_label_encoder(self, model_type):
        """加载标签编码器"""
        encoder_filename = self.get_label_encoder_filename(model_type)
        encoder_path = os.path.join(self.model_dir, encoder_filename)
        
        if not os.path.exists(encoder_path):
            print(f"?? {model_type}标签编码器文件不存在: {encoder_path}")
            return None
        
        try:
            label_encoder = joblib.load(encoder_path)
            print(f"? {model_type}标签编码器加载成功")
            return label_encoder
        except Exception as e:
            print(f"? {model_type}标签编码器加载失败: {e}")
            return None
    
    def get_available_models(self):
        """获取可用的模型列表"""
        available_models = []
        for model_type in Config.MODEL_VERSIONS:
            model_path = os.path.join(self.model_dir, self.get_model_filename(model_type))
            if os.path.exists(model_path):
                available_models.append(model_type)
        return available_models
    
    def delete_model(self, model_type):
        """删除指定类型的模型"""
        model_path = os.path.join(self.model_dir, self.get_model_filename(model_type))
        encoder_path = os.path.join(self.model_dir, self.get_label_encoder_filename(model_type))
        
        deleted = False
        if os.path.exists(model_path):
            os.remove(model_path)
            print(f"? 删除模型文件: {model_path}")
            deleted = True
        
        if os.path.exists(encoder_path):
            os.remove(encoder_path)
            print(f"? 删除标签编码器文件: {encoder_path}")
            deleted = True
        
        if not deleted:
            print(f"?? 未找到{model_type}模型文件")
        
        return deleted

# 创建全局模型管理器实例
model_manager = ModelManager()
 
```

## model_trainer.py 
文件路径: D:\program_project\4ATS\src\model_trainer.py 

```py 
# model_trainer.py (优化版本 - 中文注释)
import os
import torch
import torch.nn as nn
import numpy as np
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from config.config import Config
from src.model_manager import model_manager

class AdvancedModelTrainer:
    """带有改进技术的PyTorch模型训练器"""
    
    def __init__(self, model, preprocessor, device, model_type='simplified'):
        self.model = model
        self.preprocessor = preprocessor
        self.device = device
        self.model_type = model_type  # 模型类型
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': [],
            'learning_rates': []
        }
        
        # 标签平滑以获得更好的泛化能力
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    def train_epoch(self, train_loader, optimizer, scheduler):
        """使用混合精度训练一个epoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for data, labels in train_loader:
            data, labels = data.to(self.device), labels.to(self.device)
            
            # 混合精度训练
            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
                outputs = self.model(data)
                loss = self.criterion(outputs, labels)
            
            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            
            # 梯度裁剪
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # 统计信息
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        if scheduler:
            scheduler.step()
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = correct / total
        
        return epoch_loss, epoch_acc
    
    def validate(self, val_loader):
        """验证模型"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, labels in val_loader:
                data, labels = data.to(self.device), labels.to(self.device)
                outputs = self.model(data)
                loss = self.criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = correct / total
        
        return epoch_loss, epoch_acc
    
    def train(self, train_loader, val_loader, epochs=Config.EPOCHS, patience=20):
        """使用先进技术训练模型 - 增加稳定性"""
        # 使用AdamW和权重衰减
        optimizer = torch.optim.AdamW(
            self.model.parameters(), 
            lr=Config.LEARNING_RATE,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # 带热重启的余弦退火
        scheduler = CosineAnnealingWarmRestarts(
            optimizer, 
            T_0=10,
            T_mult=2,
            eta_min=1e-6
        )
        
        print("开始使用先进技术训练模型...")
        best_val_acc = 0.0
        patience_counter = 0
        
        # 添加梯度累积步骤
        accumulation_steps = 4  # 每4个batch更新一次权重
        
        for epoch in range(epochs):
            try:
                # 训练阶段
                self.model.train()
                running_loss = 0.0
                correct = 0
                total = 0
                optimizer.zero_grad()
                
                for i, (data, labels) in enumerate(train_loader):
                    data, labels = data.to(self.device), labels.to(self.device)
                    
                    # 混合精度训练
                    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
                        outputs = self.model(data)
                        loss = self.criterion(outputs, labels) / accumulation_steps
                    
                    # 反向传播
                    loss.backward()
                    
                    # 每 accumulation_steps 个batch更新一次权重
                    if (i + 1) % accumulation_steps == 0:
                        # 梯度裁剪
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                        optimizer.step()
                        optimizer.zero_grad()
                        
                        if scheduler:
                            scheduler.step()
                    
                    # 统计信息
                    running_loss += loss.item() * accumulation_steps
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
                
                # 处理最后一个不完整的accumulation step
                if len(train_loader) % accumulation_steps != 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    optimizer.step()
                    optimizer.zero_grad()
                    
                    if scheduler:
                        scheduler.step()
                
                train_loss = running_loss / len(train_loader)
                train_acc = correct / total
                
                # 验证阶段
                val_loss, val_acc = self.validate(val_loader)
                
                # 记录历史
                self.history['train_loss'].append(train_loss)
                self.history['val_loss'].append(val_loss)
                self.history['train_acc'].append(train_acc)
                self.history['val_acc'].append(val_acc)
                self.history['learning_rates'].append(optimizer.param_groups[0]['lr'])
                
                print(f'周期 [{epoch+1}/{epochs}] - {self.model_type}模型')
                print(f'  训练损失: {train_loss:.4f}, 训练准确率: {train_acc:.4f}')
                print(f'  验证损失: {val_loss:.4f}, 验证准确率: {val_acc:.4f}')
                print(f'  学习率: {optimizer.param_groups[0]["lr"]:.2e}')
                
                # 早停和模型保存
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                    self.save_model()
                    print(f'  ? 保存最佳{self.model_type}模型，验证准确率: {val_acc:.4f}')
                else:
                    patience_counter += 1
                    
                if patience_counter >= patience:
                    print(f'早停: 验证准确率连续{patience}个周期未提升')
                    break
                
                # 定期清理GPU缓存
                if torch.cuda.is_available() and epoch % 10 == 0:
                    torch.cuda.empty_cache()
                    
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print("GPU内存不足，尝试清理缓存并跳过该batch")
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    continue
                else:
                    raise e
            except Exception as e:
                print(f"训练过程中出现异常: {e}")
                print("跳过该epoch，继续训练...")
                continue
        
        return self.history
    
    def evaluate(self, test_loader):
        """评估模型"""
        test_loss, test_acc = self.validate(test_loader)
        print(f"{self.model_type}模型最终测试准确率: {test_acc:.4f}")
        return test_loss, test_acc
    
    def save_model(self, model_name=None):
        """保存模型和相关文件"""
        if model_name is None:
            model_name = f"best_{self.model_type}"
        
        # 使用模型管理器保存
        model_path, encoder_path = model_manager.save_model(
            self.model, 
            self.preprocessor, 
            self.model_type,
            history=self.history,
            epoch=len(self.history['train_loss'])
        )
        
        return model_path, encoder_path

class ModelTrainer(AdvancedModelTrainer):
    """向后兼容"""
    pass
 
```

## time_series_analyzer.py 
文件路径: D:\program_project\4ATS\src\time_series_analyzer.py 

```py 
import os
import torch
import librosa
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import joblib

class TimeSeriesAnalyzer:
    """时间序列乐器分析器"""
    
    def __init__(self, model, label_encoder, device):
        self.model = model
        self.label_encoder = label_encoder
        self.device = device
        self.model.eval()
    
    def analyze_audio_timeline(self, audio_path, window_size=3.0, hop_size=1.0, threshold=0.3):
        """
        分析音频时间线，检测乐器出现的时间段
        
        Args:
            audio_path: 音频文件路径
            window_size: 分析窗口大小（秒）
            hop_size: 窗口跳跃大小（秒）
            threshold: 置信度阈值
        """
        print(f"开始分析音频时间线: {audio_path}")
        
        # 加载完整音频
        y, sr = librosa.load(audio_path, sr=22050)
        duration = len(y) / sr
        print(f"音频时长: {duration:.2f}秒, 采样率: {sr}Hz")
        
        # 计算窗口参数
        window_samples = int(window_size * sr)
        hop_samples = int(hop_size * sr)
        
        # 滑动窗口分析
        predictions = []
        timestamps = []
        
        for start in range(0, len(y) - window_samples + 1, hop_samples):
            # 提取当前窗口
            end = start + window_samples
            window_audio = y[start:end]
            timestamp = start / sr  # 当前窗口开始时间
            
            # 提取特征
            features = self._extract_features(window_audio, sr)
            if features is not None:
                # 预测
                prediction = self._predict_single_window(features)
                predictions.append(prediction)
                timestamps.append(timestamp)
            
            # 显示进度
            if len(predictions) % 10 == 0:
                progress = min(100, (end / len(y)) * 100)
                print(f"分析进度: {progress:.1f}%")
        
        print("时间序列分析完成!")
        return self._process_timeline_results(predictions, timestamps, window_size, threshold)
    
    def _extract_features(self, audio, sr):
        """提取音频特征"""
        try:
            # 提取Mel频谱图
            mel_spec = librosa.feature.melspectrogram(
                y=audio, sr=sr, n_mels=128, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # 标准化
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            # 确保特征尺寸一致
            target_frames = 130  # 与训练时一致
            if log_mel.shape[1] < target_frames:
                # 填充
                pad_width = target_frames - log_mel.shape[1]
                log_mel = np.pad(log_mel, ((0, 0), (0, pad_width)), mode='constant')
            elif log_mel.shape[1] > target_frames:
                # 截断
                log_mel = log_mel[:, :target_frames]
            
            return log_mel
            
        except Exception as e:
            print(f"特征提取错误: {e}")
            return None
    
    def _predict_single_window(self, features):
        """预测单个窗口"""
        # 转换为模型输入格式
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)  # (1, 1, 128, 130)
        input_tensor = input_tensor.to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
        
        # 获取所有类别的概率
        probs = probabilities.cpu().numpy()[0]
        results = {}
        
        for idx, prob in enumerate(probs):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            results[instrument] = float(prob)
        
        return results
    
    def _process_timeline_results(self, predictions, timestamps, window_size, threshold):
        """处理时间线结果，合并连续的时间段"""
        timeline = {}
        
        # 为每个乐器初始化时间线
        for instrument in self.label_encoder.classes_:
            timeline[instrument] = {
                'segments': [],
                'total_duration': 0.0,
                'max_confidence': 0.0,
                'average_confidence': 0.0
            }
        
        # 分析每个时间点的预测
        for i, (timestamp, pred_dict) in enumerate(zip(timestamps, predictions)):
            # 找到当前窗口最可能的乐器
            if pred_dict:
                best_instrument = max(pred_dict.items(), key=lambda x: x[1])
                instrument, confidence = best_instrument
                
                # 只记录置信度高于阈值的预测
                if confidence >= threshold:
                    timeline[instrument]['segments'].append({
                        'start': timestamp,
                        'end': timestamp + window_size,
                        'confidence': confidence
                    })
        
        # 合并连续的时间段并计算统计信息
        for instrument in timeline.keys():
            segments = timeline[instrument]['segments']
            if segments:
                # 按开始时间排序
                segments.sort(key=lambda x: x['start'])
                
                # 合并连续的时间段
                merged_segments = []
                current_segment = segments[0]
                
                for segment in segments[1:]:
                    if segment['start'] <= current_segment['end'] + 0.5:  # 允许0.5秒间隙
                        # 时间段连续，合并
                        current_segment['end'] = max(current_segment['end'], segment['end'])
                        current_segment['confidence'] = max(current_segment['confidence'], segment['confidence'])
                    else:
                        # 时间段不连续，保存当前段并开始新段
                        merged_segments.append(current_segment)
                        current_segment = segment
                
                merged_segments.append(current_segment)
                
                # 更新timeline
                timeline[instrument]['segments'] = merged_segments
                
                # 计算统计信息
                total_duration = sum(seg['end'] - seg['start'] for seg in merged_segments)
                confidences = [seg['confidence'] for seg in merged_segments]
                
                timeline[instrument]['total_duration'] = total_duration
                timeline[instrument]['max_confidence'] = max(confidences) if confidences else 0
                timeline[instrument]['average_confidence'] = np.mean(confidences) if confidences else 0
        
        return timeline
    
    def visualize_timeline(self, timeline, audio_duration, save_path=None):
        """可视化时间线结果"""
        # 乐器名称映射字典
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        fig, ax = plt.subplots(figsize=(14, 8))
        
        # 使用英文名称
        english_instruments = [instrument_names.get(instr, instr) for instr in timeline.keys()]
        colors = plt.cm.Set3(np.linspace(0, 1, len(english_instruments)))
        
        # 为每个乐器创建时间线
        for i, (instrument, data) in enumerate(timeline.items()):
            english_name = instrument_names.get(instrument, instrument)
            segments = data['segments']
            if segments:
                for segment in segments:
                    # 绘制时间段矩形
                    rect = Rectangle(
                        (segment['start'], i - 0.4),
                        segment['end'] - segment['start'],
                        0.8,
                        facecolor=colors[i],
                        alpha=0.7,
                        edgecolor='black',
                        linewidth=1
                    )
                    ax.add_patch(rect)
                    
                    # 添加置信度文本
                    if segment['end'] - segment['start'] > 2:  # 只在不小的段上添加文本
                        ax.text(
                            (segment['start'] + segment['end']) / 2,
                            i,
                            f'{segment["confidence"]:.2f}',
                            ha='center',
                            va='center',
                            fontsize=8,
                            fontweight='bold'
                        )
        
        # 设置坐标轴
        ax.set_xlim(0, audio_duration)
        ax.set_ylim(-0.5, len(english_instruments) - 0.5)
        ax.set_xlabel('Time (seconds)')
        ax.set_ylabel('Instrument')
        ax.set_title('Instrument Timeline Analysis')
        
        # 设置y轴刻度（使用英文名称）
        ax.set_yticks(range(len(english_instruments)))
        ax.set_yticklabels(english_instruments)
        
        # 添加网格
        ax.grid(True, alpha=0.3)
        
        # 添加图例
        legend_elements = []
        for i, (instrument, data) in enumerate(timeline.items()):
            english_name = instrument_names.get(instrument, instrument)
            if data['segments']:
                legend_elements.append(
                    plt.Rectangle((0, 0), 1, 1, facecolor=colors[i], alpha=0.7, 
                                label=f"{english_name} ({data['total_duration']:.1f}s)")
                )
        
        if legend_elements:
            ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.15, 1))
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Timeline chart saved: {save_path}")
        
        plt.show()
        
        return fig

    def generate_report(self, timeline, audio_duration):
        """生成分析报告"""
        # 乐器名称映射字典
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        print("\n" + "="*60)
        print("               Instrument Timeline Analysis Report")
        print("="*60)
        
        # 统计活跃乐器
        active_instruments = []
        for instrument, data in timeline.items():
            if data['segments']:
                english_name = instrument_names.get(instrument, instrument)
                active_instruments.append((english_name, data['total_duration'], instrument))
        
        # 按持续时间排序
        active_instruments.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\n?? Active Instruments Statistics ({len(active_instruments)} instruments):")
        print("-" * 60)
        
        for english_name, duration, original_name in active_instruments:
            percentage = (duration / audio_duration) * 100
            max_conf = timeline[original_name]['max_confidence']
            avg_conf = timeline[original_name]['average_confidence']
            segment_count = len(timeline[original_name]['segments'])
            
            print(f"?? {english_name:15s} | {duration:6.1f}s ({percentage:5.1f}%) | "
                f"Max Confidence: {max_conf:.3f} | Segments: {segment_count}")
        
        print(f"\n?? Detailed Time Segments:")
        print("-" * 50)
        
        for english_name, duration, original_name in active_instruments:
            segments = timeline[original_name]['segments']
            
            print(f"\n{english_name}:")
            for i, segment in enumerate(segments, 1):
                print(f"  Segment {i}: {segment['start']:6.1f}s - {segment['end']:6.1f}s "
                    f"(Confidence: {segment['confidence']:.3f})")
        
        return active_instruments
 
```

## train_enhanced.py 
文件路径: D:\program_project\4ATS\src\train_enhanced.py 

```py 
# train_enhanced.py (简化版本 - 中文注释)
import os
import sys

# 添加项目根目录到路径
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

if __name__ == "__main__":
    # 直接运行主程序，使用默认参数
    from main import main
    main()
 
```

## utils.py 
文件路径: D:\program_project\4ATS\src\utils.py 

```py 
# utils.py (更新后的绘图函数)
import os
import urllib.request
import zipfile
import numpy as np
import matplotlib.pyplot as plt
import torch
from config.config import Config

def download_dataset():
    """Download dataset"""
    dataset_path = os.path.join(Config.DATA_DIR, "irmas.zip")
    extract_path = Config.DATA_DIR
    
    if not os.path.exists(os.path.join(extract_path, Config.DATASET_NAME)):
        print("Downloading dataset...")
        urllib.request.urlretrieve(Config.DATASET_URL, dataset_path)
        
        # Extract files
        print("Extracting dataset...")
        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        
        print("Dataset download completed!")
    else:
        print("Dataset already exists!")

def plot_training_history(train_losses, val_losses, train_accs, val_accs, save_path=None, show_plot=False):
    """Plot training history charts - 修改为不自动显示"""
    plt.figure(figsize=(12, 4))
    
    # Accuracy chart
    plt.subplot(1, 2, 1)
    plt.plot(train_accs, label='Training Accuracy')
    plt.plot(val_accs, label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Loss chart
    plt.subplot(1, 2, 2)
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, bbox_inches='tight', dpi=300, 
                   facecolor='white', edgecolor='none')
        print(f"Training chart saved to: {save_path}")
    
    # 只在明确要求时显示
    if show_plot:
        plt.show()
    else:
        plt.close()  # 关闭图表，释放内存

def analyze_model_performance(model, test_loader, label_encoder, device, save_dir):
    """Analyze model performance"""
    from sklearn.metrics import classification_report, confusion_matrix
    import seaborn as sns
    
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for data, labels in test_loader:
            data, labels = data.to(device), labels.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # Classification report
    print("Classification Report:")
    print(classification_report(all_labels, all_preds, 
                              target_names=label_encoder.classes_))
    
    # Confusion matrix
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(all_labels, all_preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    confusion_matrix_path = os.path.join(save_dir, 'confusion_matrix.png')
    plt.savefig(confusion_matrix_path)
    print(f"Confusion matrix saved to: {confusion_matrix_path}")
    plt.show()
    
    # Calculate accuracy for each class
    print("\nClass-wise Accuracy:")
    class_accuracy = {}
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)
    
    for i, class_name in enumerate(label_encoder.classes_):
        mask = all_labels == i
        if np.sum(mask) > 0:
            acc = np.mean(all_preds[mask] == all_labels[mask])
            class_accuracy[class_name] = acc
            print(f"  {class_name}: {acc:.3f}")
    
    return class_accuracy
 
```

## __init__.py 
文件路径: D:\program_project\4ATS\src\__init__.py 

```py 
 
```

# test 

## batch_music_tester.py 
文件路径: D:\program_project\4ATS\test\batch_music_tester.py 

```py 
import os
import sys
import torch
import argparse
import time

# 添加项目根目录到Python路径 :cite[1]:cite[2]
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # 向上两级到项目根目录
sys.path.insert(0, project_root) # 确保Python在导入模块时能搜索到项目根目录 :cite[1]

from config.config import Config
# 注意：如果inst_model_test.py也在test文件夹中，可能需要调整导入方式
# 如果inst_model_test.py在src文件夹中，应使用：
from inst_model_test import MultiModelTester

class BatchMusicTester:
    def __init__(self, model_type='simplified'):
        self.model_type = model_type
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"使用设备: {self.device}")
        print(f"测试模型: {model_type}")
        
        # 创建输出目录
        self.batch_output_dir = os.path.join(Config.OUTPUT_DIR, "batch_test_results")
        os.makedirs(self.batch_output_dir, exist_ok=True)
    
    def load_music_list(self, music_list_file):
        """从music.txt加载音乐文件列表"""
        # 音乐列表文件现在位于项目根目录
        music_list_path = os.path.join(project_root, music_list_file) # 从项目根目录查找
        
        if not os.path.exists(music_list_path):
            print(f"错误: 音乐列表文件不存在 - {music_list_path}")
            return []
        
        with open(music_list_path, 'r', encoding='utf-8') as f:
            music_files = [line.strip() for line in f if line.strip()]
        
        # 构建完整的文件路径 - music文件夹在项目根目录
        music_dir = os.path.join(project_root, "music") # 从项目根目录查找music文件夹
        full_paths = []
        
        for music_file in music_files:
            full_path = os.path.join(music_dir, music_file)
            if os.path.exists(full_path):
                full_paths.append(full_path)
            else:
                print(f"警告: 音乐文件不存在 - {full_path}")
        
        print(f"找到 {len(full_paths)} 个有效的音乐文件")
        return full_paths
    
    def test_batch_music(self, music_list_file="music.txt"):
        """批量测试音乐文件"""
        music_files = self.load_music_list(music_list_file)
        
        if not music_files:
            print("没有找到可测试的音乐文件")
            return
        
        print(f"\n开始批量测试 {len(music_files)} 个音乐文件...")
        print("=" * 60)
        
        results = []
        
        for i, music_path in enumerate(music_files, 1):
            print(f"\n[{i}/{len(music_files)}] 测试: {os.path.basename(music_path)}")
            
            try:
                # 创建测试器
                tester = MultiModelTester(self.model_type)
                if tester.model is None:
                    print(f"  ? 模型加载失败，跳过")
                    continue
                
                # 测试单个文件
                start_time = time.time()
                predictions, features = tester.predict_single_audio(music_path)
                test_time = time.time() - start_time
                
                if predictions:
                    # 记录结果
                    top_prediction = predictions[0]
                    result = {
                        'file_name': os.path.basename(music_path),
                        'top_instrument': top_prediction['instrument'],
                        'top_confidence': top_prediction['probability'],
                        'test_time': test_time,
                        'all_predictions': predictions
                    }
                    results.append(result)
                    
                    # 输出结果
                    instrument_name = tester.get_english_name(top_prediction['instrument'])
                    print(f"  ? 最可能乐器: {instrument_name} (置信度: {top_prediction['probability']:.3f})")
                    print(f"  测试耗时: {test_time:.2f}秒")
                    
                    # 保存可视化结果（可选）
                    audio_name = os.path.splitext(os.path.basename(music_path))[0]
                    output_path = os.path.join(self.batch_output_dir, 
                                             f"batch_{audio_name}_{self.model_type}.png")
                    tester.visualize_prediction(predictions, features, audio_name)
                    
                else:
                    print(f"  ? 预测失败")
                    
            except Exception as e:
                print(f"  ? 测试过程中出错: {e}")
                import traceback
                traceback.print_exc()
        
        # 生成批量测试报告
        self.generate_batch_report(results)
    
    def generate_batch_report(self, results):
        """生成批量测试报告"""
        if not results:
            print("\n没有有效的测试结果")
            return
        
        report_path = os.path.join(self.batch_output_dir, f"batch_test_report_{self.model_type}.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("批量音乐测试报告\n")
            f.write("=" * 50 + "\n")
            f.write(f"测试时间: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"测试模型: {self.model_type}\n")
            f.write(f"测试文件数: {len(results)}\n\n")
            
            f.write("详细结果:\n")
            f.write("-" * 80 + "\n")
            f.write(f"{'文件名':<20} {'最可能乐器':<15} {'置信度':<10} {'测试时间':<10}\n")
            f.write("-" * 80 + "\n")
            
            total_time = 0
            for result in results:
                instrument_name = self.get_english_name(result['top_instrument'])
                f.write(f"{result['file_name']:<20} {instrument_name:<15} {result['top_confidence']:<10.3f} {result['test_time']:<10.2f}\n")
                total_time += result['test_time']
            
            f.write(f"\n总测试时间: {total_time:.2f}秒\n")
            f.write(f"平均测试时间: {total_time/len(results):.2f}秒/文件\n")
        
        print(f"\n? 批量测试报告已保存: {report_path}")
        
        # 在控制台也输出总结
        print(f"\n批量测试总结:")
        print(f"总文件数: {len(results)}")
        print(f"总测试时间: {total_time:.2f}秒")
        print(f"平均测试时间: {total_time/len(results):.2f}秒/文件")
    
    def get_english_name(self, abbreviation):
        """获取乐器英文名称"""
        from src.instrument_mapper import InstrumentMapper
        return InstrumentMapper.get_english_name(abbreviation)

def main():
    """主函数"""
    parser = argparse.ArgumentParser(description='批量音乐测试')
    parser.add_argument('--model-type', type=str, default='simplified', 
                       choices=['basic', 'simplified', 'advanced'],
                       help='使用的模型类型')
    parser.add_argument('--music-list', type=str, default='music.txt',
                       help='音乐列表文件路径')
    
    args = parser.parse_args()
    
    print("=== AI音频分析系统 - 批量音乐测试 ===")
    
    # 初始化配置
    Config.create_directories()
    
    # 创建批量测试器
    tester = BatchMusicTester(args.model_type)
    
    # 执行批量测试
    tester.test_batch_music(args.music_list)

if __name__ == "__main__":
    main()
 
```

## batch_test_all_models.py 
文件路径: D:\program_project\4ATS\test\batch_test_all_models.py 

```py 
import os
import sys
import torch
import argparse
import time

# 添加项目根目录到Python路径
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.config import Config
from config.music_file_loader import load_music_files
from inst_model_test import MultiModelTester

class BatchAllModelsTester:
    """批量测试所有模型"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_types = ['basic', 'simplified', 'advanced']
        
        # 创建输出目录
        self.batch_output_dir = os.path.join(Config.OUTPUT_DIR, "batch_all_models_results")
        os.makedirs(self.batch_output_dir, exist_ok=True)
    
    def test_all_models_on_music(self):
        """在所有音乐文件上测试所有模型"""
        # 加载音乐文件
        music_files = load_music_files()
        if not music_files:
            print("没有找到可测试的音乐文件")
            return
        
        print(f"\n开始批量测试 {len(music_files)} 个音乐文件，使用 {len(self.model_types)} 个模型...")
        print("=" * 70)
        
        all_results = {}
        
        for model_type in self.model_types:
            print(f"\n?? 测试 {model_type} 模型:")
            print("-" * 50)
            
            model_results = []
            for i, music_path in enumerate(music_files, 1):
                print(f"  [{i}/{len(music_files)}] {os.path.basename(music_path)}")
                
                try:
                    tester = MultiModelTester(model_type)
                    if tester.model is None:
                        print(f"    ? 模型加载失败，跳过")
                        continue
                    
                    start_time = time.time()
                    predictions, _ = tester.predict_single_audio(music_path)
                    test_time = time.time() - start_time
                    
                    if predictions:
                        top_pred = predictions[0]
                        result = {
                            'file_name': os.path.basename(music_path),
                            'top_instrument': top_pred['instrument'],
                            'top_confidence': top_pred['probability'],
                            'test_time': test_time
                        }
                        model_results.append(result)
                        
                        instrument_name = tester.get_english_name(top_pred['instrument'])
                        print(f"    ? {instrument_name} (置信度: {top_pred['probability']:.3f}, 耗时: {test_time:.2f}s)")
                    else:
                        print(f"    ? 预测失败")
                        
                except Exception as e:
                    print(f"    ? 测试过程中出错: {e}")
            
            all_results[model_type] = model_results
        
        # 生成报告
        self.generate_comprehensive_report(all_results, music_files)
    
    def generate_comprehensive_report(self, all_results, music_files):
        """生成综合报告"""
        report_path = os.path.join(self.batch_output_dir, "batch_all_models_report.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("多模型批量测试报告\n")
            f.write("=" * 60 + "\n")
            f.write(f"测试时间: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"测试文件数: {len(music_files)}\n")
            f.write(f"测试模型: {', '.join(self.model_types)}\n\n")
            
            # 各模型结果
            for model_type, results in all_results.items():
                f.write(f"{model_type.upper()} 模型结果:\n")
                f.write("-" * 50 + "\n")
                
                if results:
                    for result in results:
                        f.write(f"  {result['file_name']}: {result['top_instrument']} (置信度: {result['top_confidence']:.3f})\n")
                else:
                    f.write("  无有效结果\n")
                f.write("\n")
            
            # 统计信息
            f.write("统计信息:\n")
            f.write("-" * 30 + "\n")
            for model_type, results in all_results.items():
                f.write(f"{model_type}: {len(results)}/{len(music_files)} 文件测试成功\n")
        
        print(f"\n? 综合测试报告已保存: {report_path}")

def main():
    """主函数"""
    parser = argparse.ArgumentParser(description='批量测试所有模型')
    
    args = parser.parse_args()
    
    print("=== AI音频分析系统 - 多模型批量测试 ===")
    
    # 初始化配置
    Config.create_directories()
    
    # 创建测试器并执行测试
    tester = BatchAllModelsTester()
    tester.test_all_models_on_music()

if __name__ == "__main__":
    main()
 
```

## bpm_test.py 
文件路径: D:\program_project\4ATS\test\bpm_test.py 

```py 
from pathlib import Path
import librosa

# 获取当前文件所在目录
current_file = Path(__file__)
# 获取music文件夹路径
music_dir = current_file.parent.parent / 'music'

def get_bpm_from_music_folder(filename):
    audio_path = music_dir / filename
    
    if not audio_path.exists():
        print(f"文件不存在: {audio_path}")
        return None
    
    y, sr = librosa.load(str(audio_path))
    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
    print(tempo)
    return tempo

# 使用示例
bpm = get_bpm_from_music_folder("2.flac")
# bpm = get_bpm_from_music_folder("1.mp3")
 
```

## diagnose_discrepancy.py 
文件路径: D:\program_project\4ATS\test\diagnose_discrepancy.py 

```py 
import os
import sys
import torch
import librosa
import numpy as np
import joblib
import matplotlib.pyplot as plt
import argparse

# 添加项目根目录到Python路径
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.config import Config
from config.music_file_loader import load_music_files
from src.instrument_mapper import InstrumentMapper

def diagnose_analysis_batch():
    """批量诊断分析不一致问题"""
    # 加载音乐文件
    music_files = load_music_files()
    if not music_files:
        print("没有找到可诊断的音乐文件")
        return
    
    print(f"开始批量诊断分析 {len(music_files)} 个音乐文件...")
    
    # 设置设备
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    for i, audio_path in enumerate(music_files, 1):
        print(f"\n{'='*60}")
        print(f"[{i}/{len(music_files)}] 诊断分析: {os.path.basename(audio_path)}")
        print(f"{'='*60}")
        
        # 这里可以调用原来的诊断函数，但需要调整以支持批量处理
        # diagnose_single_analysis(audio_path, device)
        
        # 简化版的诊断分析
        try:
            # 加载第一个可用模型进行诊断
            model_type = 'simplified'
            model_path = os.path.join(project_root, "model", f"model_{model_type}.pth")
            label_encoder_path = os.path.join(project_root, "model", f"model_{model_type}_label_encoder.pkl")
            
            if os.path.exists(model_path) and os.path.exists(label_encoder_path):
                diagnose_single_analysis(audio_path, model_path, label_encoder_path, device)
            else:
                print(f"模型文件不存在，跳过诊断")
                
        except Exception as e:
            print(f"诊断过程中出错: {e}")

def diagnose_single_analysis(audio_path, model_path, label_encoder_path, device):
    """诊断单个文件的分析不一致问题"""
    # 加载模型和标签编码器
    label_encoder = joblib.load(label_encoder_path)
    checkpoint = torch.load(model_path, map_location=device)
    
    from src.model_builder import ImprovedInstrumentClassifier
    model = ImprovedInstrumentClassifier((1, 128, 130), len(label_encoder.classes_))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    # 加载音频
    y, sr = librosa.load(audio_path, sr=22050)
    duration = len(y) / sr
    print(f"音频时长: {duration:.2f}秒")
    
    # 测试1: 整体分析
    print("\n1. 整体分析（3秒片段）:")
    test_overall_analysis(y, sr, model, label_encoder, device)
    
    # 测试2: 多个窗口分析
    print("\n2. 多窗口分析:")
    test_multiple_windows(y, sr, model, label_encoder, device)
    
    # 测试3: 置信度分布
    print("\n3. 置信度分布分析:")
    test_confidence_distribution(y, sr, model, label_encoder, device)

# 保留原有的测试函数（test_overall_analysis, test_multiple_windows等）
def test_overall_analysis(y, sr, model, label_encoder, device):
    """测试整体分析"""
    segment = y[:3*sr] if len(y) > 3*sr else y
    features = extract_features(segment, sr)
    if features is not None:
        probs = predict_single_window(features, model, label_encoder, device)
        print_top_predictions(probs, label_encoder, "整体分析")

def test_multiple_windows(y, sr, model, label_encoder, device):
    """测试多个窗口"""
    window_size = 3.0
    hop_size = 1.0
    window_samples = int(window_size * sr)
    hop_samples = int(hop_size * sr)
    
    all_predictions = []
    
    for start in range(0, min(len(y) - window_samples, 10 * hop_samples), hop_samples):
        end = start + window_samples
        segment = y[start:end]
        features = extract_features(segment, sr)
        if features is not None:
            probs = predict_single_window(features, model, label_encoder, device)
            all_predictions.append(probs)
    
    if all_predictions:
        avg_probs = np.mean(all_predictions, axis=0)
        print_top_predictions(avg_probs, label_encoder, "多窗口平均")

def test_confidence_distribution(y, sr, model, label_encoder, device):
    """测试置信度分布"""
    window_size = 3.0
    hop_size = 1.0
    window_samples = int(window_size * sr)
    hop_samples = int(hop_size * sr)
    
    instrument_confidences = {inst: [] for inst in label_encoder.classes_}
    
    for start in range(0, min(len(y) - window_samples, 20 * hop_samples), hop_samples):
        end = start + window_samples
        segment = y[start:end]
        features = extract_features(segment, sr)
        if features is not None:
            probs = predict_single_window(features, model, label_encoder, device)
            for idx, prob in enumerate(probs):
                instrument = label_encoder.inverse_transform([idx])[0]
                instrument_confidences[instrument].append(prob)
    
    print("\n置信度统计:")
    for instrument, confidences in instrument_confidences.items():
        if confidences:
            english_name = InstrumentMapper.get_english_name(instrument)
            avg_conf = np.mean(confidences)
            max_conf = np.max(confidences)
            print(f"  {english_name:15s}: 平均{avg_conf:.3f}, 最大{max_conf:.3f}")

def extract_features(audio, sr, target_shape=(128, 130)):
    """提取特征"""
    try:
        mel_spec = librosa.feature.melspectrogram(
            y=audio, sr=sr, n_mels=128, fmax=8000, 
            n_fft=2048, hop_length=512
        )
        log_mel = librosa.power_to_db(mel_spec)
        log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
        
        if log_mel.shape[1] < target_shape[1]:
            log_mel = np.pad(log_mel, ((0, 0), (0, target_shape[1] - log_mel.shape[1])), mode='constant')
        else:
            log_mel = log_mel[:, :target_shape[1]]
            
        return log_mel
    except Exception as e:
        print(f"特征提取错误: {e}")
        return None

def predict_single_window(features, model, label_encoder, device):
    """预测单个窗口"""
    input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0).to(device)
    
    with torch.no_grad():
        outputs = model(input_tensor)
        probabilities = torch.softmax(outputs, dim=1)
    
    return probabilities.cpu().numpy()[0]

def print_top_predictions(probs, label_encoder, title):
    """打印前3个预测"""
    top_indices = np.argsort(probs)[-3:][::-1]
    
    print(f"\n{title}:")
    for i, idx in enumerate(top_indices):
        instrument = label_encoder.inverse_transform([idx])[0]
        english_name = InstrumentMapper.get_english_name(instrument)
        prob = probs[idx]
        print(f"  {i+1}. {english_name:15s}: {prob:.3f}")

def main():
    """主诊断函数"""
    parser = argparse.ArgumentParser(description='批量诊断分析')
    
    args = parser.parse_args()
    
    print("=== 批量分析不一致诊断 ===")
    
    # 初始化配置
    Config.create_directories()
    
    # 执行批量诊断
    diagnose_analysis_batch()

if __name__ == "__main__":
    main()
 
```

## inst_model_test.py 
文件路径: D:\program_project\4ATS\test\inst_model_test.py 

```py 
import os
import sys
import torch
import librosa
import numpy as np
import joblib
import matplotlib.pyplot as plt
import argparse

# 添加项目根目录到Python路径
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.config import Config
from config.music_file_loader import load_music_files  # 新增导入
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier
from src.instrument_mapper import InstrumentMapper

class MultiModelTester:
    """多模型测试类"""
    
    def __init__(self, model_type='simplified'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_type = model_type
        print(f"使用设备: {self.device}")
        print(f"测试模型: {model_type}")
        
        # 加载标签编码器
        label_encoder_path = os.path.join(project_root, "model", f"model_{model_type}_label_encoder.pkl")
        if not os.path.exists(label_encoder_path):
            print(f"错误: 标签编码器文件不存在 - {label_encoder_path}")
            return
        
        self.label_encoder = joblib.load(label_encoder_path)
        print(f"标签编码器加载成功，类别: {list(self.label_encoder.classes_)}")
        
        # 加载模型
        self.model = self.load_model(model_type)
        if self.model:
            self.model.eval()  # 设置为评估模式
            print(f"{model_type}模型加载成功!")
    
    def load_model(self, model_type):
        """加载指定类型的模型"""
        model_path = os.path.join(project_root, "model", f"model_{model_type}.pth")
        
        if not os.path.exists(model_path):
            print(f"错误: 模型文件不存在 - {model_path}")
            return None
        
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # 获取模型参数
            input_shape = checkpoint.get('input_shape', (1, 128, 130))
            num_classes = checkpoint.get('num_classes', len(self.label_encoder.classes_))
            
            print(f"模型参数 - 输入形状: {input_shape}, 类别数: {num_classes}")
            
            # 根据模型类型创建对应的模型实例
            if model_type == 'basic':
                model = create_improved_classifier(input_shape, num_classes)
            elif model_type == 'advanced':
                model = create_advanced_classifier(input_shape, num_classes)
            elif model_type == 'simplified':
                model = create_simplified_classifier(input_shape, num_classes)
            else:
                print(f"未知的模型类型: {model_type}")
                return None
            
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(self.device)
            
            return model
            
        except Exception as e:
            print(f"模型加载失败: {e}")
            return None
    
    def preprocess_audio(self, audio_path):
        """预处理音频文件"""
        try:
            # 加载音频
            y, sr = librosa.load(audio_path, sr=Config.TARGET_SAMPLE_RATE)
            
            # 确保音频长度一致
            y = librosa.util.fix_length(y, size=Config.TARGET_SAMPLE_RATE * Config.AUDIO_DURATION)
            
            # 提取Mel频谱图
            mel_spec = librosa.feature.melspectrogram(
                y=y, sr=sr, n_mels=Config.N_MELS, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # 标准化
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            return log_mel
            
        except Exception as e:
            print(f"处理音频 {audio_path} 时出错: {e}")
            return None
    
    def predict_single_audio(self, audio_path):
        """预测单个音频文件的乐器种类"""
        if self.model is None:
            print("模型未正确加载")
            return None, None
            
        print(f"\n分析音频文件: {audio_path}")
        
        # 预处理音频
        features = self.preprocess_audio(audio_path)
        if features is None:
            return None, None
        
        # 转换为模型输入格式
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)  # (1, 1, 128, 130)
        input_tensor = input_tensor.to(self.device)
        print(f"输入张量形状: {input_tensor.shape}")
        
        # 预测
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            top_probs, top_indices = torch.topk(probabilities, k=3)  # 获取前3个预测
        
        # 转换为numpy
        top_probs = top_probs.cpu().numpy()[0]
        top_indices = top_indices.cpu().numpy()[0]
        
        # 解码预测结果
        predictions = []
        for i, (idx, prob) in enumerate(zip(top_indices, top_probs)):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            predictions.append({
                'rank': i+1,
                'instrument': instrument,
                'probability': prob
            })
        
        return predictions, features
    
    def visualize_prediction(self, predictions, features, audio_name):
        """可视化预测结果"""
        # 乐器名称映射字典
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        plt.figure(figsize=(12, 5))
        
        # 绘制Mel频谱图
        plt.subplot(1, 2, 1)
        librosa.display.specshow(features, sr=Config.TARGET_SAMPLE_RATE, 
                                hop_length=512, x_axis='time', y_axis='mel')
        plt.colorbar(format='%+2.0f dB')
        plt.title(f'Mel Spectrogram - {audio_name}\n({self.model_type}模型)')
        
        # 绘制预测概率
        plt.subplot(1, 2, 2)
        # 将缩写转换为英文名称
        instruments = [instrument_names.get(p['instrument'], p['instrument']) for p in predictions]
        probabilities = [p['probability'] for p in predictions]
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(predictions)))
        bars = plt.bar(instruments, probabilities, color=colors)
        
        # 在柱状图上添加数值标签
        for bar, prob in zip(bars, probabilities):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{prob:.3f}', ha='center', va='bottom')
        
        plt.title(f'{self.model_type}模型预测结果')
        plt.xlabel('Instrument')
        plt.ylabel('Probability')
        plt.ylim(0, 1)
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # 保存结果
        output_path = os.path.join(Config.OUTPUT_DIR, f"prediction_{audio_name}_{self.model_type}.png")
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"预测结果图已保存: {output_path}")
        
        plt.show()

def test_all_models(audio_path):
    """测试所有三个模型"""
    model_types = ['basic', 'simplified', 'advanced']
    all_results = {}
    
    for model_type in model_types:
        print(f"\n{'='*50}")
        print(f"测试 {model_type} 模型")
        print(f"{'='*50}")
        
        tester = MultiModelTester(model_type)
        if tester.model is None:
            print(f"跳过 {model_type} 模型（加载失败）")
            continue
            
        predictions, features = tester.predict_single_audio(audio_path)
        
        if predictions:
            print(f"\n=== {model_type}模型预测结果 ===")
            for pred in predictions:
                instrument_name = InstrumentMapper.get_english_name(pred['instrument'])
                print(f"{pred['rank']}. {instrument_name}: {pred['probability']:.3f}")
            
            # 可视化结果
            audio_name = os.path.splitext(os.path.basename(audio_path))[0]
            tester.visualize_prediction(predictions, features, audio_name)
            
            # 存储结果
            all_results[model_type] = {
                'top_prediction': predictions[0],
                'all_predictions': predictions
            }
    
    return all_results

def main():
    """主测试函数"""
    parser = argparse.ArgumentParser(description='多模型测试')
    parser.add_argument('--model-type', type=str, default='all', 
                       choices=['basic', 'simplified', 'advanced', 'all'],
                       help='要测试的模型类型')
    parser.add_argument('--audio-path', type=str, 
                       default=None,  # 修改为None，使用music.txt
                       help='测试音频路径（如未指定则使用music.txt中的文件）')
    parser.add_argument('--batch', action='store_true',
                       help='批量测试music.txt中的所有文件')
    
    args = parser.parse_args()
    
    print("=== AI音频分析与自动扒谱系统 - 多模型测试 ===")
    
    # 1. 初始化配置
    Config.create_directories()
    
    if args.batch or args.audio_path is None:
        # 批量测试模式
        music_files = load_music_files()
        if not music_files:
            print("错误: 没有找到可测试的音乐文件")
            return
        
        print(f"批量测试 {len(music_files)} 个音乐文件...")
        
        for music_path in music_files:
            print(f"\n测试文件: {os.path.basename(music_path)}")
            
            if args.model_type == 'all':
                test_all_models(music_path)
            else:
                tester = MultiModelTester(args.model_type)
                if tester.model is None:
                    continue
                    
                predictions, features = tester.predict_single_audio(music_path)
                
                if predictions:
                    print(f"\n=== {args.model_type}模型预测结果 ===")
                    for pred in predictions:
                        instrument_name = InstrumentMapper.get_english_name(pred['instrument'])
                        print(f"{pred['rank']}. {instrument_name}: {pred['probability']:.3f}")
                    
                    # 可视化结果
                    audio_name = os.path.splitext(os.path.basename(music_path))[0]
                    tester.visualize_prediction(predictions, features, audio_name)
    else:
        # 单个文件测试模式（原有逻辑）
        if not os.path.exists(args.audio_path):
            print(f"错误: 测试音频文件不存在 - {args.audio_path}")
            print("请确保音频文件存在")
            return
        
        if args.model_type == 'all':
            test_all_models(args.audio_path)
        else:
            tester = MultiModelTester(args.model_type)
            if tester.model is None:
                return
                
            predictions, features = tester.predict_single_audio(args.audio_path)
            
            if predictions:
                print(f"\n=== {args.model_type}模型预测结果 ===")
                for pred in predictions:
                    instrument_name = InstrumentMapper.get_english_name(pred['instrument'])
                    print(f"{pred['rank']}. {instrument_name}: {pred['probability']:.3f}")
                
                # 可视化结果
                audio_name = os.path.splitext(os.path.basename(args.audio_path))[0]
                tester.visualize_prediction(predictions, features, audio_name)
                
if __name__ == "__main__":
    main()
 
```

## model_evaluation.py 
文件路径: D:\program_project\4ATS\test\model_evaluation.py 

```py 
import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_curve, auc
from sklearn.preprocessing import label_binarize
import argparse

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.config import Config
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier
from src.audio_preprocessor import AudioDataPreprocessor
from src.instrument_mapper import InstrumentMapper

class MultiModelEvaluator:
    """多模型评估器"""
    
    def __init__(self, model, label_encoder, device, model_type):
        self.model = model
        self.label_encoder = label_encoder
        self.device = device
        self.model_type = model_type
        self.model.eval()
    
    def comprehensive_evaluation(self, test_loader):
        """全面评估模型"""
        print(f"\n=== {self.model_type}模型全面评估 ===")
        
        # 1. 基础指标
        accuracy, class_accuracy = self.calculate_accuracy(test_loader)
        
        # 2. 详细分类报告
        y_true, y_pred, y_prob = self.get_predictions(test_loader)
        self.print_classification_report(y_true, y_pred)
        
        # 3. 混淆矩阵
        self.plot_confusion_matrix(y_true, y_pred)
        
        # 4. ROC曲线和AUC
        self.plot_roc_curves(y_true, y_prob)
        
        # 5. 各类别性能分析
        self.analyze_class_performance(y_true, y_pred, y_prob)
        
        return {
            'accuracy': accuracy,
            'class_accuracy': class_accuracy,
            'y_true': y_true,
            'y_pred': y_pred,
            'y_prob': y_prob
        }
    
    def calculate_accuracy(self, test_loader):
        """计算准确率"""
        self.model.eval()
        correct = 0
        total = 0
        class_correct = {i: 0 for i in range(len(self.label_encoder.classes_))}
        class_total = {i: 0 for i in range(len(self.label_encoder.classes_))}
        
        with torch.no_grad():
            for data, labels in test_loader:
                data, labels = data.to(self.device), labels.to(self.device)
                outputs = self.model(data)
                _, predicted = torch.max(outputs.data, 1)
                
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                # 各类别准确率
                for i in range(labels.size(0)):
                    label = labels[i].item()
                    pred = predicted[i].item()
                    class_total[label] += 1
                    if label == pred:
                        class_correct[label] += 1
        
        accuracy = 100 * correct / total
        class_accuracy = {}
        
        print(f"\n?? {self.model_type}模型总体准确率: {accuracy:.2f}%")
        print(f"\n?? 各类别准确率:")
        print("-" * 50)
        
        for i in range(len(self.label_encoder.classes_)):
            instrument = self.label_encoder.inverse_transform([i])[0]
            english_name = InstrumentMapper.get_english_name(instrument)
            if class_total[i] > 0:
                acc = 100 * class_correct[i] / class_total[i]
                class_accuracy[english_name] = acc
                print(f"  {english_name:15s}: {acc:6.2f}% ({class_correct[i]}/{class_total[i]})")
            else:
                class_accuracy[english_name] = 0
                print(f"  {english_name:15s}:   0.00% (0/0)")
        
        return accuracy, class_accuracy
    
    def get_predictions(self, test_loader):
        """获取预测结果"""
        self.model.eval()
        all_preds = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for data, labels in test_loader:
                data, labels = data.to(self.device), labels.to(self.device)
                outputs = self.model(data)
                probabilities = torch.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs.data, 1)
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probabilities.cpu().numpy())
        
        return np.array(all_labels), np.array(all_preds), np.array(all_probs)
    
    def print_classification_report(self, y_true, y_pred):
        """打印详细分类报告"""
        target_names = [InstrumentMapper.get_english_name(instr) 
                       for instr in self.label_encoder.classes_]
        
        print(f"\n?? {self.model_type}模型详细分类报告:")
        print("=" * 70)
        report = classification_report(y_true, y_pred, target_names=target_names, digits=4)
        print(report)
    
    def plot_confusion_matrix(self, y_true, y_pred):
        """绘制混淆矩阵"""
        target_names = [InstrumentMapper.get_english_name(instr) 
                       for instr in self.label_encoder.classes_]
        
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=target_names, yticklabels=target_names)
        plt.title(f'{self.model_type}模型 - Confusion Matrix', fontsize=16, fontweight='bold')
        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        output_path = os.path.join(Config.OUTPUT_DIR, f'confusion_matrix_{self.model_type}.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"? {self.model_type}模型混淆矩阵已保存: {output_path}")
        plt.show()
    
    def plot_roc_curves(self, y_true, y_prob):
        """绘制ROC曲线"""
        # 将标签二值化
        y_true_bin = label_binarize(y_true, classes=range(len(self.label_encoder.classes_)))
        
        # 计算每个类别的ROC曲线和AUC
        fpr = {}
        tpr = {}
        roc_auc = {}
        
        plt.figure(figsize=(10, 8))
        
        for i in range(len(self.label_encoder.classes_)):
            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
            
            instrument = InstrumentMapper.get_english_name(
                self.label_encoder.inverse_transform([i])[0]
            )
            plt.plot(fpr[i], tpr[i], label=f'{instrument} (AUC = {roc_auc[i]:.3f})')
        
        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'{self.model_type}模型 - ROC Curves')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        
        output_path = os.path.join(Config.OUTPUT_DIR, f'roc_curves_{self.model_type}.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"? {self.model_type}模型ROC曲线已保存: {output_path}")
        plt.show()
        
        # 显示AUC统计
        print(f"\n?? {self.model_type}模型AUC统计:")
        print("-" * 40)
        avg_auc = np.mean(list(roc_auc.values()))
        print(f"平均AUC: {avg_auc:.4f}")
        for i, auc_val in roc_auc.items():
            instrument = InstrumentMapper.get_english_name(
                self.label_encoder.inverse_transform([i])[0]
            )
            print(f"  {instrument:15s}: {auc_val:.4f}")
    
    def analyze_class_performance(self, y_true, y_pred, y_prob):
        """分析各类别性能"""
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        print(f"\n?? {self.model_type}模型各类别详细性能指标:")
        print("=" * 70)
        print(f"{'Instrument':<20} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
        print("-" * 70)
        
        precision_per_class = precision_score(y_true, y_pred, average=None)
        recall_per_class = recall_score(y_true, y_pred, average=None)
        f1_per_class = f1_score(y_true, y_pred, average=None)
        
        for i in range(len(self.label_encoder.classes_)):
            instrument = InstrumentMapper.get_english_name(
                self.label_encoder.inverse_transform([i])[0]
            )
            support = np.sum(y_true == i)
            
            print(f"{instrument:<20} {precision_per_class[i]:<10.4f} "
                  f"{recall_per_class[i]:<10.4f} {f1_per_class[i]:<10.4f} {support:<10}")
        
        # 宏观平均和加权平均
        macro_precision = precision_score(y_true, y_pred, average='macro')
        macro_recall = recall_score(y_true, y_pred, average='macro')
        macro_f1 = f1_score(y_true, y_pred, average='macro')
        
        weighted_precision = precision_score(y_true, y_pred, average='weighted')
        weighted_recall = recall_score(y_true, y_pred, average='weighted')
        weighted_f1 = f1_score(y_true, y_pred, average='weighted')
        
        print("-" * 70)
        print(f"{'Macro Avg':<20} {macro_precision:<10.4f} {macro_recall:<10.4f} {macro_f1:<10.4f}")
        print(f"{'Weighted Avg':<20} {weighted_precision:<10.4f} {weighted_recall:<10.4f} {weighted_f1:<10.4f}")

def load_model_and_evaluator(model_type, device):
    """加载模型和评估器"""
    import joblib
    
    # 加载标签编码器
    label_encoder_path = os.path.join(project_root, "model", f"model_{model_type}_label_encoder.pkl")
    if not os.path.exists(label_encoder_path):
        print(f"错误: 标签编码器文件不存在 - {label_encoder_path}")
        return None
    
    label_encoder = joblib.load(label_encoder_path)
    
    # 加载模型
    model_path = os.path.join(project_root, "model", f"model_{model_type}.pth")
    if not os.path.exists(model_path):
        print(f"错误: 模型文件不存在 - {model_path}")
        return None
    
    checkpoint = torch.load(model_path, map_location=device)
    
    # 根据模型类型创建对应的模型
    input_shape = checkpoint.get('input_shape', (1, 128, 130))
    num_classes = checkpoint.get('num_classes', len(label_encoder.classes_))
    
    if model_type == 'basic':
        model = create_improved_classifier(input_shape, num_classes)
    elif model_type == 'advanced':
        model = create_advanced_classifier(input_shape, num_classes)
    elif model_type == 'simplified':
        model = create_simplified_classifier(input_shape, num_classes)
    else:
        print(f"未知的模型类型: {model_type}")
        return None
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    
    print(f"? {model_type}模型加载成功")
    
    return MultiModelEvaluator(model, label_encoder, device, model_type)

def compare_all_models(test_loader, device):
    """比较所有模型"""
    model_types = ['basic', 'simplified', 'advanced']
    results = {}
    
    for model_type in model_types:
        evaluator = load_model_and_evaluator(model_type, device)
        if evaluator is None:
            continue
            
        result = evaluator.comprehensive_evaluation(test_loader)
        results[model_type] = result
    
    # 绘制比较图
    if len(results) > 1:
        plt.figure(figsize=(12, 6))
        
        # 准确率比较
        models = list(results.keys())
        accuracies = [results[model]['accuracy'] for model in models]
        
        plt.subplot(1, 2, 1)
        bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'])
        plt.title('模型准确率比较', fontsize=14)
        plt.ylabel('准确率 (%)', fontsize=12)
        plt.ylim(0, 100)
        
        # 在柱子上添加数值
        for bar, accuracy in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{accuracy:.2f}%', ha='center', va='bottom', fontsize=10)
        
        # 各类别准确率比较
        plt.subplot(1, 2, 2)
        instruments = list(results[models[0]]['class_accuracy'].keys())
        x = np.arange(len(instruments))
        width = 0.25
        
        for i, model_type in enumerate(models):
            accuracies = [results[model_type]['class_accuracy'][inst] for inst in instruments]
            plt.bar(x + i*width, accuracies, width, label=model_type)
        
        plt.xlabel('乐器类别', fontsize=12)
        plt.ylabel('准确率 (%)', fontsize=12)
        plt.title('各类别准确率比较', fontsize=14)
        plt.xticks(x + width, instruments, rotation=45, ha='right')
        plt.legend()
        plt.tight_layout()
        
        output_path = os.path.join(Config.OUTPUT_DIR, 'model_comparison.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"? 模型比较图已保存: {output_path}")
        plt.show()
        
        # 输出最佳模型
        best_model = max(results, key=lambda x: results[x]['accuracy'])
        print(f"\n?? 最佳模型: {best_model} (准确率: {results[best_model]['accuracy']:.2f}%)")

def main():
    """主评估函数"""
    parser = argparse.ArgumentParser(description='多模型评估')
    parser.add_argument('--model-type', type=str, default='all', 
                       choices=['basic', 'simplified', 'advanced', 'all'],
                       help='要评估的模型类型')
    
    args = parser.parse_args()
    
    print("=== 多模型训练程度评估 ===")
    
    # 1. 初始化配置
    Config.create_directories()
    
    # 2. 设置设备
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"使用设备: {device}")
    
    # 3. 加载测试数据
    print("\n加载测试数据...")
    preprocessor = AudioDataPreprocessor(use_cache=True)
    _, _, test_loader, num_classes = preprocessor.create_data_loaders(use_cache=True, augment=False)
    
    if args.model_type == 'all':
        # 比较所有模型
        compare_all_models(test_loader, device)
    else:
        # 评估单个模型
        evaluator = load_model_and_evaluator(args.model_type, device)
        if evaluator is None:
            return
            
        results = evaluator.comprehensive_evaluation(test_loader)
        
        # 模型训练程度总结
        print(f"\n?? {args.model_type}模型训练程度总结:")
        print("=" * 50)
        accuracy = results['accuracy']
        
        if accuracy > 90:
            print("? 优秀! 模型训练得很好")
        elif accuracy > 80:
            print("? 良好! 模型训练得不错")  
        elif accuracy > 70:
            print("?? 一般! 模型需要进一步优化")
        else:
            print("? 较差! 建议重新训练或调整模型")
        
        print(f"总体准确率: {accuracy:.2f}%")

if __name__ == "__main__":
    main()
 
```

## multi_scale_analyzer.py 
文件路径: D:\program_project\4ATS\test\multi_scale_analyzer.py 

```py 
import os
import torch
import librosa
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

class MultiScaleTimeSeriesAnalyzer:
    """多尺度时间序列分析器"""
    
    def __init__(self, model, label_encoder, device):
        self.model = model
        self.label_encoder = label_encoder
        self.device = device
        self.model.eval()
    
    def multi_scale_analysis(self, audio_path, scales=None):
        """
        多尺度分析
        
        Args:
            audio_path: 音频文件路径
            scales: 分析尺度配置
        """
        if scales is None:
            scales = [
                {'window_size': 3.0, 'hop_size': 1.0, 'threshold': 0.3, 'weight': 1.0, 'name': 'Coarse'},
                {'window_size': 1.5, 'hop_size': 0.5, 'threshold': 0.2, 'weight': 0.8, 'name': 'Medium'},
                {'window_size': 1.0, 'hop_size': 0.3, 'threshold': 0.15, 'weight': 0.6, 'name': 'Fine'}
            ]
        
        print("开始多尺度分析...")
        all_results = []
        
        for scale in scales:
            print(f"\n?? 运行 {scale['name']} 尺度分析:")
            print(f"   窗口: {scale['window_size']}s, 跳跃: {scale['hop_size']}s, 阈值: {scale['threshold']}")
            
            timeline = self._single_scale_analysis(audio_path, scale)
            all_results.append({
                'scale': scale,
                'timeline': timeline
            })
        
        # 融合多尺度结果
        fused_timeline = self._fuse_multi_scale_results(all_results)
        return fused_timeline
    
    def _single_scale_analysis(self, audio_path, scale_config):
        """单尺度分析"""
        y, sr = librosa.load(audio_path, sr=22050)
        
        window_size = scale_config['window_size']
        hop_size = scale_config['hop_size']
        threshold = scale_config['threshold']
        
        window_samples = int(window_size * sr)
        hop_samples = int(hop_size * sr)
        
        predictions = []
        timestamps = []
        
        for start in range(0, len(y) - window_samples + 1, hop_samples):
            end = start + window_samples
            window_audio = y[start:end]
            timestamp = start / sr
            
            features = self._extract_features(window_audio, sr)
            if features is not None:
                prediction = self._predict_single_window(features)
                predictions.append(prediction)
                timestamps.append(timestamp)
        
        return self._process_timeline_results(predictions, timestamps, window_size, threshold)
    
    def _fuse_multi_scale_results(self, all_results):
        """融合多尺度结果"""
        fused_timeline = {}
        
        # 初始化时间线
        for instrument in self.label_encoder.classes_:
            fused_timeline[instrument] = {
                'segments': [],
                'total_duration': 0.0,
                'max_confidence': 0.0,
                'average_confidence': 0.0,
                'scale_scores': []  # 记录来自不同尺度的得分
            }
        
        # 融合逻辑
        for result in all_results:
            scale = result['scale']
            timeline = result['timeline']
            weight = scale['weight']
            
            for instrument, data in timeline.items():
                for segment in data['segments']:
                    # 加权融合
                    weighted_segment = segment.copy()
                    weighted_segment['confidence'] *= weight
                    weighted_segment['scale'] = scale['name']
                    weighted_segment['original_confidence'] = segment['confidence']
                    
                    fused_timeline[instrument]['segments'].append(weighted_segment)
        
        # 合并和过滤融合后的时间段
        for instrument in fused_timeline.keys():
            segments = fused_timeline[instrument]['segments']
            if segments:
                # 按开始时间排序
                segments.sort(key=lambda x: x['start'])
                
                # 合并重叠时间段
                merged_segments = []
                current_segment = segments[0]
                
                for segment in segments[1:]:
                    if self._segments_overlap(current_segment, segment):
                        # 合并重叠段，取最大置信度
                        current_segment['end'] = max(current_segment['end'], segment['end'])
                        current_segment['confidence'] = max(current_segment['confidence'], segment['confidence'])
                        if 'scale_scores' not in current_segment:
                            current_segment['scale_scores'] = []
                        current_segment['scale_scores'].append({
                            'scale': segment.get('scale', 'unknown'),
                            'confidence': segment.get('original_confidence', segment['confidence'])
                        })
                    else:
                        merged_segments.append(current_segment)
                        current_segment = segment
                
                merged_segments.append(current_segment)
                
                # 过滤低置信度段
                filtered_segments = [seg for seg in merged_segments if seg['confidence'] >= 0.15]
                
                fused_timeline[instrument]['segments'] = filtered_segments
                
                # 计算统计信息
                if filtered_segments:
                    total_duration = sum(seg['end'] - seg['start'] for seg in filtered_segments)
                    confidences = [seg['confidence'] for seg in filtered_segments]
                    
                    fused_timeline[instrument]['total_duration'] = total_duration
                    fused_timeline[instrument]['max_confidence'] = max(confidences)
                    fused_timeline[instrument]['average_confidence'] = np.mean(confidences)
        
        return fused_timeline
    
    def _segments_overlap(self, seg1, seg2, gap_tolerance=0.5):
        """判断两个时间段是否重叠"""
        return seg1['end'] + gap_tolerance >= seg2['start'] and seg2['end'] + gap_tolerance >= seg1['start']
    
    def _extract_features(self, audio, sr, target_shape=(128, 130)):
        """提取音频特征"""
        try:
            mel_spec = librosa.feature.melspectrogram(
                y=audio, sr=sr, n_mels=128, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            # 调整形状
            if log_mel.shape[1] < target_shape[1]:
                log_mel = np.pad(log_mel, ((0, 0), (0, target_shape[1] - log_mel.shape[1])), mode='constant')
            else:
                log_mel = log_mel[:, :target_shape[1]]
                
            return log_mel
        except Exception as e:
            print(f"特征提取错误: {e}")
            return None
    
    def _predict_single_window(self, features):
        """预测单个窗口"""
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)
        input_tensor = input_tensor.to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
        
        probs = probabilities.cpu().numpy()[0]
        results = {}
        
        for idx, prob in enumerate(probs):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            results[instrument] = float(prob)
        
        return results
    
    def _process_timeline_results(self, predictions, timestamps, window_size, threshold):
        """处理时间线结果"""
        timeline = {}
        
        for instrument in self.label_encoder.classes_:
            timeline[instrument] = {
                'segments': [],
                'total_duration': 0.0,
                'max_confidence': 0.0,
                'average_confidence': 0.0
            }
        
        for timestamp, pred_dict in zip(timestamps, predictions):
            if pred_dict:
                best_instrument, best_confidence = max(pred_dict.items(), key=lambda x: x[1])
                if best_confidence >= threshold:
                    timeline[best_instrument]['segments'].append({
                        'start': timestamp,
                        'end': timestamp + window_size,
                        'confidence': best_confidence
                    })
        
        # 合并连续时间段
        for instrument in timeline.keys():
            segments = timeline[instrument]['segments']
            if segments:
                segments.sort(key=lambda x: x['start'])
                merged_segments = []
                current_segment = segments[0]
                
                for segment in segments[1:]:
                    if segment['start'] <= current_segment['end'] + 0.5:
                        current_segment['end'] = max(current_segment['end'], segment['end'])
                        current_segment['confidence'] = max(current_segment['confidence'], segment['confidence'])
                    else:
                        merged_segments.append(current_segment)
                        current_segment = segment
                
                merged_segments.append(current_segment)
                timeline[instrument]['segments'] = merged_segments
                
                # 计算统计
                total_duration = sum(seg['end'] - seg['start'] for seg in merged_segments)
                confidences = [seg['confidence'] for seg in merged_segments]
                
                timeline[instrument]['total_duration'] = total_duration
                timeline[instrument]['max_confidence'] = max(confidences) if confidences else 0
                timeline[instrument]['average_confidence'] = np.mean(confidences) if confidences else 0
        
        return timeline
 
```

## test_config.py 
文件路径: D:\program_project\4ATS\test\test_config.py 

```py 
import os

class TestConfig:
    """测试配置"""
    
    # 测试音频路径
    TEST_AUDIO_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "music")
    
    # 模型路径
    MODEL_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "model")
    
    # 输出路径
    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "output", "test_results")
    
    @classmethod
    def create_directories(cls):
        """创建必要的目录"""
        os.makedirs(cls.OUTPUT_DIR, exist_ok=True)
        print(f"测试输出目录: {cls.OUTPUT_DIR}")
 
```

## test_timeline.py 
文件路径: D:\program_project\4ATS\test\test_timeline.py 

```py 
import os
import sys
import torch
import joblib
import librosa
import numpy as np
import argparse

# 添加项目根目录到Python路径
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.config import Config
from config.music_file_loader import load_music_files  # 新增导入
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier
from src.time_series_analyzer import TimeSeriesAnalyzer

def load_model_with_fix(model_type, device):
    """加载指定类型的模型并修复输入形状问题"""
    # 加载标签编码器
    label_encoder_path = os.path.join(project_root, "model", f"model_{model_type}_label_encoder.pkl")
    if not os.path.exists(label_encoder_path):
        print(f"错误: 标签编码器文件不存在 - {label_encoder_path}")
        return None, None
    
    label_encoder = joblib.load(label_encoder_path)
    
    # 加载模型
    model_path = os.path.join(project_root, "model", f"model_{model_type}.pth")
    if not os.path.exists(model_path):
        print(f"错误: 模型文件不存在 - {model_path}")
        return None, None
    
    checkpoint = torch.load(model_path, map_location=device)
    
    # 修复输入形状
    input_shape = (1, 128, 130)  # (channels, height, width)
    num_classes = len(label_encoder.classes_)
    
    print(f"使用输入形状: {input_shape}")
    print(f"类别数量: {num_classes}")
    
    # 根据模型类型创建对应的模型
    if model_type == 'basic':
        model = create_improved_classifier(input_shape, num_classes)
    elif model_type == 'advanced':
        model = create_advanced_classifier(input_shape, num_classes)
    elif model_type == 'simplified':
        model = create_simplified_classifier(input_shape, num_classes)
    else:
        print(f"未知的模型类型: {model_type}")
        return None, None
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    
    return model, label_encoder

def ensure_feature_shape(features, target_shape=(128, 130)):
    """确保特征形状一致"""
    if features.shape != target_shape:
        # 调整到目标形状
        if features.shape[1] < target_shape[1]:
            # 填充
            pad_width = target_shape[1] - features.shape[1]
            features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')
        elif features.shape[1] > target_shape[1]:
            # 截断
            features = features[:, :target_shape[1]]
    
    return features

class FixedTimeSeriesAnalyzer(TimeSeriesAnalyzer):
    """修复的时间序列分析器"""
    
    def __init__(self, model, label_encoder, device, model_type):
        super().__init__(model, label_encoder, device)
        self.model_type = model_type
    
    def _extract_features(self, audio, sr):
        """提取音频特征"""
        try:
            # 提取Mel频谱图
            mel_spec = librosa.feature.melspectrogram(
                y=audio, sr=sr, n_mels=128, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # 标准化
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            # 确保特征尺寸一致
            log_mel = ensure_feature_shape(log_mel, (128, 130))
            
            return log_mel
            
        except Exception as e:
            print(f"特征提取错误: {e}")
            return None
    
    def _predict_single_window(self, features):
        """预测单个窗口"""
        # 确保特征形状正确
        features = ensure_feature_shape(features, (128, 130))
        
        # 转换为模型输入格式: (1, 1, 128, 130)
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)
        input_tensor = input_tensor.to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
        
        # 获取所有类别的概率
        probs = probabilities.cpu().numpy()[0]
        results = {}
        
        for idx, prob in enumerate(probs):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            results[instrument] = float(prob)
        
        return results
    
    def generate_report(self, timeline, audio_duration):
        """生成分析报告"""
        from src.instrument_mapper import InstrumentMapper
        
        # 乐器名称映射字典
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        print(f"\n{'='*60}")
        print(f"      {self.model_type}模型 - Instrument Timeline Analysis Report")
        print(f"{'='*60}")
        
        # 统计活跃乐器
        active_instruments = []
        for instrument, data in timeline.items():
            if data['segments']:
                english_name = instrument_names.get(instrument, instrument)
                active_instruments.append((english_name, data['total_duration'], instrument))
        
        # 按持续时间排序
        active_instruments.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\n?? Active Instruments Statistics ({len(active_instruments)} instruments):")
        print("-" * 60)
        
        for english_name, duration, original_name in active_instruments:
            percentage = (duration / audio_duration) * 100
            max_conf = timeline[original_name]['max_confidence']
            avg_conf = timeline[original_name]['average_confidence']
            segment_count = len(timeline[original_name]['segments'])
            
            print(f"?? {english_name:15s} | {duration:6.1f}s ({percentage:5.1f}%) | "
                f"Max Confidence: {max_conf:.3f} | Segments: {segment_count}")
        
        return active_instruments

def test_timeline_with_model(model_type, audio_path):
    """使用指定模型测试时间线分析"""
    print(f"\n{'='*50}")
    print(f"使用 {model_type} 模型进行时间线分析")
    print(f"{'='*50}")
    
    # 设置设备
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 加载模型和标签编码器
    model, label_encoder = load_model_with_fix(model_type, device)
    if model is None or label_encoder is None:
        return None
    
    print(f"乐器类别: {list(label_encoder.classes_)}")
    print("模型加载成功!")
    
    # 创建时间序列分析器
    analyzer = FixedTimeSeriesAnalyzer(model, label_encoder, device, model_type)
    
    # 分析音频时间线
    print("\n开始时间线分析...")
    timeline = analyzer.analyze_audio_timeline(
        audio_path, 
        window_size=1.5,      # 减小窗口大小，提高时间分辨率
        hop_size=0.3,         # 减小跳跃步长，增加采样密度
        threshold=0.15         # 降低阈值，捕捉更多弱信号
    )
    
    # 获取音频时长用于可视化
    y, sr = librosa.load(audio_path, sr=22050)
    audio_duration = len(y) / sr
    
    # 生成报告
    active_instruments = analyzer.generate_report(timeline, audio_duration)
    
    # 可视化时间线
    output_path = os.path.join(Config.OUTPUT_DIR, f"instrument_timeline_{model_type}.png")
    analyzer.visualize_timeline(timeline, audio_duration, output_path)
    
    print(f"\n? {model_type}模型分析完成!")
    print(f"?? 发现了 {len(active_instruments)} 种活跃乐器")
    print(f"?? 时间线图已保存: {output_path}")
    
    return active_instruments

def main():
    """主测试函数"""
    parser = argparse.ArgumentParser(description='多模型时间线分析')
    parser.add_argument('--model-type', type=str, default='all', 
                       choices=['basic', 'simplified', 'advanced', 'all'],
                       help='要测试的模型类型')
    parser.add_argument('--audio-path', type=str, 
                       default=None,  # 修改为None
                       help='测试音频路径（如未指定则使用music.txt中的文件）')
    parser.add_argument('--batch', action='store_true',
                       help='批量测试music.txt中的所有文件')
    
    args = parser.parse_args()
    
    print("=== AI音频分析与自动扒谱系统 - 多模型时间线分析 ===")
    
    # 1. 初始化配置
    Config.create_directories()
    
    if args.batch or args.audio_path is None:
        # 批量测试模式
        music_files = load_music_files()
        if not music_files:
            print("错误: 没有找到可测试的音乐文件")
            return
        
        print(f"批量时间线分析 {len(music_files)} 个音乐文件...")
        
        for music_path in music_files:
            print(f"\n分析文件: {os.path.basename(music_path)}")
            
            if args.model_type == 'all':
                # 测试所有模型
                model_types = ['basic', 'simplified', 'advanced']
                all_results = {}
                
                for model_type in model_types:
                    result = test_timeline_with_model(model_type, music_path)
                    if result:
                        all_results[model_type] = result
                
                # 输出比较结果
                print(f"\n{'='*60}")
                print(f"文件 {os.path.basename(music_path)} 的时间线分析模型比较结果")
                print(f"{'='*60}")
                
                for model_type, instruments in all_results.items():
                    print(f"\n{model_type:>10}模型: 检测到 {len(instruments)} 种乐器")
                    for i, (name, duration, _) in enumerate(instruments[:3]):
                        print(f"            {i+1}. {name}: {duration:.1f}s")
            else:
                # 测试单个模型
                test_timeline_with_model(args.model_type, music_path)
    else:
        # 单个文件测试模式（原有逻辑）
        if not os.path.exists(args.audio_path):
            print(f"错误: 测试音频文件不存在 - {args.audio_path}")
            print("请确保音频文件存在")
            return
        
        if args.model_type == 'all':
            # 测试所有模型
            model_types = ['basic', 'simplified', 'advanced']
            all_results = {}
            
            for model_type in model_types:
                result = test_timeline_with_model(model_type, args.audio_path)
                if result:
                    all_results[model_type] = result
            
            # 输出比较结果
            print(f"\n{'='*60}")
            print("时间线分析模型比较结果")
            print(f"{'='*60}")
            
            for model_type, instruments in all_results.items():
                print(f"\n{model_type:>10}模型: 检测到 {len(instruments)} 种乐器")
                for i, (name, duration, _) in enumerate(instruments[:3]):
                    print(f"            {i+1}. {name}: {duration:.1f}s")
        else:
            # 测试单个模型
            test_timeline_with_model(args.model_type, args.audio_path)

if __name__ == "__main__":
    main()
 
```

