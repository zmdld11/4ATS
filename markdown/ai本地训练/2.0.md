æ­¤ç‰ˆæœ¬é‡‡ç”¨3ä¸ªæ¨¡å‹ä¸€èµ·è®­ç»ƒçš„å½¢å¼

# config

## config.py

```py
# config.py (æ›´æ–°ç‰ˆæœ¬ - ä¸­æ–‡æ³¨é‡Š)
import os

class Config:
    """é¡¹ç›®é…ç½®ç±»"""
    MODEL_VERSIONS = ['basic', 'simplified', 'advanced', 'stable_advanced']  # æ”¯æŒçš„æ¨¡å‹ç‰ˆæœ¬

    # è·¯å¾„é…ç½®
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    DATA_DIR = os.path.join(BASE_DIR, "data")
    MODEL_DIR = os.path.join(BASE_DIR, "model")
    OUTPUT_DIR = os.path.join(BASE_DIR, "output")
    
    # æ•°æ®é›†é…ç½®
    DATASET_NAME = "IRMAS-TrainingData"
    DATASET_URL = "https://zenodo.org/record/1290750/files/IRMAS-TrainingData.zip"
    
    # éŸ³é¢‘å¤„ç†é…ç½®
    TARGET_SAMPLE_RATE = 22050
    AUDIO_DURATION = 3  # ç§’
    N_MELS = 128
    
    # è®­ç»ƒé…ç½®
    BATCH_SIZE = 32
    EPOCHS = 150  # å¢åŠ ä»¥è·å¾—æ›´å¥½çš„æ”¶æ•›
    LEARNING_RATE = 0.001  # ä¸ºAdamWè°ƒæ•´
    VALIDATION_SPLIT = 0.2
    
    # æ¨¡å‹é…ç½®
    INPUT_SHAPE = (128, 130, 1)  # Melé¢‘è°±å›¾å½¢çŠ¶
    
    # é«˜çº§è®­ç»ƒé…ç½®
    USE_ADVANCED_MODEL = True  # è®¾ç½®ä¸ºFalseä½¿ç”¨åŸºç¡€æ¨¡å‹
    USE_DATA_AUGMENTATION = True
    EARLY_STOPPING_PATIENCE = 20
    
    # å¤šç‰ˆæœ¬æ¨¡å‹é…ç½®
    SAVE_MULTIPLE_VERSIONS = True  # åŒæ—¶ä¿å­˜å¤šä¸ªæ¨¡å‹ç‰ˆæœ¬
    MODEL_VERSIONS = ['basic', 'simplified', 'advanced']  # æ”¯æŒçš„æ¨¡å‹ç‰ˆæœ¬
    
    @classmethod
    def create_directories(cls):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [cls.DATA_DIR, cls.MODEL_DIR, cls.OUTPUT_DIR]
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            print(f"ç›®å½•å·²åˆ›å»º: {directory}")
```

# src

## advanced_models.py

```py
# advanced_models.py (ä¿®å¤æƒé‡åˆå§‹åŒ–)
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from config.config import Config

class ResidualBlock(nn.Module):
    """å¸¦æœ‰æ‰¹å½’ä¸€åŒ–çš„æ®‹å·®å—"""
    
    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.3):
        super(ResidualBlock, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.dropout = nn.Dropout2d(dropout_rate)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        out = self.dropout(out)
        return out

class AttentionModule(nn.Module):
    """é€šé“æ³¨æ„åŠ›æ¨¡å—"""
    
    def __init__(self, channels, reduction=16):
        super(AttentionModule, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class AdvancedInstrumentClassifier(nn.Module):
    """å¸¦æœ‰æ®‹å·®è¿æ¥å’Œæ³¨æ„åŠ›æœºåˆ¶çš„é«˜çº§ä¹å™¨åˆ†ç±»å™¨"""
    
    def __init__(self, input_shape, num_classes):
        super(AdvancedInstrumentClassifier, self).__init__()
        
        if len(input_shape) == 3:
            self.input_shape = input_shape
        elif len(input_shape) == 2:
            self.input_shape = (1, input_shape[0], input_shape[1])
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥å½¢çŠ¶: {input_shape}")
            
        self.num_classes = num_classes
        
        print(f"æ¨¡å‹è¾“å…¥å½¢çŠ¶: {self.input_shape}")
        
        # åˆå§‹å·ç§¯å±‚ - ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„è¾“å…¥é€šé“æ•°
        self.conv1 = nn.Sequential(
            nn.Conv2d(self.input_shape[0], 64, 7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1)
        )
        
        # å¸¦æœ‰æ³¨æ„åŠ›çš„æ®‹å·®å—
        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.attention1 = AttentionModule(64)
        
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.attention2 = AttentionModule(128)
        
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.attention3 = AttentionModule(256)
        
        self.layer4 = self._make_layer(256, 512, 2, stride=2)
        self.attention4 = AttentionModule(512)
        
        # å…¨å±€æ± åŒ–å’Œåˆ†ç±»å™¨
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )
        
        # å»¶è¿Ÿæƒé‡åˆå§‹åŒ–ï¼Œåœ¨æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡åè¿›è¡Œ
        self.weights_initialized = False
    
    def _make_layer(self, in_channels, out_channels, blocks, stride):
        """åˆ›å»ºæ®‹å·®å±‚"""
        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride))
        for _ in range(1, blocks):
            layers.append(ResidualBlock(out_channels, out_channels))
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        """æ­£ç¡®åˆå§‹åŒ–æƒé‡ - ä¿®å¤ç‰ˆæœ¬"""
        print("åˆå§‹åŒ–é«˜çº§æ¨¡å‹æƒé‡...")
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:  # åªæœ‰åœ¨åç½®å­˜åœ¨æ—¶æ‰åˆå§‹åŒ–
                    nn.init.constant_(m.bias, 0)
                    print(f"åˆå§‹åŒ–å·ç§¯å±‚åç½®: {m.bias.shape}")
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
                print(f"åˆå§‹åŒ–BNå±‚: weight={m.weight.shape}, bias={m.bias.shape}")
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:  # åªæœ‰åœ¨åç½®å­˜åœ¨æ—¶æ‰åˆå§‹åŒ–
                    nn.init.constant_(m.bias, 0)
                    print(f"åˆå§‹åŒ–å…¨è¿æ¥å±‚åç½®: {m.bias.shape}")
        
        self.weights_initialized = True
        print("âœ… é«˜çº§æ¨¡å‹æƒé‡åˆå§‹åŒ–å®Œæˆ")
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # å»¶è¿Ÿåˆå§‹åŒ–æƒé‡ï¼ˆç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­æ—¶ï¼‰
        if not self.weights_initialized:
            self._initialize_weights()
        
        x = self.conv1(x)
        
        x = self.layer1(x)
        x = self.attention1(x)
        
        x = self.layer2(x)
        x = self.attention2(x)
        
        x = self.layer3(x)
        x = self.attention3(x)
        
        x = self.layer4(x)
        x = self.attention4(x)
        
        x = self.global_avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        
        return x

def create_advanced_classifier(input_shape, num_classes):
    """åˆ›å»ºé«˜çº§ä¹å™¨åˆ†ç±»å™¨"""
    return AdvancedInstrumentClassifier(input_shape, num_classes)

# æ·»åŠ ä¸€ä¸ªæ›´ç¨³å®šçš„ç®€åŒ–é«˜çº§æ¨¡å‹
class StableAdvancedClassifier(nn.Module):
    """ç¨³å®šçš„é«˜çº§åˆ†ç±»å™¨ - é¿å…å¤æ‚çš„åˆå§‹åŒ–é—®é¢˜"""
    
    def __init__(self, input_shape, num_classes):
        super(StableAdvancedClassifier, self).__init__()
        
        if len(input_shape) == 3:
            self.input_shape = input_shape
        elif len(input_shape) == 2:
            self.input_shape = (1, input_shape[0], input_shape[1])
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥å½¢çŠ¶: {input_shape}")
            
        self.num_classes = num_classes
        
        print(f"æ¨¡å‹è¾“å…¥å½¢çŠ¶: {self.input_shape}")
        
        # ä½¿ç”¨æ›´ç®€å•çš„ç»“æ„
        self.features = nn.Sequential(
            # å—1
            nn.Conv2d(self.input_shape[0], 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # å—2 - æ®‹å·®å—
            self._residual_block(64, 128, stride=2),
            
            # å—3 - æ®‹å·®å—
            self._residual_block(128, 256, stride=2),
            
            # å—4 - æ®‹å·®å—
            self._residual_block(256, 512, stride=2),
            
            # å…¨å±€å¹³å‡æ± åŒ–
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def _residual_block(self, in_channels, out_channels, stride=1):
        """ç®€åŒ–çš„æ®‹å·®å—"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.25)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

def create_stable_advanced_classifier(input_shape, num_classes):
    """åˆ›å»ºç¨³å®šçš„é«˜çº§åˆ†ç±»å™¨"""
    return StableAdvancedClassifier(input_shape, num_classes)

class SimplifiedAdvancedClassifier(nn.Module):
    """ç®€åŒ–ä½†æœ‰æ•ˆçš„åˆ†ç±»å™¨"""
    
    def __init__(self, input_shape, num_classes):
        super(SimplifiedAdvancedClassifier, self).__init__()
        
        if len(input_shape) == 3:
            self.input_shape = input_shape
        elif len(input_shape) == 2:
            self.input_shape = (1, input_shape[0], input_shape[1])
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥å½¢çŠ¶: {input_shape}")
            
        self.num_classes = num_classes
        
        print(f"æ¨¡å‹è¾“å…¥å½¢çŠ¶: {self.input_shape}")
        
        # å¢å¼ºçš„CNNä¸»å¹²ç½‘ç»œ
        self.features = nn.Sequential(
            # å—1
            nn.Conv2d(self.input_shape[0], 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # å—2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # å—3
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # å—4
            nn.Conv2d(256, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

def create_simplified_classifier(input_shape, num_classes):
    """åˆ›å»ºç®€åŒ–ä½†æœ‰æ•ˆçš„åˆ†ç±»å™¨"""
    return SimplifiedAdvancedClassifier(input_shape, num_classes)
```

## audio_preprocessor.py

```py
# audio_preprocessor.py (æ·»åŠ æ•°æ®å¢å¼º - ä¸­æ–‡æ³¨é‡Š)
import os
import librosa
import numpy as np
import pickle
import hashlib
import torch
import random
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from config.config import Config

class AudioDataset(Dataset):
    """å¸¦æœ‰æ•°æ®å¢å¼ºçš„PyTorchéŸ³é¢‘æ•°æ®é›†"""
    
    def __init__(self, features, labels, transform=None, augment=False):
        self.features = features
        self.labels = labels
        self.transform = transform
        self.augment = augment
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        feature = self.features[idx].copy()  # é‡è¦ï¼šåˆ›å»ºå‰¯æœ¬
        label = self.labels[idx]
        
        # è®­ç»ƒæœŸé—´çš„æ•°æ®å¢å¼º
        if self.augment:
            feature = self._apply_augmentation(feature)
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        feature = torch.FloatTensor(feature).unsqueeze(0)  # æ·»åŠ é€šé“ç»´åº¦
        label = torch.LongTensor([label])[0]
        
        if self.transform:
            feature = self.transform(feature)
            
        return feature, label
    
    def _apply_augmentation(self, feature):
        """å¯¹éŸ³é¢‘ç‰¹å¾åº”ç”¨æ•°æ®å¢å¼º"""
        # æ—¶é—´æ©ç ï¼ˆç±»ä¼¼äºSpecAugmentï¼‰
        if random.random() > 0.5:
            max_mask_width = feature.shape[1] // 4
            mask_width = random.randint(1, max_mask_width)
            mask_start = random.randint(0, feature.shape[1] - mask_width)
            feature[:, mask_start:mask_start + mask_width] = 0
        
        # é¢‘ç‡æ©ç 
        if random.random() > 0.5:
            max_mask_height = feature.shape[0] // 8
            mask_height = random.randint(1, max_mask_height)
            mask_start = random.randint(0, feature.shape[0] - mask_height)
            feature[mask_start:mask_start + mask_height, :] = 0
        
        # æ·»åŠ å°å™ªå£°
        if random.random() > 0.7:
            noise = np.random.normal(0, 0.01, feature.shape)
            feature = feature + noise
        
        return feature

class AudioDataPreprocessor:
    """å¸¦æœ‰å¢å¼ºåŠŸèƒ½å’Œæ•°æ®å¢å¼ºçš„éŸ³é¢‘æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, target_sr=Config.TARGET_SAMPLE_RATE, 
                 duration=Config.AUDIO_DURATION,
                 use_cache=True):
        self.target_sr = target_sr
        self.duration = duration
        self.label_encoder = LabelEncoder()
        self.use_cache = use_cache
        self.cache_dir = os.path.join(Config.DATA_DIR, "preprocessed_cache")
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    def extract_enhanced_features(self, audio_path):
        """æå–å¸¦æœ‰å¤šç§è¡¨ç¤ºçš„å¢å¼ºéŸ³é¢‘ç‰¹å¾"""
        try:
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(audio_path, sr=self.target_sr)
            
            # ç¡®ä¿éŸ³é¢‘é•¿åº¦ä¸€è‡´
            y = librosa.util.fix_length(y, size=self.target_sr * self.duration)
            
            # æå–å¤šç§ç‰¹å¾
            # 1. Melé¢‘è°±å›¾ï¼ˆä¸»è¦ç‰¹å¾ï¼‰
            mel_spec = librosa.feature.melspectrogram(
                y=y, sr=sr, n_mels=Config.N_MELS, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # 2. MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
            
            # 3. Chromaç‰¹å¾
            chroma = librosa.feature.chroma_stft(y=y, sr=sr)
            
            # ç»„åˆç‰¹å¾ï¼ˆå¯ä»¥é€‰æ‹©ä½¿ç”¨å“ªäº›ï¼‰
            # é€‰é¡¹1ï¼šä»…ä½¿ç”¨melé¢‘è°±å›¾ï¼ˆä¸å½“å‰æ¶æ„å…¼å®¹ï¼‰
            combined_features = log_mel
            
            # é€‰é¡¹2ï¼šå †å å¤šä¸ªç‰¹å¾ï¼ˆéœ€è¦æ›´æ”¹æ¨¡å‹æ¶æ„ï¼‰
            # combined_features = np.vstack([log_mel, mfcc, chroma])
            
            # æ ‡å‡†åŒ–
            combined_features = (combined_features - np.mean(combined_features)) / (np.std(combined_features) + 1e-8)
            
            return combined_features
            
        except Exception as e:
            print(f"å¤„ç†éŸ³é¢‘ {audio_path} æ—¶å‡ºé”™: {e}")
            return None
    
    def create_data_loaders(self, batch_size=Config.BATCH_SIZE, use_cache=True, augment=True):
        """åˆ›å»ºå¸¦æœ‰å¯é€‰æ•°æ®å¢å¼ºçš„PyTorchæ•°æ®åŠ è½½å™¨"""
        # åŠ è½½éŸ³é¢‘æ ·æœ¬
        audio_paths, labels = self.load_audio_samples(Config.DATA_DIR)
        
        # æå–ç‰¹å¾ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        X, y = self.prepare_dataset(audio_paths, labels, use_cache=use_cache)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=Config.VALIDATION_SPLIT, 
            random_state=42, stratify=y
        )
        
        # è¿›ä¸€æ­¥åˆ’åˆ†è®­ç»ƒé›†ç”¨äºéªŒè¯
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, 
            random_state=42, stratify=y_train
        )
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = AudioDataset(X_train, y_train, augment=augment)
        val_dataset = AudioDataset(X_val, y_val, augment=False)
        test_dataset = AudioDataset(X_test, y_test, augment=False)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
        
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_dataset)}")
        
        return train_loader, val_loader, test_loader, len(self.label_encoder.classes_)

    def _get_cache_filename(self, data_dir):
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶å"""
        config_str = f"{data_dir}_{self.target_sr}_{self.duration}_{Config.N_MELS}"
        hash_obj = hashlib.md5(config_str.encode())
        return os.path.join(self.cache_dir, f"preprocessed_{hash_obj.hexdigest()}.pkl")
    
    def _save_to_cache(self, cache_file, X, y, label_encoder):
        """ä¿å­˜é¢„å¤„ç†ç»“æœåˆ°ç¼“å­˜"""
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'features': X,
                    'labels': y,
                    'label_encoder': label_encoder,
                    'config': {
                        'target_sr': self.target_sr,
                        'duration': self.duration,
                        'n_mels': Config.N_MELS
                    }
                }, f)
            print(f"âœ… é¢„å¤„ç†æ•°æ®å·²ç¼“å­˜åˆ°: {cache_file}")
            return True
        except Exception as e:
            print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _load_from_cache(self, cache_file):
        """ä»ç¼“å­˜åŠ è½½é¢„å¤„ç†ç»“æœ"""
        try:
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # éªŒè¯é…ç½®æ˜¯å¦åŒ¹é…
            config = cache_data['config']
            if (config['target_sr'] == self.target_sr and 
                config['duration'] == self.duration and 
                config['n_mels'] == Config.N_MELS):
                
                print("âœ… ä»ç¼“å­˜åŠ è½½é¢„å¤„ç†æ•°æ®")
                return cache_data['features'], cache_data['labels'], cache_data['label_encoder']
            else:
                print("âš ï¸ ç¼“å­˜é…ç½®ä¸åŒ¹é…ï¼Œé‡æ–°é¢„å¤„ç†")
                return None, None, None
                
        except Exception as e:
            print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None, None, None
    
    def load_audio_samples(self, data_dir):
        """åŠ è½½éŸ³é¢‘æ ·æœ¬å’Œæ ‡ç­¾"""
        samples = []
        labels = []
        
        dataset_path = os.path.join(data_dir, Config.DATASET_NAME)
        print(f"ä» {dataset_path} åŠ è½½æ•°æ®...")
        
        # éå†æ¯ä¸ªä¹å™¨æ–‡ä»¶å¤¹
        for instrument in os.listdir(dataset_path):
            instrument_path = os.path.join(dataset_path, instrument)
            if os.path.isdir(instrument_path):
                for audio_file in os.listdir(instrument_path):
                    if audio_file.endswith('.wav'):
                        audio_path = os.path.join(instrument_path, audio_file)
                        samples.append(audio_path)
                        labels.append(instrument)
        
        print(f"æ‰¾åˆ° {len(samples)} ä¸ªéŸ³é¢‘æ ·æœ¬")
        print(f"ä¹å™¨ç±»åˆ«: {set(labels)}")
        
        return samples, labels
    
    def prepare_dataset(self, audio_paths, labels, use_cache=True):
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›† - å¸¦ç¼“å­˜åŠŸèƒ½"""
        cache_file = self._get_cache_filename(Config.DATA_DIR)
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache and os.path.exists(cache_file):
            X, y, label_encoder = self._load_from_cache(cache_file)
            if X is not None and y is not None:
                self.label_encoder = label_encoder
                return X, y
        
        # ç¼“å­˜ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨ï¼Œé‡æ–°å¤„ç†
        print("ğŸ”„ é¢„å¤„ç†éŸ³é¢‘æ•°æ®ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
        features = []
        valid_labels = []
        
        for i, (path, label) in enumerate(zip(audio_paths, labels)):
            if i % 100 == 0:  # æ¯100ä¸ªæ ·æœ¬æ˜¾ç¤ºè¿›åº¦
                print(f"å·²å¤„ç† {i}/{len(audio_paths)} ä¸ªæ ·æœ¬ ({i/len(audio_paths)*100:.1f}%)")
                
            feature = self.extract_enhanced_features(path)
            if feature is not None:
                features.append(feature)
                valid_labels.append(label)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        X = np.array(features)
        y = self.label_encoder.fit_transform(valid_labels)
        
        print(f"æœ€ç»ˆæ•°æ®é›†å½¢çŠ¶: {X.shape}")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if use_cache:
            self._save_to_cache(cache_file, X, y, self.label_encoder)
        
        return X, y
```

## instrument_mapper.py

```py
"""
ä¹å™¨åç§°æ˜ å°„å·¥å…·
"""

class InstrumentMapper:
    """ä¹å™¨åç§°æ˜ å°„ç±»"""
    
    # IRMASæ•°æ®é›†ä¹å™¨æ˜ å°„
    IRMAS_MAPPING = {
        'cel': 'Cello',
        'cla': 'Clarinet', 
        'flu': 'Flute',
        'gac': 'Acoustic Guitar',
        'gel': 'Electric Guitar',
        'org': 'Organ',
        'pia': 'Piano',
        'sax': 'Saxophone',
        'tru': 'Trumpet',
        'vio': 'Violin',
        'voi': 'Voice'
    }
    
    # åå‘æ˜ å°„ï¼ˆè‹±æ–‡åˆ°ç¼©å†™ï¼‰
    REVERSE_MAPPING = {v: k for k, v in IRMAS_MAPPING.items()}
    
    # ä¹å™¨ç±»åˆ«æè¿°
    INSTRUMENT_DESCRIPTIONS = {
        'Cello': 'Bowed string instrument, bass voice of the violin family',
        'Clarinet': 'Woodwind instrument with a single-reed mouthpiece',
        'Flute': 'Woodwind instrument that produces sound from the flow of air',
        'Acoustic Guitar': 'String instrument that produces sound acoustically',
        'Electric Guitar': 'Guitar that uses pickups to convert string vibration into electrical signals',
        'Organ': 'Keyboard instrument of one or more pipe divisions',
        'Piano': 'Acoustic stringed keyboard instrument',
        'Saxophone': 'Woodwind instrument made of brass',
        'Trumpet': 'Brass instrument commonly used in classical and jazz ensembles',
        'Violin': 'Bowed string instrument, smallest and highest-pitched in the violin family',
        'Voice': 'Human singing voice'
    }
    
    @classmethod
    def get_english_name(cls, abbreviation):
        """è·å–è‹±æ–‡åç§°"""
        return cls.IRMAS_MAPPING.get(abbreviation, abbreviation)
    
    @classmethod
    def get_abbreviation(cls, english_name):
        """è·å–ç¼©å†™"""
        return cls.REVERSE_MAPPING.get(english_name, english_name)
    
    @classmethod
    def get_description(cls, name):
        """è·å–ä¹å™¨æè¿°"""
        # å…ˆå°è¯•ä½œä¸ºç¼©å†™æŸ¥æ‰¾
        english_name = cls.get_english_name(name)
        return cls.INSTRUMENT_DESCRIPTIONS.get(english_name, "No description available")
    
    @classmethod
    def translate_labels(cls, labels):
        """ç¿»è¯‘æ ‡ç­¾åˆ—è¡¨"""
        if isinstance(labels, list):
            return [cls.get_english_name(label) for label in labels]
        elif isinstance(labels, np.ndarray):
            return np.array([cls.get_english_name(label) for label in labels])
        else:
            return labels
    
    @classmethod
    def print_instrument_info(cls):
        """æ‰“å°æ‰€æœ‰ä¹å™¨ä¿¡æ¯"""
        print("IRMAS Dataset Instrument Information:")
        print("=" * 50)
        for abbrev, english_name in cls.IRMAS_MAPPING.items():
            description = cls.INSTRUMENT_DESCRIPTIONS.get(english_name, "No description")
            print(f"{abbrev:4s} -> {english_name:15s} : {description}")
```

## model_builder.py

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
from config.config import Config

class ImprovedInstrumentClassifier(nn.Module):
    """æ”¹è¿›çš„ä¹å™¨åˆ†ç±»CNNæ¨¡å‹ - PyTorchç‰ˆæœ¬"""
    
    def __init__(self, input_shape, num_classes):
        super(ImprovedInstrumentClassifier, self).__init__()
        
        # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ˜¯ (channels, height, width) æ ¼å¼
        if len(input_shape) == 3:
            self.input_shape = input_shape  # (channels, height, width)
        elif len(input_shape) == 2:
            self.input_shape = (1, input_shape[0], input_shape[1])  # æ·»åŠ channelç»´åº¦
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥å½¢çŠ¶: {input_shape}")
            
        self.num_classes = num_classes
        
        print(f"æ¨¡å‹è¾“å…¥å½¢çŠ¶: {self.input_shape}")
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        self.conv1 = nn.Sequential(
            nn.Conv2d(self.input_shape[0], 64, 3, padding=1),  # ä½¿ç”¨æ­£ç¡®çš„è¾“å…¥é€šé“æ•°
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        self.conv3 = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )
        
        # è®¡ç®—å·ç§¯åçš„ç‰¹å¾å›¾å°ºå¯¸
        with torch.no_grad():
            self.feature_size = self._get_conv_output(self.input_shape)
        
        # å…¨è¿æ¥å±‚
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_size, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def _get_conv_output(self, shape):
        """è®¡ç®—å·ç§¯å±‚è¾“å‡ºå°ºå¯¸"""
        batch_size = 1
        # åˆ›å»ºæµ‹è¯•è¾“å…¥ (batch_size, channels, height, width)
        input_tensor = torch.rand(batch_size, *shape)
        output = self.conv1(input_tensor)
        output = self.conv2(output)
        output = self.conv3(output)
        return output.view(batch_size, -1).size(1)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = x.view(x.size(0), -1)  # å±•å¹³
        x = self.classifier(x)
        return x

def create_improved_classifier(input_shape, num_classes):
    """åˆ›å»ºæ”¹è¿›çš„ä¹å™¨åˆ†ç±»å™¨"""
    return ImprovedInstrumentClassifier(input_shape, num_classes)
```

## model_comparison.py

```py
# model_comparison.py (æ–°æ–‡ä»¶)
import os
import torch
import matplotlib.pyplot as plt
import numpy as np
from config.config import Config
from src.model_manager import model_manager
from src.audio_preprocessor import AudioDataPreprocessor
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier

def compare_models():
    """æ¯”è¾ƒæ‰€æœ‰å¯ç”¨æ¨¡å‹çš„æ€§èƒ½"""
    print("=== æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ ===")
    
    # è·å–å¯ç”¨æ¨¡å‹
    available_models = model_manager.get_available_models()
    if not available_models:
        print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹")
        return
    
    print(f"æ‰¾åˆ°çš„æ¨¡å‹: {', '.join(available_models)}")
    
    # åŠ è½½æ•°æ®
    preprocessor = AudioDataPreprocessor(use_cache=True)
    train_loader, val_loader, test_loader, num_classes = preprocessor.create_data_loaders(
        use_cache=True, augment=False
    )
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    results = {}
    
    for model_type in available_models:
        print(f"\nè¯„ä¼° {model_type} æ¨¡å‹...")
        
        try:
            # åˆ›å»ºæ¨¡å‹
            if model_type == 'basic':
                model = create_improved_classifier((1, 128, 130), num_classes)
            elif model_type == 'advanced':
                model = create_advanced_classifier((1, 128, 130), num_classes)
            elif model_type == 'simplified':
                model = create_simplified_classifier((1, 128, 130), num_classes)
            else:
                continue
                
            model = model.to(device)
            
            # åŠ è½½æƒé‡
            load_success, _ = model_manager.load_model(model, model_type, device)
            if not load_success:
                continue
            
            # è¯„ä¼°æ¨¡å‹
            model.eval()
            correct = 0
            total = 0
            
            with torch.no_grad():
                for data, labels in test_loader:
                    data, labels = data.to(device), labels.to(device)
                    outputs = model(data)
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
            
            accuracy = correct / total
            results[model_type] = accuracy
            print(f"{model_type}æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")
            
        except Exception as e:
            print(f"è¯„ä¼°{model_type}æ¨¡å‹å¤±è´¥: {e}")
    
    # ç»˜åˆ¶æ¯”è¾ƒå›¾
    if results:
        plt.figure(figsize=(10, 6))
        models = list(results.keys())
        accuracies = [results[model] for model in models]
        
        bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'])
        plt.title('Model Performance Comparison', fontsize=14)
        plt.ylabel('Accuracy', fontsize=12)
        plt.ylim(0, 1.0)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, accuracy in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{accuracy:.4f}', ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        comparison_path = os.path.join(Config.OUTPUT_DIR, 'model_comparison.png')
        plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
        print(f"\næ¯”è¾ƒå›¾å·²ä¿å­˜åˆ°: {comparison_path}")
        plt.show()
        
        # è¾“å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(results, key=results.get)
        print(f"\nğŸ‰ æœ€ä½³æ¨¡å‹: {best_model} (å‡†ç¡®ç‡: {results[best_model]:.4f})")

if __name__ == "__main__":
    compare_models()
```

## model_manager.py

```py
# model_manager.py (æ–°æ–‡ä»¶)
import os
import torch
import joblib
from config.config import Config

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ - å¤„ç†å¤šç‰ˆæœ¬æ¨¡å‹çš„ä¿å­˜å’ŒåŠ è½½"""
    
    def __init__(self):
        self.model_dir = Config.MODEL_DIR
    
    def get_model_filename(self, model_type, suffix="pth"):
        """è·å–æ¨¡å‹æ–‡ä»¶å"""
        return f"model_{model_type}.{suffix}"
    
    def get_label_encoder_filename(self, model_type):
        """è·å–æ ‡ç­¾ç¼–ç å™¨æ–‡ä»¶å"""
        return f"model_{model_type}_label_encoder.pkl"
    
    def save_model(self, model, preprocessor, model_type, history=None, epoch=0):
        """ä¿å­˜æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
        # æ¨¡å‹æ–‡ä»¶è·¯å¾„
        model_filename = self.get_model_filename(model_type)
        model_path = os.path.join(self.model_dir, model_filename)
        
        # æ ‡ç­¾ç¼–ç å™¨æ–‡ä»¶è·¯å¾„
        encoder_filename = self.get_label_encoder_filename(model_type)
        encoder_path = os.path.join(self.model_dir, encoder_filename)
        
        # ä¿å­˜æ¨¡å‹
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_architecture': model.__class__.__name__,
            'model_type': model_type,
            'input_shape': getattr(model, 'input_shape', None),
            'num_classes': getattr(model, 'num_classes', None),
            'history': history or {},
            'epoch': epoch
        }, model_path)
        
        # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
        joblib.dump(preprocessor.label_encoder, encoder_path)
        
        print(f"âœ… {model_type}æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        print(f"âœ… {model_type}æ ‡ç­¾ç¼–ç å™¨å·²ä¿å­˜åˆ°: {encoder_path}")
        
        return model_path, encoder_path
    
    def load_model(self, model, model_type, device):
        """åŠ è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
        model_filename = self.get_model_filename(model_type)
        model_path = os.path.join(self.model_dir, model_filename)
        
        if not os.path.exists(model_path):
            print(f"âš ï¸ {model_type}æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False, None
        
        try:
            checkpoint = torch.load(model_path, map_location=device)
            
            # æ£€æŸ¥æ¨¡å‹ç±»å‹æ˜¯å¦åŒ¹é…
            saved_model_type = checkpoint.get('model_type', 'unknown')
            if saved_model_type != model_type:
                print(f"âš ï¸ æ¨¡å‹ç±»å‹ä¸åŒ¹é…: ä¿å­˜çš„æ˜¯ '{saved_model_type}'ï¼Œå½“å‰è¯·æ±‚çš„æ˜¯ '{model_type}'")
                return False, checkpoint
            
            # åŠ è½½æ¨¡å‹æƒé‡
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… {model_type}æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True, checkpoint
            
        except Exception as e:
            print(f"âŒ {model_type}æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False, None
    
    def load_label_encoder(self, model_type):
        """åŠ è½½æ ‡ç­¾ç¼–ç å™¨"""
        encoder_filename = self.get_label_encoder_filename(model_type)
        encoder_path = os.path.join(self.model_dir, encoder_filename)
        
        if not os.path.exists(encoder_path):
            print(f"âš ï¸ {model_type}æ ‡ç­¾ç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨: {encoder_path}")
            return None
        
        try:
            label_encoder = joblib.load(encoder_path)
            print(f"âœ… {model_type}æ ‡ç­¾ç¼–ç å™¨åŠ è½½æˆåŠŸ")
            return label_encoder
        except Exception as e:
            print(f"âŒ {model_type}æ ‡ç­¾ç¼–ç å™¨åŠ è½½å¤±è´¥: {e}")
            return None
    
    def get_available_models(self):
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        available_models = []
        for model_type in Config.MODEL_VERSIONS:
            model_path = os.path.join(self.model_dir, self.get_model_filename(model_type))
            if os.path.exists(model_path):
                available_models.append(model_type)
        return available_models
    
    def delete_model(self, model_type):
        """åˆ é™¤æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
        model_path = os.path.join(self.model_dir, self.get_model_filename(model_type))
        encoder_path = os.path.join(self.model_dir, self.get_label_encoder_filename(model_type))
        
        deleted = False
        if os.path.exists(model_path):
            os.remove(model_path)
            print(f"âœ… åˆ é™¤æ¨¡å‹æ–‡ä»¶: {model_path}")
            deleted = True
        
        if os.path.exists(encoder_path):
            os.remove(encoder_path)
            print(f"âœ… åˆ é™¤æ ‡ç­¾ç¼–ç å™¨æ–‡ä»¶: {encoder_path}")
            deleted = True
        
        if not deleted:
            print(f"âš ï¸ æœªæ‰¾åˆ°{model_type}æ¨¡å‹æ–‡ä»¶")
        
        return deleted

# åˆ›å»ºå…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = ModelManager()
```

## model_trainer.py

```py
# model_trainer.py (ä¼˜åŒ–ç‰ˆæœ¬ - ä¸­æ–‡æ³¨é‡Š)
import os
import torch
import torch.nn as nn
import numpy as np
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from config.config import Config
from src.model_manager import model_manager

class AdvancedModelTrainer:
    """å¸¦æœ‰æ”¹è¿›æŠ€æœ¯çš„PyTorchæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model, preprocessor, device, model_type='simplified'):
        self.model = model
        self.preprocessor = preprocessor
        self.device = device
        self.model_type = model_type  # æ¨¡å‹ç±»å‹
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': [],
            'learning_rates': []
        }
        
        # æ ‡ç­¾å¹³æ»‘ä»¥è·å¾—æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    def train_epoch(self, train_loader, optimizer, scheduler):
        """ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for data, labels in train_loader:
            data, labels = data.to(self.device), labels.to(self.device)
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
                outputs = self.model(data)
                loss = self.criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # ç»Ÿè®¡ä¿¡æ¯
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        if scheduler:
            scheduler.step()
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = correct / total
        
        return epoch_loss, epoch_acc
    
    def validate(self, val_loader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, labels in val_loader:
                data, labels = data.to(self.device), labels.to(self.device)
                outputs = self.model(data)
                loss = self.criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = correct / total
        
        return epoch_loss, epoch_acc
    
    def train(self, train_loader, val_loader, epochs=Config.EPOCHS, patience=20):
        """ä½¿ç”¨å…ˆè¿›æŠ€æœ¯è®­ç»ƒæ¨¡å‹"""
        # ä½¿ç”¨AdamWå’Œæƒé‡è¡°å‡
        optimizer = torch.optim.AdamW(
            self.model.parameters(), 
            lr=Config.LEARNING_RATE,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # å¸¦çƒ­é‡å¯çš„ä½™å¼¦é€€ç«
        scheduler = CosineAnnealingWarmRestarts(
            optimizer, 
            T_0=10,  # ç¬¬ä¸€æ¬¡é‡å¯çš„è¿­ä»£æ¬¡æ•°
            T_mult=2,  # æ¯æ¬¡é‡å¯åTå¢åŠ çš„å› ç´ 
            eta_min=1e-6  # æœ€å°å­¦ä¹ ç‡
        )
        
        print("å¼€å§‹ä½¿ç”¨å…ˆè¿›æŠ€æœ¯è®­ç»ƒæ¨¡å‹...")
        best_val_acc = 0.0
        patience_counter = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self.train_epoch(train_loader, optimizer, scheduler)
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self.validate(val_loader)
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_acc'].append(val_acc)
            self.history['learning_rates'].append(optimizer.param_groups[0]['lr'])
            
            print(f'å‘¨æœŸ [{epoch+1}/{epochs}] - {self.model_type}æ¨¡å‹')
            print(f'  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}')
            print(f'  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}')
            print(f'  å­¦ä¹ ç‡: {optimizer.param_groups[0]["lr"]:.2e}')
            
            # æ—©åœå’Œæ¨¡å‹ä¿å­˜
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                self.save_model()
                print(f'  âœ… ä¿å­˜æœ€ä½³{self.model_type}æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}')
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                print(f'æ—©åœ: éªŒè¯å‡†ç¡®ç‡è¿ç»­{patience}ä¸ªå‘¨æœŸæœªæå‡')
                break
        
        return self.history
    
    def evaluate(self, test_loader):
        """è¯„ä¼°æ¨¡å‹"""
        test_loss, test_acc = self.validate(test_loader)
        print(f"{self.model_type}æ¨¡å‹æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        return test_loss, test_acc
    
    def save_model(self, model_name=None):
        """ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶"""
        if model_name is None:
            model_name = f"best_{self.model_type}"
        
        # ä½¿ç”¨æ¨¡å‹ç®¡ç†å™¨ä¿å­˜
        model_path, encoder_path = model_manager.save_model(
            self.model, 
            self.preprocessor, 
            self.model_type,
            history=self.history,
            epoch=len(self.history['train_loss'])
        )
        
        return model_path, encoder_path

class ModelTrainer(AdvancedModelTrainer):
    """å‘åå…¼å®¹"""
    pass
```

## time_series_analyzer.py

```py
import os
import torch
import librosa
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import joblib

class TimeSeriesAnalyzer:
    """æ—¶é—´åºåˆ—ä¹å™¨åˆ†æå™¨"""
    
    def __init__(self, model, label_encoder, device):
        self.model = model
        self.label_encoder = label_encoder
        self.device = device
        self.model.eval()
    
    def analyze_audio_timeline(self, audio_path, window_size=3.0, hop_size=1.0, threshold=0.3):
        """
        åˆ†æéŸ³é¢‘æ—¶é—´çº¿ï¼Œæ£€æµ‹ä¹å™¨å‡ºç°çš„æ—¶é—´æ®µ
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            window_size: åˆ†æçª—å£å¤§å°ï¼ˆç§’ï¼‰
            hop_size: çª—å£è·³è·ƒå¤§å°ï¼ˆç§’ï¼‰
            threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        print(f"å¼€å§‹åˆ†æéŸ³é¢‘æ—¶é—´çº¿: {audio_path}")
        
        # åŠ è½½å®Œæ•´éŸ³é¢‘
        y, sr = librosa.load(audio_path, sr=22050)
        duration = len(y) / sr
        print(f"éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’, é‡‡æ ·ç‡: {sr}Hz")
        
        # è®¡ç®—çª—å£å‚æ•°
        window_samples = int(window_size * sr)
        hop_samples = int(hop_size * sr)
        
        # æ»‘åŠ¨çª—å£åˆ†æ
        predictions = []
        timestamps = []
        
        for start in range(0, len(y) - window_samples + 1, hop_samples):
            # æå–å½“å‰çª—å£
            end = start + window_samples
            window_audio = y[start:end]
            timestamp = start / sr  # å½“å‰çª—å£å¼€å§‹æ—¶é—´
            
            # æå–ç‰¹å¾
            features = self._extract_features(window_audio, sr)
            if features is not None:
                # é¢„æµ‹
                prediction = self._predict_single_window(features)
                predictions.append(prediction)
                timestamps.append(timestamp)
            
            # æ˜¾ç¤ºè¿›åº¦
            if len(predictions) % 10 == 0:
                progress = min(100, (end / len(y)) * 100)
                print(f"åˆ†æè¿›åº¦: {progress:.1f}%")
        
        print("æ—¶é—´åºåˆ—åˆ†æå®Œæˆ!")
        return self._process_timeline_results(predictions, timestamps, window_size, threshold)
    
    def _extract_features(self, audio, sr):
        """æå–éŸ³é¢‘ç‰¹å¾"""
        try:
            # æå–Melé¢‘è°±å›¾
            mel_spec = librosa.feature.melspectrogram(
                y=audio, sr=sr, n_mels=128, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # æ ‡å‡†åŒ–
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            # ç¡®ä¿ç‰¹å¾å°ºå¯¸ä¸€è‡´
            target_frames = 130  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
            if log_mel.shape[1] < target_frames:
                # å¡«å……
                pad_width = target_frames - log_mel.shape[1]
                log_mel = np.pad(log_mel, ((0, 0), (0, pad_width)), mode='constant')
            elif log_mel.shape[1] > target_frames:
                # æˆªæ–­
                log_mel = log_mel[:, :target_frames]
            
            return log_mel
            
        except Exception as e:
            print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
            return None
    
    def _predict_single_window(self, features):
        """é¢„æµ‹å•ä¸ªçª—å£"""
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)  # (1, 1, 128, 130)
        input_tensor = input_tensor.to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
        
        # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        probs = probabilities.cpu().numpy()[0]
        results = {}
        
        for idx, prob in enumerate(probs):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            results[instrument] = float(prob)
        
        return results
    
    def _process_timeline_results(self, predictions, timestamps, window_size, threshold):
        """å¤„ç†æ—¶é—´çº¿ç»“æœï¼Œåˆå¹¶è¿ç»­çš„æ—¶é—´æ®µ"""
        timeline = {}
        
        # ä¸ºæ¯ä¸ªä¹å™¨åˆå§‹åŒ–æ—¶é—´çº¿
        for instrument in self.label_encoder.classes_:
            timeline[instrument] = {
                'segments': [],
                'total_duration': 0.0,
                'max_confidence': 0.0,
                'average_confidence': 0.0
            }
        
        # åˆ†ææ¯ä¸ªæ—¶é—´ç‚¹çš„é¢„æµ‹
        for i, (timestamp, pred_dict) in enumerate(zip(timestamps, predictions)):
            # æ‰¾åˆ°å½“å‰çª—å£æœ€å¯èƒ½çš„ä¹å™¨
            if pred_dict:
                best_instrument = max(pred_dict.items(), key=lambda x: x[1])
                instrument, confidence = best_instrument
                
                # åªè®°å½•ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼çš„é¢„æµ‹
                if confidence >= threshold:
                    timeline[instrument]['segments'].append({
                        'start': timestamp,
                        'end': timestamp + window_size,
                        'confidence': confidence
                    })
        
        # åˆå¹¶è¿ç»­çš„æ—¶é—´æ®µå¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        for instrument in timeline.keys():
            segments = timeline[instrument]['segments']
            if segments:
                # æŒ‰å¼€å§‹æ—¶é—´æ’åº
                segments.sort(key=lambda x: x['start'])
                
                # åˆå¹¶è¿ç»­çš„æ—¶é—´æ®µ
                merged_segments = []
                current_segment = segments[0]
                
                for segment in segments[1:]:
                    if segment['start'] <= current_segment['end'] + 0.5:  # å…è®¸0.5ç§’é—´éš™
                        # æ—¶é—´æ®µè¿ç»­ï¼Œåˆå¹¶
                        current_segment['end'] = max(current_segment['end'], segment['end'])
                        current_segment['confidence'] = max(current_segment['confidence'], segment['confidence'])
                    else:
                        # æ—¶é—´æ®µä¸è¿ç»­ï¼Œä¿å­˜å½“å‰æ®µå¹¶å¼€å§‹æ–°æ®µ
                        merged_segments.append(current_segment)
                        current_segment = segment
                
                merged_segments.append(current_segment)
                
                # æ›´æ–°timeline
                timeline[instrument]['segments'] = merged_segments
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                total_duration = sum(seg['end'] - seg['start'] for seg in merged_segments)
                confidences = [seg['confidence'] for seg in merged_segments]
                
                timeline[instrument]['total_duration'] = total_duration
                timeline[instrument]['max_confidence'] = max(confidences) if confidences else 0
                timeline[instrument]['average_confidence'] = np.mean(confidences) if confidences else 0
        
        return timeline
    
    def visualize_timeline(self, timeline, audio_duration, save_path=None):
        """å¯è§†åŒ–æ—¶é—´çº¿ç»“æœ"""
        # ä¹å™¨åç§°æ˜ å°„å­—å…¸
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        fig, ax = plt.subplots(figsize=(14, 8))
        
        # ä½¿ç”¨è‹±æ–‡åç§°
        english_instruments = [instrument_names.get(instr, instr) for instr in timeline.keys()]
        colors = plt.cm.Set3(np.linspace(0, 1, len(english_instruments)))
        
        # ä¸ºæ¯ä¸ªä¹å™¨åˆ›å»ºæ—¶é—´çº¿
        for i, (instrument, data) in enumerate(timeline.items()):
            english_name = instrument_names.get(instrument, instrument)
            segments = data['segments']
            if segments:
                for segment in segments:
                    # ç»˜åˆ¶æ—¶é—´æ®µçŸ©å½¢
                    rect = Rectangle(
                        (segment['start'], i - 0.4),
                        segment['end'] - segment['start'],
                        0.8,
                        facecolor=colors[i],
                        alpha=0.7,
                        edgecolor='black',
                        linewidth=1
                    )
                    ax.add_patch(rect)
                    
                    # æ·»åŠ ç½®ä¿¡åº¦æ–‡æœ¬
                    if segment['end'] - segment['start'] > 2:  # åªåœ¨ä¸å°çš„æ®µä¸Šæ·»åŠ æ–‡æœ¬
                        ax.text(
                            (segment['start'] + segment['end']) / 2,
                            i,
                            f'{segment["confidence"]:.2f}',
                            ha='center',
                            va='center',
                            fontsize=8,
                            fontweight='bold'
                        )
        
        # è®¾ç½®åæ ‡è½´
        ax.set_xlim(0, audio_duration)
        ax.set_ylim(-0.5, len(english_instruments) - 0.5)
        ax.set_xlabel('Time (seconds)')
        ax.set_ylabel('Instrument')
        ax.set_title('Instrument Timeline Analysis')
        
        # è®¾ç½®yè½´åˆ»åº¦ï¼ˆä½¿ç”¨è‹±æ–‡åç§°ï¼‰
        ax.set_yticks(range(len(english_instruments)))
        ax.set_yticklabels(english_instruments)
        
        # æ·»åŠ ç½‘æ ¼
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ å›¾ä¾‹
        legend_elements = []
        for i, (instrument, data) in enumerate(timeline.items()):
            english_name = instrument_names.get(instrument, instrument)
            if data['segments']:
                legend_elements.append(
                    plt.Rectangle((0, 0), 1, 1, facecolor=colors[i], alpha=0.7, 
                                label=f"{english_name} ({data['total_duration']:.1f}s)")
                )
        
        if legend_elements:
            ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.15, 1))
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Timeline chart saved: {save_path}")
        
        plt.show()
        
        return fig

    def generate_report(self, timeline, audio_duration):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        # ä¹å™¨åç§°æ˜ å°„å­—å…¸
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        print("\n" + "="*60)
        print("               Instrument Timeline Analysis Report")
        print("="*60)
        
        # ç»Ÿè®¡æ´»è·ƒä¹å™¨
        active_instruments = []
        for instrument, data in timeline.items():
            if data['segments']:
                english_name = instrument_names.get(instrument, instrument)
                active_instruments.append((english_name, data['total_duration'], instrument))
        
        # æŒ‰æŒç»­æ—¶é—´æ’åº
        active_instruments.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ“Š Active Instruments Statistics ({len(active_instruments)} instruments):")
        print("-" * 60)
        
        for english_name, duration, original_name in active_instruments:
            percentage = (duration / audio_duration) * 100
            max_conf = timeline[original_name]['max_confidence']
            avg_conf = timeline[original_name]['average_confidence']
            segment_count = len(timeline[original_name]['segments'])
            
            print(f"ğŸµ {english_name:15s} | {duration:6.1f}s ({percentage:5.1f}%) | "
                f"Max Confidence: {max_conf:.3f} | Segments: {segment_count}")
        
        print(f"\nğŸ“ˆ Detailed Time Segments:")
        print("-" * 50)
        
        for english_name, duration, original_name in active_instruments:
            segments = timeline[original_name]['segments']
            
            print(f"\n{english_name}:")
            for i, segment in enumerate(segments, 1):
                print(f"  Segment {i}: {segment['start']:6.1f}s - {segment['end']:6.1f}s "
                    f"(Confidence: {segment['confidence']:.3f})")
        
        return active_instruments
```

## train_enhanced.py

```py
# train_enhanced.py (ç®€åŒ–ç‰ˆæœ¬ - ä¸­æ–‡æ³¨é‡Š)
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œä¸»ç¨‹åºï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
    from main import main
    main()
```

## time_series_analyzer.py

```py
import os
import torch
import librosa
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import joblib

class TimeSeriesAnalyzer:
    """æ—¶é—´åºåˆ—ä¹å™¨åˆ†æå™¨"""
    
    def __init__(self, model, label_encoder, device):
        self.model = model
        self.label_encoder = label_encoder
        self.device = device
        self.model.eval()
    
    def analyze_audio_timeline(self, audio_path, window_size=3.0, hop_size=1.0, threshold=0.3):
        """
        åˆ†æéŸ³é¢‘æ—¶é—´çº¿ï¼Œæ£€æµ‹ä¹å™¨å‡ºç°çš„æ—¶é—´æ®µ
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            window_size: åˆ†æçª—å£å¤§å°ï¼ˆç§’ï¼‰
            hop_size: çª—å£è·³è·ƒå¤§å°ï¼ˆç§’ï¼‰
            threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        print(f"å¼€å§‹åˆ†æéŸ³é¢‘æ—¶é—´çº¿: {audio_path}")
        
        # åŠ è½½å®Œæ•´éŸ³é¢‘
        y, sr = librosa.load(audio_path, sr=22050)
        duration = len(y) / sr
        print(f"éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’, é‡‡æ ·ç‡: {sr}Hz")
        
        # è®¡ç®—çª—å£å‚æ•°
        window_samples = int(window_size * sr)
        hop_samples = int(hop_size * sr)
        
        # æ»‘åŠ¨çª—å£åˆ†æ
        predictions = []
        timestamps = []
        
        for start in range(0, len(y) - window_samples + 1, hop_samples):
            # æå–å½“å‰çª—å£
            end = start + window_samples
            window_audio = y[start:end]
            timestamp = start / sr  # å½“å‰çª—å£å¼€å§‹æ—¶é—´
            
            # æå–ç‰¹å¾
            features = self._extract_features(window_audio, sr)
            if features is not None:
                # é¢„æµ‹
                prediction = self._predict_single_window(features)
                predictions.append(prediction)
                timestamps.append(timestamp)
            
            # æ˜¾ç¤ºè¿›åº¦
            if len(predictions) % 10 == 0:
                progress = min(100, (end / len(y)) * 100)
                print(f"åˆ†æè¿›åº¦: {progress:.1f}%")
        
        print("æ—¶é—´åºåˆ—åˆ†æå®Œæˆ!")
        return self._process_timeline_results(predictions, timestamps, window_size, threshold)
    
    def _extract_features(self, audio, sr):
        """æå–éŸ³é¢‘ç‰¹å¾"""
        try:
            # æå–Melé¢‘è°±å›¾
            mel_spec = librosa.feature.melspectrogram(
                y=audio, sr=sr, n_mels=128, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # æ ‡å‡†åŒ–
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            # ç¡®ä¿ç‰¹å¾å°ºå¯¸ä¸€è‡´
            target_frames = 130  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
            if log_mel.shape[1] < target_frames:
                # å¡«å……
                pad_width = target_frames - log_mel.shape[1]
                log_mel = np.pad(log_mel, ((0, 0), (0, pad_width)), mode='constant')
            elif log_mel.shape[1] > target_frames:
                # æˆªæ–­
                log_mel = log_mel[:, :target_frames]
            
            return log_mel
            
        except Exception as e:
            print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
            return None
    
    def _predict_single_window(self, features):
        """é¢„æµ‹å•ä¸ªçª—å£"""
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)  # (1, 1, 128, 130)
        input_tensor = input_tensor.to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
        
        # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        probs = probabilities.cpu().numpy()[0]
        results = {}
        
        for idx, prob in enumerate(probs):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            results[instrument] = float(prob)
        
        return results
    
    def _process_timeline_results(self, predictions, timestamps, window_size, threshold):
        """å¤„ç†æ—¶é—´çº¿ç»“æœï¼Œåˆå¹¶è¿ç»­çš„æ—¶é—´æ®µ"""
        timeline = {}
        
        # ä¸ºæ¯ä¸ªä¹å™¨åˆå§‹åŒ–æ—¶é—´çº¿
        for instrument in self.label_encoder.classes_:
            timeline[instrument] = {
                'segments': [],
                'total_duration': 0.0,
                'max_confidence': 0.0,
                'average_confidence': 0.0
            }
        
        # åˆ†ææ¯ä¸ªæ—¶é—´ç‚¹çš„é¢„æµ‹
        for i, (timestamp, pred_dict) in enumerate(zip(timestamps, predictions)):
            # æ‰¾åˆ°å½“å‰çª—å£æœ€å¯èƒ½çš„ä¹å™¨
            if pred_dict:
                best_instrument = max(pred_dict.items(), key=lambda x: x[1])
                instrument, confidence = best_instrument
                
                # åªè®°å½•ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼çš„é¢„æµ‹
                if confidence >= threshold:
                    timeline[instrument]['segments'].append({
                        'start': timestamp,
                        'end': timestamp + window_size,
                        'confidence': confidence
                    })
        
        # åˆå¹¶è¿ç»­çš„æ—¶é—´æ®µå¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        for instrument in timeline.keys():
            segments = timeline[instrument]['segments']
            if segments:
                # æŒ‰å¼€å§‹æ—¶é—´æ’åº
                segments.sort(key=lambda x: x['start'])
                
                # åˆå¹¶è¿ç»­çš„æ—¶é—´æ®µ
                merged_segments = []
                current_segment = segments[0]
                
                for segment in segments[1:]:
                    if segment['start'] <= current_segment['end'] + 0.5:  # å…è®¸0.5ç§’é—´éš™
                        # æ—¶é—´æ®µè¿ç»­ï¼Œåˆå¹¶
                        current_segment['end'] = max(current_segment['end'], segment['end'])
                        current_segment['confidence'] = max(current_segment['confidence'], segment['confidence'])
                    else:
                        # æ—¶é—´æ®µä¸è¿ç»­ï¼Œä¿å­˜å½“å‰æ®µå¹¶å¼€å§‹æ–°æ®µ
                        merged_segments.append(current_segment)
                        current_segment = segment
                
                merged_segments.append(current_segment)
                
                # æ›´æ–°timeline
                timeline[instrument]['segments'] = merged_segments
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                total_duration = sum(seg['end'] - seg['start'] for seg in merged_segments)
                confidences = [seg['confidence'] for seg in merged_segments]
                
                timeline[instrument]['total_duration'] = total_duration
                timeline[instrument]['max_confidence'] = max(confidences) if confidences else 0
                timeline[instrument]['average_confidence'] = np.mean(confidences) if confidences else 0
        
        return timeline
    
    def visualize_timeline(self, timeline, audio_duration, save_path=None):
        """å¯è§†åŒ–æ—¶é—´çº¿ç»“æœ"""
        # ä¹å™¨åç§°æ˜ å°„å­—å…¸
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        fig, ax = plt.subplots(figsize=(14, 8))
        
        # ä½¿ç”¨è‹±æ–‡åç§°
        english_instruments = [instrument_names.get(instr, instr) for instr in timeline.keys()]
        colors = plt.cm.Set3(np.linspace(0, 1, len(english_instruments)))
        
        # ä¸ºæ¯ä¸ªä¹å™¨åˆ›å»ºæ—¶é—´çº¿
        for i, (instrument, data) in enumerate(timeline.items()):
            english_name = instrument_names.get(instrument, instrument)
            segments = data['segments']
            if segments:
                for segment in segments:
                    # ç»˜åˆ¶æ—¶é—´æ®µçŸ©å½¢
                    rect = Rectangle(
                        (segment['start'], i - 0.4),
                        segment['end'] - segment['start'],
                        0.8,
                        facecolor=colors[i],
                        alpha=0.7,
                        edgecolor='black',
                        linewidth=1
                    )
                    ax.add_patch(rect)
                    
                    # æ·»åŠ ç½®ä¿¡åº¦æ–‡æœ¬
                    if segment['end'] - segment['start'] > 2:  # åªåœ¨ä¸å°çš„æ®µä¸Šæ·»åŠ æ–‡æœ¬
                        ax.text(
                            (segment['start'] + segment['end']) / 2,
                            i,
                            f'{segment["confidence"]:.2f}',
                            ha='center',
                            va='center',
                            fontsize=8,
                            fontweight='bold'
                        )
        
        # è®¾ç½®åæ ‡è½´
        ax.set_xlim(0, audio_duration)
        ax.set_ylim(-0.5, len(english_instruments) - 0.5)
        ax.set_xlabel('Time (seconds)')
        ax.set_ylabel('Instrument')
        ax.set_title('Instrument Timeline Analysis')
        
        # è®¾ç½®yè½´åˆ»åº¦ï¼ˆä½¿ç”¨è‹±æ–‡åç§°ï¼‰
        ax.set_yticks(range(len(english_instruments)))
        ax.set_yticklabels(english_instruments)
        
        # æ·»åŠ ç½‘æ ¼
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ å›¾ä¾‹
        legend_elements = []
        for i, (instrument, data) in enumerate(timeline.items()):
            english_name = instrument_names.get(instrument, instrument)
            if data['segments']:
                legend_elements.append(
                    plt.Rectangle((0, 0), 1, 1, facecolor=colors[i], alpha=0.7, 
                                label=f"{english_name} ({data['total_duration']:.1f}s)")
                )
        
        if legend_elements:
            ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.15, 1))
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Timeline chart saved: {save_path}")
        
        plt.show()
        
        return fig

    def generate_report(self, timeline, audio_duration):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        # ä¹å™¨åç§°æ˜ å°„å­—å…¸
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        print("\n" + "="*60)
        print("               Instrument Timeline Analysis Report")
        print("="*60)
        
        # ç»Ÿè®¡æ´»è·ƒä¹å™¨
        active_instruments = []
        for instrument, data in timeline.items():
            if data['segments']:
                english_name = instrument_names.get(instrument, instrument)
                active_instruments.append((english_name, data['total_duration'], instrument))
        
        # æŒ‰æŒç»­æ—¶é—´æ’åº
        active_instruments.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ“Š Active Instruments Statistics ({len(active_instruments)} instruments):")
        print("-" * 60)
        
        for english_name, duration, original_name in active_instruments:
            percentage = (duration / audio_duration) * 100
            max_conf = timeline[original_name]['max_confidence']
            avg_conf = timeline[original_name]['average_confidence']
            segment_count = len(timeline[original_name]['segments'])
            
            print(f"ğŸµ {english_name:15s} | {duration:6.1f}s ({percentage:5.1f}%) | "
                f"Max Confidence: {max_conf:.3f} | Segments: {segment_count}")
        
        print(f"\nğŸ“ˆ Detailed Time Segments:")
        print("-" * 50)
        
        for english_name, duration, original_name in active_instruments:
            segments = timeline[original_name]['segments']
            
            print(f"\n{english_name}:")
            for i, segment in enumerate(segments, 1):
                print(f"  Segment {i}: {segment['start']:6.1f}s - {segment['end']:6.1f}s "
                    f"(Confidence: {segment['confidence']:.3f})")
        
        return active_instruments
```

## utils.py

```py
# utils.py (æ›´æ–°åçš„ç»˜å›¾å‡½æ•°)
import os
import urllib.request
import zipfile
import numpy as np
import matplotlib.pyplot as plt
import torch
from config.config import Config

def download_dataset():
    """Download dataset"""
    dataset_path = os.path.join(Config.DATA_DIR, "irmas.zip")
    extract_path = Config.DATA_DIR
    
    if not os.path.exists(os.path.join(extract_path, Config.DATASET_NAME)):
        print("Downloading dataset...")
        urllib.request.urlretrieve(Config.DATASET_URL, dataset_path)
        
        # Extract files
        print("Extracting dataset...")
        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        
        print("Dataset download completed!")
    else:
        print("Dataset already exists!")

def plot_training_history(train_losses, val_losses, train_accs, val_accs, save_path=None):
    """Plot training history charts"""
    plt.figure(figsize=(12, 4))
    
    # Accuracy chart
    plt.subplot(1, 2, 1)
    plt.plot(train_accs, label='Training Accuracy')
    plt.plot(val_accs, label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Loss chart
    plt.subplot(1, 2, 2)
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, bbox_inches='tight', dpi=300, 
                   facecolor='white', edgecolor='none')
        print(f"Training chart saved to: {save_path}")
    
    plt.show()

def analyze_model_performance(model, test_loader, label_encoder, device, save_dir):
    """Analyze model performance"""
    from sklearn.metrics import classification_report, confusion_matrix
    import seaborn as sns
    
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for data, labels in test_loader:
            data, labels = data.to(device), labels.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # Classification report
    print("Classification Report:")
    print(classification_report(all_labels, all_preds, 
                              target_names=label_encoder.classes_))
    
    # Confusion matrix
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(all_labels, all_preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    confusion_matrix_path = os.path.join(save_dir, 'confusion_matrix.png')
    plt.savefig(confusion_matrix_path)
    print(f"Confusion matrix saved to: {confusion_matrix_path}")
    plt.show()
    
    # Calculate accuracy for each class
    print("\nClass-wise Accuracy:")
    class_accuracy = {}
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)
    
    for i, class_name in enumerate(label_encoder.classes_):
        mask = all_labels == i
        if np.sum(mask) > 0:
            acc = np.mean(all_preds[mask] == all_labels[mask])
            class_accuracy[class_name] = acc
            print(f"  {class_name}: {acc:.3f}")
    
    return class_accuracy
```

## main.py

```py
# main.py (æ›´æ–°ç‰ˆæœ¬ - ä¸­æ–‡è¾“å‡º)
import os
import torch
import sys
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.config import Config
from src.audio_preprocessor import AudioDataPreprocessor
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier
from src.model_trainer import AdvancedModelTrainer
from src.model_manager import model_manager
from src.utils import download_dataset, plot_training_history, analyze_model_performance

def setup_device():
    """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"ğŸ‰ ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device('cpu')
        print("âŒ ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")
    return device

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='AIéŸ³é¢‘åˆ†æä¸è‡ªåŠ¨æ‰’è°±ç³»ç»Ÿ')
    parser.add_argument('--no-cache', action='store_true', 
                       help='ä¸ä½¿ç”¨ç¼“å­˜ï¼Œé‡æ–°é¢„å¤„ç†æ•°æ®')
    parser.add_argument('--resume', type=str, default=None,
                       help='ä»æŒ‡å®šæ¨¡å‹ç»§ç»­è®­ç»ƒ')
    parser.add_argument('--epochs', type=int, default=Config.EPOCHS,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--no-resume', action='store_true',
                       help='å¼ºåˆ¶ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¿½ç•¥ç°æœ‰æ¨¡å‹')
    parser.add_argument('--basic-model', action='store_true',
                       help='ä½¿ç”¨åŸºç¡€æ¨¡å‹è€Œä¸æ˜¯é«˜çº§æ¨¡å‹')
    parser.add_argument('--no-augmentation', action='store_true',
                       help='ç¦ç”¨æ•°æ®å¢å¼º')
    parser.add_argument('--model-type', type=str, default='simplified', 
                       choices=['basic', 'advanced', 'simplified', 'all'],
                       help='ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ (basic, advanced, simplified, all)')
    parser.add_argument('--train-all', action='store_true',
                       help='è®­ç»ƒæ‰€æœ‰æ¨¡å‹ç±»å‹')
    return parser.parse_args()

def create_model(model_type, input_shape, num_classes, device):
    """æ ¹æ®ç±»å‹åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡"""
    if model_type == 'basic':
        model = create_improved_classifier(input_shape, num_classes)
    elif model_type == 'advanced':
        try:
            model = create_advanced_classifier(input_shape, num_classes)
        except Exception as e:
            print(f"é«˜çº§æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            print("å›é€€åˆ°ç¨³å®šé«˜çº§æ¨¡å‹")
            model = create_stable_advanced_classifier(input_shape, num_classes)
    elif model_type == 'simplified':
        model = create_simplified_classifier(input_shape, num_classes)
    elif model_type == 'stable_advanced':
        model = create_stable_advanced_classifier(input_shape, num_classes)
    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
    
    # ç«‹å³ç§»åŠ¨åˆ°è®¾å¤‡
    model = model.to(device)
    print(f"âœ… {model_type}æ¨¡å‹å·²åˆ›å»ºå¹¶ç§»åŠ¨åˆ°{device}")
    
    return model


def train_single_model(model_type, train_loader, val_loader, test_loader, preprocessor, device, args):
    """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
    print(f"\n{'='*50}")
    print(f"è®­ç»ƒ {model_type} æ¨¡å‹")
    print(f"{'='*50}")
    
    # è·å–è¾“å…¥å½¢çŠ¶
    for data, _ in train_loader:
        input_shape = data.shape[1:]  # (1, 128, 130)
        break
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_shape}")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹å¹¶ç«‹å³ç§»åŠ¨åˆ°è®¾å¤‡
    model = create_model(model_type, input_shape, len(preprocessor.label_encoder.classes_), device)
    
    # ç»Ÿè®¡å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    print(f"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = AdvancedModelTrainer(model, preprocessor, device, model_type=model_type)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤è®­ç»ƒ
    resume_model = not args.no_resume
    if resume_model:
        load_success, checkpoint = model_manager.load_model(model, model_type, device)
        if load_success and checkpoint and 'history' in checkpoint:
            trainer.history = checkpoint['history']
            print(f"æ¢å¤è®­ç»ƒå†å²ï¼Œå·²è®­ç»ƒ {len(trainer.history['train_loss'])} ä¸ªå‘¨æœŸ")
            print(f"âœ… {model_type}æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»§ç»­è®­ç»ƒ...")
        else:
            print(f"âœ… ä»å¤´å¼€å§‹è®­ç»ƒ{model_type}æ¨¡å‹...")
    else:
        print(f"âœ… å¼ºåˆ¶ä»å¤´å¼€å§‹è®­ç»ƒ{model_type}æ¨¡å‹...")
    
    # è®­ç»ƒæ¨¡å‹
    history = trainer.train(train_loader, val_loader, epochs=args.epochs, 
                           patience=Config.EARLY_STOPPING_PATIENCE)
    
    # è¯„ä¼°æ¨¡å‹
    test_loss, test_acc = trainer.evaluate(test_loader)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_model()
    
    return test_acc, history

def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    print("=== AIéŸ³é¢‘åˆ†æä¸è‡ªåŠ¨æ‰’è°±ç³»ç»Ÿ - å¤šç‰ˆæœ¬æ¨¡å‹ ===")
    print(f"ä½¿ç”¨ç¼“å­˜: {not args.no_cache}")
    print(f"æ¨¡å‹ç±»å‹: {args.model_type}")
    print(f"ä½¿ç”¨æ•°æ®å¢å¼º: {not args.no_augmentation}")
    print(f"åŒæ—¶ä¿å­˜å¤šä¸ªç‰ˆæœ¬: {Config.SAVE_MULTIPLE_VERSIONS}")
    
    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    available_models = model_manager.get_available_models()
    if available_models:
        print(f"å·²å­˜åœ¨çš„æ¨¡å‹: {', '.join(available_models)}")
    else:
        print("æ²¡æœ‰æ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹")
    
    # 1. åˆå§‹åŒ–é…ç½®
    print("\n1. åˆå§‹åŒ–é…ç½®...")
    Config.create_directories()
    
    # 2. è®¾ç½®è®¾å¤‡
    print("2. æ£€æŸ¥ç¡¬ä»¶é…ç½®...")
    device = setup_device()
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # 3. æ•°æ®é¢„å¤„ç†
    print("3. æ•°æ®é¢„å¤„ç†...")
    preprocessor = AudioDataPreprocessor(use_cache=not args.no_cache)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œå¯é€‰æ•°æ®å¢å¼º
    use_augmentation = not args.no_augmentation and Config.USE_DATA_AUGMENTATION
    train_loader, val_loader, test_loader, num_classes = preprocessor.create_data_loaders(
        use_cache=not args.no_cache, 
        augment=use_augmentation
    )
    
    print(f"ç±»åˆ«æ•°é‡: {num_classes}")
    
    # ç¡®å®šè¦è®­ç»ƒçš„æ¨¡å‹ç±»å‹
    if args.model_type == 'all' or args.train_all:
        model_types = ['basic', 'simplified', 'advanced']
    else:
        model_types = [args.model_type]
    
    # è®­ç»ƒç»“æœç»Ÿè®¡
    results = {}
    
    # 4. è®­ç»ƒæ¨¡å‹
    for model_type in model_types:
        try:
            test_acc, history = train_single_model(
                model_type, train_loader, val_loader, test_loader, 
                preprocessor, device, args
            )
            results[model_type] = {
                'test_accuracy': test_acc,
                'history': history
            }
            
            å¯è§†åŒ–ç»“æœ
            print(f"8. ç”Ÿæˆ{model_type}æ¨¡å‹å¯è§†åŒ–ç»“æœ...")
            plot_training_history(
                history['train_loss'], history['val_loss'],
                history['train_acc'], history['val_acc'],
                os.path.join(Config.OUTPUT_DIR, f'training_curves_{model_type}.png')
            )
            
            # æ€§èƒ½åˆ†æï¼ˆåªåœ¨æœ€åä¸€ä¸ªæ¨¡å‹ä¸Šæ‰§è¡Œï¼Œé¿å…é‡å¤ï¼‰
            if model_type == model_types[-1]:
                print("9. æ€§èƒ½åˆ†æ...")
                # é‡æ–°åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°
                best_model = create_model(model_type, train_loader.dataset[0][0].shape, num_classes)
                best_model = best_model.to(device)
                load_success, _ = model_manager.load_model(best_model, model_type, device)
                
                if load_success:
                    analyze_model_performance(
                        best_model, test_loader, preprocessor.label_encoder, device, Config.OUTPUT_DIR
                    )
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒ{model_type}æ¨¡å‹æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # 5. è¾“å‡ºè®­ç»ƒæ€»ç»“
    print("\n" + "="*60)
    print("è®­ç»ƒæ€»ç»“")
    print("="*60)
    
    for model_type, result in results.items():
        print(f"{model_type:>10}æ¨¡å‹: æµ‹è¯•å‡†ç¡®ç‡ = {result['test_accuracy']:.4f}")
    
    print(f"\næ¨¡å‹ä¿å­˜ä½ç½®: {Config.MODEL_DIR}")
    print(f"è¾“å‡ºæ–‡ä»¶ä½ç½®: {Config.OUTPUT_DIR}")
    
    # æ˜¾ç¤ºæœ€ç»ˆå¯ç”¨çš„æ¨¡å‹
    final_models = model_manager.get_available_models()
    print(f"æœ€ç»ˆå¯ç”¨æ¨¡å‹: {', '.join(final_models)}")

if __name__ == "__main__":
    main()
```

# test

## diagnose_discrepancy.py

```py
import os
import sys
import torch
import librosa
import numpy as np
import joblib
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.config import Config
from src.instrument_mapper import InstrumentMapper

def diagnose_analysis(audio_path, model_path, label_encoder_path):
    """è¯Šæ–­åˆ†æä¸ä¸€è‡´é—®é¢˜"""
    print("=== åˆ†æä¸ä¸€è‡´è¯Šæ–­ ===")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åŠ è½½æ¨¡å‹å’Œæ ‡ç­¾ç¼–ç å™¨
    label_encoder = joblib.load(label_encoder_path)
    checkpoint = torch.load(model_path, map_location=device)
    
    from src.model_builder import ImprovedInstrumentClassifier
    model = ImprovedInstrumentClassifier((1, 128, 130), len(label_encoder.classes_))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    # åŠ è½½éŸ³é¢‘
    y, sr = librosa.load(audio_path, sr=22050)
    duration = len(y) / sr
    print(f"éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’")
    
    # æµ‹è¯•1: æ•´ä½“åˆ†æï¼ˆç±»ä¼¼inst_model_test.pyï¼‰
    print("\n1. æ•´ä½“åˆ†æï¼ˆ3ç§’ç‰‡æ®µï¼‰:")
    test_overall_analysis(y, sr, model, label_encoder, device)
    
    # æµ‹è¯•2: å¤šä¸ªçª—å£åˆ†æï¼ˆç±»ä¼¼timelineåˆ†æï¼‰
    print("\n2. å¤šçª—å£åˆ†æ:")
    test_multiple_windows(y, sr, model, label_encoder, device)
    
    # æµ‹è¯•3: ä¸åŒæ—¶é—´ç‚¹çš„åˆ†æ
    print("\n3. ä¸åŒæ—¶é—´ç‚¹åˆ†æ:")
    test_different_timestamps(y, sr, model, label_encoder, device, duration)
    
    # æµ‹è¯•4: ç½®ä¿¡åº¦åˆ†å¸ƒ
    print("\n4. ç½®ä¿¡åº¦åˆ†å¸ƒåˆ†æ:")
    test_confidence_distribution(y, sr, model, label_encoder, device)

def test_overall_analysis(y, sr, model, label_encoder, device):
    """æµ‹è¯•æ•´ä½“åˆ†æ"""
    # å–å‰3ç§’ï¼ˆç±»ä¼¼inst_model_test.pyï¼‰
    segment = y[:3*sr] if len(y) > 3*sr else y
    
    features = extract_features(segment, sr)
    if features is not None:
        probs = predict_single_window(features, model, label_encoder, device)
        print_top_predictions(probs, label_encoder, "æ•´ä½“åˆ†æ")

def test_multiple_windows(y, sr, model, label_encoder, device):
    """æµ‹è¯•å¤šä¸ªçª—å£"""
    window_size = 3.0
    hop_size = 1.0
    window_samples = int(window_size * sr)
    hop_samples = int(hop_size * sr)
    
    all_predictions = []
    
    for start in range(0, min(len(y) - window_samples, 10 * hop_samples), hop_samples):
        end = start + window_samples
        segment = y[start:end]
        
        features = extract_features(segment, sr)
        if features is not None:
            probs = predict_single_window(features, model, label_encoder, device)
            all_predictions.append(probs)
    
    # è®¡ç®—å¹³å‡æ¦‚ç‡
    if all_predictions:
        avg_probs = np.mean(all_predictions, axis=0)
        print_top_predictions(avg_probs, label_encoder, "å¤šçª—å£å¹³å‡")

def test_different_timestamps(y, sr, model, label_encoder, device, duration):
    """æµ‹è¯•ä¸åŒæ—¶é—´ç‚¹"""
    test_points = [0, duration/4, duration/2, 3*duration/4]
    
    for i, time_point in enumerate(test_points):
        start_sample = int(time_point * sr)
        end_sample = start_sample + 3 * sr
        
        if end_sample < len(y):
            segment = y[start_sample:end_sample]
            features = extract_features(segment, sr)
            if features is not None:
                probs = predict_single_window(features, model, label_encoder, device)
                print_top_predictions(probs, label_encoder, f"æ—¶é—´ç‚¹ {time_point:.1f}s")

def test_confidence_distribution(y, sr, model, label_encoder, device):
    """æµ‹è¯•ç½®ä¿¡åº¦åˆ†å¸ƒ"""
    window_size = 3.0
    hop_size = 1.0
    window_samples = int(window_size * sr)
    hop_samples = int(hop_size * sr)
    
    instrument_confidences = {inst: [] for inst in label_encoder.classes_}
    
    for start in range(0, min(len(y) - window_samples, 20 * hop_samples), hop_samples):
        end = start + window_samples
        segment = y[start:end]
        
        features = extract_features(segment, sr)
        if features is not None:
            probs = predict_single_window(features, model, label_encoder, device)
            
            for idx, prob in enumerate(probs):
                instrument = label_encoder.inverse_transform([idx])[0]
                instrument_confidences[instrument].append(prob)
    
    # æ‰“å°ç½®ä¿¡åº¦ç»Ÿè®¡
    print("\nç½®ä¿¡åº¦ç»Ÿè®¡:")
    for instrument, confidences in instrument_confidences.items():
        if confidences:
            english_name = InstrumentMapper.get_english_name(instrument)
            avg_conf = np.mean(confidences)
            max_conf = np.max(confidences)
            print(f"  {english_name:15s}: å¹³å‡{avg_conf:.3f}, æœ€å¤§{max_conf:.3f}")

def extract_features(audio, sr, target_shape=(128, 130)):
    """æå–ç‰¹å¾"""
    try:
        mel_spec = librosa.feature.melspectrogram(
            y=audio, sr=sr, n_mels=128, fmax=8000, 
            n_fft=2048, hop_length=512
        )
        log_mel = librosa.power_to_db(mel_spec)
        log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
        
        # è°ƒæ•´å½¢çŠ¶
        if log_mel.shape[1] < target_shape[1]:
            log_mel = np.pad(log_mel, ((0, 0), (0, target_shape[1] - log_mel.shape[1])), mode='constant')
        else:
            log_mel = log_mel[:, :target_shape[1]]
            
        return log_mel
    except Exception as e:
        print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
        return None

def predict_single_window(features, model, label_encoder, device):
    """é¢„æµ‹å•ä¸ªçª—å£"""
    input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0).to(device)
    
    with torch.no_grad():
        outputs = model(input_tensor)
        probabilities = torch.softmax(outputs, dim=1)
    
    return probabilities.cpu().numpy()[0]

def print_top_predictions(probs, label_encoder, title):
    """æ‰“å°å‰3ä¸ªé¢„æµ‹"""
    top_indices = np.argsort(probs)[-3:][::-1]
    
    print(f"\n{title}:")
    for i, idx in enumerate(top_indices):
        instrument = label_encoder.inverse_transform([idx])[0]
        english_name = InstrumentMapper.get_english_name(instrument)
        prob = probs[idx]
        print(f"  {i+1}. {english_name:15s}: {prob:.3f}")

def main():
    """ä¸»è¯Šæ–­å‡½æ•°"""
    audio_path = os.path.join(project_root, "music", "3.flac")
    model_path = os.path.join(project_root, "model", "best_model.pth")
    label_encoder_path = os.path.join(project_root, "model", "best_model_label_encoder.pkl")
    
    diagnose_analysis(audio_path, model_path, label_encoder_path)

if __name__ == "__main__":
    main()
```

## inst_model_test.py

```py
import os
import sys
import torch
import librosa
import numpy as np
import joblib
import matplotlib.pyplot as plt
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.config import Config
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier
from src.instrument_mapper import InstrumentMapper

class MultiModelTester:
    """å¤šæ¨¡å‹æµ‹è¯•ç±»"""
    
    def __init__(self, model_type='simplified'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_type = model_type
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æµ‹è¯•æ¨¡å‹: {model_type}")
        
        # åŠ è½½æ ‡ç­¾ç¼–ç å™¨
        label_encoder_path = os.path.join(project_root, "model", f"model_{model_type}_label_encoder.pkl")
        if not os.path.exists(label_encoder_path):
            print(f"é”™è¯¯: æ ‡ç­¾ç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨ - {label_encoder_path}")
            return
        
        self.label_encoder = joblib.load(label_encoder_path)
        print(f"æ ‡ç­¾ç¼–ç å™¨åŠ è½½æˆåŠŸï¼Œç±»åˆ«: {list(self.label_encoder.classes_)}")
        
        # åŠ è½½æ¨¡å‹
        self.model = self.load_model(model_type)
        if self.model:
            self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            print(f"{model_type}æ¨¡å‹åŠ è½½æˆåŠŸ!")
    
    def load_model(self, model_type):
        """åŠ è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
        model_path = os.path.join(project_root, "model", f"model_{model_type}.pth")
        
        if not os.path.exists(model_path):
            print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ - {model_path}")
            return None
        
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # è·å–æ¨¡å‹å‚æ•°
            input_shape = checkpoint.get('input_shape', (1, 128, 130))
            num_classes = checkpoint.get('num_classes', len(self.label_encoder.classes_))
            
            print(f"æ¨¡å‹å‚æ•° - è¾“å…¥å½¢çŠ¶: {input_shape}, ç±»åˆ«æ•°: {num_classes}")
            
            # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºå¯¹åº”çš„æ¨¡å‹å®ä¾‹
            if model_type == 'basic':
                model = create_improved_classifier(input_shape, num_classes)
            elif model_type == 'advanced':
                model = create_advanced_classifier(input_shape, num_classes)
            elif model_type == 'simplified':
                model = create_simplified_classifier(input_shape, num_classes)
            else:
                print(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
                return None
            
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(self.device)
            
            return model
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None
    
    def preprocess_audio(self, audio_path):
        """é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶"""
        try:
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(audio_path, sr=Config.TARGET_SAMPLE_RATE)
            
            # ç¡®ä¿éŸ³é¢‘é•¿åº¦ä¸€è‡´
            y = librosa.util.fix_length(y, size=Config.TARGET_SAMPLE_RATE * Config.AUDIO_DURATION)
            
            # æå–Melé¢‘è°±å›¾
            mel_spec = librosa.feature.melspectrogram(
                y=y, sr=sr, n_mels=Config.N_MELS, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # æ ‡å‡†åŒ–
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            return log_mel
            
        except Exception as e:
            print(f"å¤„ç†éŸ³é¢‘ {audio_path} æ—¶å‡ºé”™: {e}")
            return None
    
    def predict_single_audio(self, audio_path):
        """é¢„æµ‹å•ä¸ªéŸ³é¢‘æ–‡ä»¶çš„ä¹å™¨ç§ç±»"""
        if self.model is None:
            print("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
            return None, None
            
        print(f"\nåˆ†æéŸ³é¢‘æ–‡ä»¶: {audio_path}")
        
        # é¢„å¤„ç†éŸ³é¢‘
        features = self.preprocess_audio(audio_path)
        if features is None:
            return None, None
        
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)  # (1, 1, 128, 130)
        input_tensor = input_tensor.to(self.device)
        print(f"è¾“å…¥å¼ é‡å½¢çŠ¶: {input_tensor.shape}")
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            top_probs, top_indices = torch.topk(probabilities, k=3)  # è·å–å‰3ä¸ªé¢„æµ‹
        
        # è½¬æ¢ä¸ºnumpy
        top_probs = top_probs.cpu().numpy()[0]
        top_indices = top_indices.cpu().numpy()[0]
        
        # è§£ç é¢„æµ‹ç»“æœ
        predictions = []
        for i, (idx, prob) in enumerate(zip(top_indices, top_probs)):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            predictions.append({
                'rank': i+1,
                'instrument': instrument,
                'probability': prob
            })
        
        return predictions, features
    
    def visualize_prediction(self, predictions, features, audio_name):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        # ä¹å™¨åç§°æ˜ å°„å­—å…¸
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        plt.figure(figsize=(12, 5))
        
        # ç»˜åˆ¶Melé¢‘è°±å›¾
        plt.subplot(1, 2, 1)
        librosa.display.specshow(features, sr=Config.TARGET_SAMPLE_RATE, 
                                hop_length=512, x_axis='time', y_axis='mel')
        plt.colorbar(format='%+2.0f dB')
        plt.title(f'Mel Spectrogram - {audio_name}\n({self.model_type}æ¨¡å‹)')
        
        # ç»˜åˆ¶é¢„æµ‹æ¦‚ç‡
        plt.subplot(1, 2, 2)
        # å°†ç¼©å†™è½¬æ¢ä¸ºè‹±æ–‡åç§°
        instruments = [instrument_names.get(p['instrument'], p['instrument']) for p in predictions]
        probabilities = [p['probability'] for p in predictions]
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(predictions)))
        bars = plt.bar(instruments, probabilities, color=colors)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, prob in zip(bars, probabilities):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{prob:.3f}', ha='center', va='bottom')
        
        plt.title(f'{self.model_type}æ¨¡å‹é¢„æµ‹ç»“æœ')
        plt.xlabel('Instrument')
        plt.ylabel('Probability')
        plt.ylim(0, 1)
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # ä¿å­˜ç»“æœ
        output_path = os.path.join(Config.OUTPUT_DIR, f"prediction_{audio_name}_{self.model_type}.png")
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"é¢„æµ‹ç»“æœå›¾å·²ä¿å­˜: {output_path}")
        
        plt.show()

def test_all_models(audio_path):
    """æµ‹è¯•æ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹"""
    model_types = ['basic', 'simplified', 'advanced']
    all_results = {}
    
    for model_type in model_types:
        print(f"\n{'='*50}")
        print(f"æµ‹è¯• {model_type} æ¨¡å‹")
        print(f"{'='*50}")
        
        tester = MultiModelTester(model_type)
        if tester.model is None:
            print(f"è·³è¿‡ {model_type} æ¨¡å‹ï¼ˆåŠ è½½å¤±è´¥ï¼‰")
            continue
            
        predictions, features = tester.predict_single_audio(audio_path)
        
        if predictions:
            print(f"\n=== {model_type}æ¨¡å‹é¢„æµ‹ç»“æœ ===")
            for pred in predictions:
                instrument_name = InstrumentMapper.get_english_name(pred['instrument'])
                print(f"{pred['rank']}. {instrument_name}: {pred['probability']:.3f}")
            
            # å¯è§†åŒ–ç»“æœ
            audio_name = os.path.splitext(os.path.basename(audio_path))[0]
            tester.visualize_prediction(predictions, features, audio_name)
            
            # å­˜å‚¨ç»“æœ
            all_results[model_type] = {
                'top_prediction': predictions[0],
                'all_predictions': predictions
            }
    
    return all_results

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ¨¡å‹æµ‹è¯•')
    parser.add_argument('--model-type', type=str, default='all', 
                       choices=['basic', 'simplified', 'advanced', 'all'],
                       help='è¦æµ‹è¯•çš„æ¨¡å‹ç±»å‹')
    parser.add_argument('--audio-path', type=str, 
                       default=os.path.join(project_root, "music", "3.flac"),
                       help='æµ‹è¯•éŸ³é¢‘è·¯å¾„')
    
    args = parser.parse_args()
    
    print("=== AIéŸ³é¢‘åˆ†æä¸è‡ªåŠ¨æ‰’è°±ç³»ç»Ÿ - å¤šæ¨¡å‹æµ‹è¯• ===")
    
    # 1. åˆå§‹åŒ–é…ç½®
    Config.create_directories()
    
    # 2. æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.audio_path):
        print(f"é”™è¯¯: æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ - {args.audio_path}")
        print("è¯·ç¡®ä¿éŸ³é¢‘æ–‡ä»¶å­˜åœ¨")
        return
    
    if args.model_type == 'all':
        # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
        results = test_all_models(args.audio_path)
        
        # è¾“å‡ºæ¯”è¾ƒç»“æœ
        print(f"\n{'='*60}")
        print("æ¨¡å‹æ¯”è¾ƒç»“æœ")
        print(f"{'='*60}")
        
        for model_type, result in results.items():
            top_pred = result['top_prediction']
            instrument_name = InstrumentMapper.get_english_name(top_pred['instrument'])
            print(f"{model_type:>10}æ¨¡å‹: {instrument_name:15s} (ç½®ä¿¡åº¦: {top_pred['probability']:.3f})")
    
    else:
        # æµ‹è¯•å•ä¸ªæ¨¡å‹
        tester = MultiModelTester(args.model_type)
        if tester.model is None:
            return
            
        predictions, features = tester.predict_single_audio(args.audio_path)
        
        if predictions:
            print(f"\n=== {args.model_type}æ¨¡å‹é¢„æµ‹ç»“æœ ===")
            for pred in predictions:
                instrument_name = InstrumentMapper.get_english_name(pred['instrument'])
                print(f"{pred['rank']}. {instrument_name}: {pred['probability']:.3f}")
            
            # å¯è§†åŒ–ç»“æœ
            audio_name = os.path.splitext(os.path.basename(args.audio_path))[0]
            tester.visualize_prediction(predictions, features, audio_name)
            
            # è¾“å‡ºæœ€å¯èƒ½çš„ä¹å™¨
            top_prediction = predictions[0]
            instrument_name = InstrumentMapper.get_english_name(top_prediction['instrument'])
            print(f"\nğŸµ æœ€å¯èƒ½çš„ä¹å™¨: {instrument_name} (ç½®ä¿¡åº¦: {top_prediction['probability']:.3f})")

if __name__ == "__main__":
    main()
```

## model_evaluation.py

```py
import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_curve, auc
from sklearn.preprocessing import label_binarize
import argparse

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.config import Config
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier
from src.audio_preprocessor import AudioDataPreprocessor
from src.instrument_mapper import InstrumentMapper

class MultiModelEvaluator:
    """å¤šæ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model, label_encoder, device, model_type):
        self.model = model
        self.label_encoder = label_encoder
        self.device = device
        self.model_type = model_type
        self.model.eval()
    
    def comprehensive_evaluation(self, test_loader):
        """å…¨é¢è¯„ä¼°æ¨¡å‹"""
        print(f"\n=== {self.model_type}æ¨¡å‹å…¨é¢è¯„ä¼° ===")
        
        # 1. åŸºç¡€æŒ‡æ ‡
        accuracy, class_accuracy = self.calculate_accuracy(test_loader)
        
        # 2. è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        y_true, y_pred, y_prob = self.get_predictions(test_loader)
        self.print_classification_report(y_true, y_pred)
        
        # 3. æ··æ·†çŸ©é˜µ
        self.plot_confusion_matrix(y_true, y_pred)
        
        # 4. ROCæ›²çº¿å’ŒAUC
        self.plot_roc_curves(y_true, y_prob)
        
        # 5. å„ç±»åˆ«æ€§èƒ½åˆ†æ
        self.analyze_class_performance(y_true, y_pred, y_prob)
        
        return {
            'accuracy': accuracy,
            'class_accuracy': class_accuracy,
            'y_true': y_true,
            'y_pred': y_pred,
            'y_prob': y_prob
        }
    
    def calculate_accuracy(self, test_loader):
        """è®¡ç®—å‡†ç¡®ç‡"""
        self.model.eval()
        correct = 0
        total = 0
        class_correct = {i: 0 for i in range(len(self.label_encoder.classes_))}
        class_total = {i: 0 for i in range(len(self.label_encoder.classes_))}
        
        with torch.no_grad():
            for data, labels in test_loader:
                data, labels = data.to(self.device), labels.to(self.device)
                outputs = self.model(data)
                _, predicted = torch.max(outputs.data, 1)
                
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                # å„ç±»åˆ«å‡†ç¡®ç‡
                for i in range(labels.size(0)):
                    label = labels[i].item()
                    pred = predicted[i].item()
                    class_total[label] += 1
                    if label == pred:
                        class_correct[label] += 1
        
        accuracy = 100 * correct / total
        class_accuracy = {}
        
        print(f"\nğŸ“Š {self.model_type}æ¨¡å‹æ€»ä½“å‡†ç¡®ç‡: {accuracy:.2f}%")
        print(f"\nğŸ“ˆ å„ç±»åˆ«å‡†ç¡®ç‡:")
        print("-" * 50)
        
        for i in range(len(self.label_encoder.classes_)):
            instrument = self.label_encoder.inverse_transform([i])[0]
            english_name = InstrumentMapper.get_english_name(instrument)
            if class_total[i] > 0:
                acc = 100 * class_correct[i] / class_total[i]
                class_accuracy[english_name] = acc
                print(f"  {english_name:15s}: {acc:6.2f}% ({class_correct[i]}/{class_total[i]})")
            else:
                class_accuracy[english_name] = 0
                print(f"  {english_name:15s}:   0.00% (0/0)")
        
        return accuracy, class_accuracy
    
    def get_predictions(self, test_loader):
        """è·å–é¢„æµ‹ç»“æœ"""
        self.model.eval()
        all_preds = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for data, labels in test_loader:
                data, labels = data.to(self.device), labels.to(self.device)
                outputs = self.model(data)
                probabilities = torch.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs.data, 1)
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probabilities.cpu().numpy())
        
        return np.array(all_labels), np.array(all_preds), np.array(all_probs)
    
    def print_classification_report(self, y_true, y_pred):
        """æ‰“å°è¯¦ç»†åˆ†ç±»æŠ¥å‘Š"""
        target_names = [InstrumentMapper.get_english_name(instr) 
                       for instr in self.label_encoder.classes_]
        
        print(f"\nğŸ“‹ {self.model_type}æ¨¡å‹è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print("=" * 70)
        report = classification_report(y_true, y_pred, target_names=target_names, digits=4)
        print(report)
    
    def plot_confusion_matrix(self, y_true, y_pred):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        target_names = [InstrumentMapper.get_english_name(instr) 
                       for instr in self.label_encoder.classes_]
        
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=target_names, yticklabels=target_names)
        plt.title(f'{self.model_type}æ¨¡å‹ - Confusion Matrix', fontsize=16, fontweight='bold')
        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        output_path = os.path.join(Config.OUTPUT_DIR, f'confusion_matrix_{self.model_type}.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ… {self.model_type}æ¨¡å‹æ··æ·†çŸ©é˜µå·²ä¿å­˜: {output_path}")
        plt.show()
    
    def plot_roc_curves(self, y_true, y_prob):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        # å°†æ ‡ç­¾äºŒå€¼åŒ–
        y_true_bin = label_binarize(y_true, classes=range(len(self.label_encoder.classes_)))
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ROCæ›²çº¿å’ŒAUC
        fpr = {}
        tpr = {}
        roc_auc = {}
        
        plt.figure(figsize=(10, 8))
        
        for i in range(len(self.label_encoder.classes_)):
            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
            
            instrument = InstrumentMapper.get_english_name(
                self.label_encoder.inverse_transform([i])[0]
            )
            plt.plot(fpr[i], tpr[i], label=f'{instrument} (AUC = {roc_auc[i]:.3f})')
        
        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'{self.model_type}æ¨¡å‹ - ROC Curves')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        
        output_path = os.path.join(Config.OUTPUT_DIR, f'roc_curves_{self.model_type}.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ… {self.model_type}æ¨¡å‹ROCæ›²çº¿å·²ä¿å­˜: {output_path}")
        plt.show()
        
        # æ˜¾ç¤ºAUCç»Ÿè®¡
        print(f"\nğŸ¯ {self.model_type}æ¨¡å‹AUCç»Ÿè®¡:")
        print("-" * 40)
        avg_auc = np.mean(list(roc_auc.values()))
        print(f"å¹³å‡AUC: {avg_auc:.4f}")
        for i, auc_val in roc_auc.items():
            instrument = InstrumentMapper.get_english_name(
                self.label_encoder.inverse_transform([i])[0]
            )
            print(f"  {instrument:15s}: {auc_val:.4f}")
    
    def analyze_class_performance(self, y_true, y_pred, y_prob):
        """åˆ†æå„ç±»åˆ«æ€§èƒ½"""
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        print(f"\nğŸ¼ {self.model_type}æ¨¡å‹å„ç±»åˆ«è¯¦ç»†æ€§èƒ½æŒ‡æ ‡:")
        print("=" * 70)
        print(f"{'Instrument':<20} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
        print("-" * 70)
        
        precision_per_class = precision_score(y_true, y_pred, average=None)
        recall_per_class = recall_score(y_true, y_pred, average=None)
        f1_per_class = f1_score(y_true, y_pred, average=None)
        
        for i in range(len(self.label_encoder.classes_)):
            instrument = InstrumentMapper.get_english_name(
                self.label_encoder.inverse_transform([i])[0]
            )
            support = np.sum(y_true == i)
            
            print(f"{instrument:<20} {precision_per_class[i]:<10.4f} "
                  f"{recall_per_class[i]:<10.4f} {f1_per_class[i]:<10.4f} {support:<10}")
        
        # å®è§‚å¹³å‡å’ŒåŠ æƒå¹³å‡
        macro_precision = precision_score(y_true, y_pred, average='macro')
        macro_recall = recall_score(y_true, y_pred, average='macro')
        macro_f1 = f1_score(y_true, y_pred, average='macro')
        
        weighted_precision = precision_score(y_true, y_pred, average='weighted')
        weighted_recall = recall_score(y_true, y_pred, average='weighted')
        weighted_f1 = f1_score(y_true, y_pred, average='weighted')
        
        print("-" * 70)
        print(f"{'Macro Avg':<20} {macro_precision:<10.4f} {macro_recall:<10.4f} {macro_f1:<10.4f}")
        print(f"{'Weighted Avg':<20} {weighted_precision:<10.4f} {weighted_recall:<10.4f} {weighted_f1:<10.4f}")

def load_model_and_evaluator(model_type, device):
    """åŠ è½½æ¨¡å‹å’Œè¯„ä¼°å™¨"""
    import joblib
    
    # åŠ è½½æ ‡ç­¾ç¼–ç å™¨
    label_encoder_path = os.path.join(project_root, "model", f"model_{model_type}_label_encoder.pkl")
    if not os.path.exists(label_encoder_path):
        print(f"é”™è¯¯: æ ‡ç­¾ç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨ - {label_encoder_path}")
        return None
    
    label_encoder = joblib.load(label_encoder_path)
    
    # åŠ è½½æ¨¡å‹
    model_path = os.path.join(project_root, "model", f"model_{model_type}.pth")
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ - {model_path}")
        return None
    
    checkpoint = torch.load(model_path, map_location=device)
    
    # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºå¯¹åº”çš„æ¨¡å‹
    input_shape = checkpoint.get('input_shape', (1, 128, 130))
    num_classes = checkpoint.get('num_classes', len(label_encoder.classes_))
    
    if model_type == 'basic':
        model = create_improved_classifier(input_shape, num_classes)
    elif model_type == 'advanced':
        model = create_advanced_classifier(input_shape, num_classes)
    elif model_type == 'simplified':
        model = create_simplified_classifier(input_shape, num_classes)
    else:
        print(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
        return None
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    
    print(f"âœ… {model_type}æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    return MultiModelEvaluator(model, label_encoder, device, model_type)

def compare_all_models(test_loader, device):
    """æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹"""
    model_types = ['basic', 'simplified', 'advanced']
    results = {}
    
    for model_type in model_types:
        evaluator = load_model_and_evaluator(model_type, device)
        if evaluator is None:
            continue
            
        result = evaluator.comprehensive_evaluation(test_loader)
        results[model_type] = result
    
    # ç»˜åˆ¶æ¯”è¾ƒå›¾
    if len(results) > 1:
        plt.figure(figsize=(12, 6))
        
        # å‡†ç¡®ç‡æ¯”è¾ƒ
        models = list(results.keys())
        accuracies = [results[model]['accuracy'] for model in models]
        
        plt.subplot(1, 2, 1)
        bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'])
        plt.title('æ¨¡å‹å‡†ç¡®ç‡æ¯”è¾ƒ', fontsize=14)
        plt.ylabel('å‡†ç¡®ç‡ (%)', fontsize=12)
        plt.ylim(0, 100)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, accuracy in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{accuracy:.2f}%', ha='center', va='bottom', fontsize=10)
        
        # å„ç±»åˆ«å‡†ç¡®ç‡æ¯”è¾ƒ
        plt.subplot(1, 2, 2)
        instruments = list(results[models[0]]['class_accuracy'].keys())
        x = np.arange(len(instruments))
        width = 0.25
        
        for i, model_type in enumerate(models):
            accuracies = [results[model_type]['class_accuracy'][inst] for inst in instruments]
            plt.bar(x + i*width, accuracies, width, label=model_type)
        
        plt.xlabel('ä¹å™¨ç±»åˆ«', fontsize=12)
        plt.ylabel('å‡†ç¡®ç‡ (%)', fontsize=12)
        plt.title('å„ç±»åˆ«å‡†ç¡®ç‡æ¯”è¾ƒ', fontsize=14)
        plt.xticks(x + width, instruments, rotation=45, ha='right')
        plt.legend()
        plt.tight_layout()
        
        output_path = os.path.join(Config.OUTPUT_DIR, 'model_comparison.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ… æ¨¡å‹æ¯”è¾ƒå›¾å·²ä¿å­˜: {output_path}")
        plt.show()
        
        # è¾“å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(results, key=lambda x: results[x]['accuracy'])
        print(f"\nğŸ‰ æœ€ä½³æ¨¡å‹: {best_model} (å‡†ç¡®ç‡: {results[best_model]['accuracy']:.2f}%)")

def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ¨¡å‹è¯„ä¼°')
    parser.add_argument('--model-type', type=str, default='all', 
                       choices=['basic', 'simplified', 'advanced', 'all'],
                       help='è¦è¯„ä¼°çš„æ¨¡å‹ç±»å‹')
    
    args = parser.parse_args()
    
    print("=== å¤šæ¨¡å‹è®­ç»ƒç¨‹åº¦è¯„ä¼° ===")
    
    # 1. åˆå§‹åŒ–é…ç½®
    Config.create_directories()
    
    # 2. è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 3. åŠ è½½æµ‹è¯•æ•°æ®
    print("\nåŠ è½½æµ‹è¯•æ•°æ®...")
    preprocessor = AudioDataPreprocessor(use_cache=True)
    _, _, test_loader, num_classes = preprocessor.create_data_loaders(use_cache=True, augment=False)
    
    if args.model_type == 'all':
        # æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹
        compare_all_models(test_loader, device)
    else:
        # è¯„ä¼°å•ä¸ªæ¨¡å‹
        evaluator = load_model_and_evaluator(args.model_type, device)
        if evaluator is None:
            return
            
        results = evaluator.comprehensive_evaluation(test_loader)
        
        # æ¨¡å‹è®­ç»ƒç¨‹åº¦æ€»ç»“
        print(f"\nğŸ¯ {args.model_type}æ¨¡å‹è®­ç»ƒç¨‹åº¦æ€»ç»“:")
        print("=" * 50)
        accuracy = results['accuracy']
        
        if accuracy > 90:
            print("âœ… ä¼˜ç§€! æ¨¡å‹è®­ç»ƒå¾—å¾ˆå¥½")
        elif accuracy > 80:
            print("âœ… è‰¯å¥½! æ¨¡å‹è®­ç»ƒå¾—ä¸é”™")  
        elif accuracy > 70:
            print("âš ï¸ ä¸€èˆ¬! æ¨¡å‹éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        else:
            print("âŒ è¾ƒå·®! å»ºè®®é‡æ–°è®­ç»ƒæˆ–è°ƒæ•´æ¨¡å‹")
        
        print(f"æ€»ä½“å‡†ç¡®ç‡: {accuracy:.2f}%")

if __name__ == "__main__":
    main()
```

## multi_scale_analyzer.py

```py
import os
import torch
import librosa
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

class MultiScaleTimeSeriesAnalyzer:
    """å¤šå°ºåº¦æ—¶é—´åºåˆ—åˆ†æå™¨"""
    
    def __init__(self, model, label_encoder, device):
        self.model = model
        self.label_encoder = label_encoder
        self.device = device
        self.model.eval()
    
    def multi_scale_analysis(self, audio_path, scales=None):
        """
        å¤šå°ºåº¦åˆ†æ
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            scales: åˆ†æå°ºåº¦é…ç½®
        """
        if scales is None:
            scales = [
                {'window_size': 3.0, 'hop_size': 1.0, 'threshold': 0.3, 'weight': 1.0, 'name': 'Coarse'},
                {'window_size': 1.5, 'hop_size': 0.5, 'threshold': 0.2, 'weight': 0.8, 'name': 'Medium'},
                {'window_size': 1.0, 'hop_size': 0.3, 'threshold': 0.15, 'weight': 0.6, 'name': 'Fine'}
            ]
        
        print("å¼€å§‹å¤šå°ºåº¦åˆ†æ...")
        all_results = []
        
        for scale in scales:
            print(f"\nğŸ”§ è¿è¡Œ {scale['name']} å°ºåº¦åˆ†æ:")
            print(f"   çª—å£: {scale['window_size']}s, è·³è·ƒ: {scale['hop_size']}s, é˜ˆå€¼: {scale['threshold']}")
            
            timeline = self._single_scale_analysis(audio_path, scale)
            all_results.append({
                'scale': scale,
                'timeline': timeline
            })
        
        # èåˆå¤šå°ºåº¦ç»“æœ
        fused_timeline = self._fuse_multi_scale_results(all_results)
        return fused_timeline
    
    def _single_scale_analysis(self, audio_path, scale_config):
        """å•å°ºåº¦åˆ†æ"""
        y, sr = librosa.load(audio_path, sr=22050)
        
        window_size = scale_config['window_size']
        hop_size = scale_config['hop_size']
        threshold = scale_config['threshold']
        
        window_samples = int(window_size * sr)
        hop_samples = int(hop_size * sr)
        
        predictions = []
        timestamps = []
        
        for start in range(0, len(y) - window_samples + 1, hop_samples):
            end = start + window_samples
            window_audio = y[start:end]
            timestamp = start / sr
            
            features = self._extract_features(window_audio, sr)
            if features is not None:
                prediction = self._predict_single_window(features)
                predictions.append(prediction)
                timestamps.append(timestamp)
        
        return self._process_timeline_results(predictions, timestamps, window_size, threshold)
    
    def _fuse_multi_scale_results(self, all_results):
        """èåˆå¤šå°ºåº¦ç»“æœ"""
        fused_timeline = {}
        
        # åˆå§‹åŒ–æ—¶é—´çº¿
        for instrument in self.label_encoder.classes_:
            fused_timeline[instrument] = {
                'segments': [],
                'total_duration': 0.0,
                'max_confidence': 0.0,
                'average_confidence': 0.0,
                'scale_scores': []  # è®°å½•æ¥è‡ªä¸åŒå°ºåº¦çš„å¾—åˆ†
            }
        
        # èåˆé€»è¾‘
        for result in all_results:
            scale = result['scale']
            timeline = result['timeline']
            weight = scale['weight']
            
            for instrument, data in timeline.items():
                for segment in data['segments']:
                    # åŠ æƒèåˆ
                    weighted_segment = segment.copy()
                    weighted_segment['confidence'] *= weight
                    weighted_segment['scale'] = scale['name']
                    weighted_segment['original_confidence'] = segment['confidence']
                    
                    fused_timeline[instrument]['segments'].append(weighted_segment)
        
        # åˆå¹¶å’Œè¿‡æ»¤èåˆåçš„æ—¶é—´æ®µ
        for instrument in fused_timeline.keys():
            segments = fused_timeline[instrument]['segments']
            if segments:
                # æŒ‰å¼€å§‹æ—¶é—´æ’åº
                segments.sort(key=lambda x: x['start'])
                
                # åˆå¹¶é‡å æ—¶é—´æ®µ
                merged_segments = []
                current_segment = segments[0]
                
                for segment in segments[1:]:
                    if self._segments_overlap(current_segment, segment):
                        # åˆå¹¶é‡å æ®µï¼Œå–æœ€å¤§ç½®ä¿¡åº¦
                        current_segment['end'] = max(current_segment['end'], segment['end'])
                        current_segment['confidence'] = max(current_segment['confidence'], segment['confidence'])
                        if 'scale_scores' not in current_segment:
                            current_segment['scale_scores'] = []
                        current_segment['scale_scores'].append({
                            'scale': segment.get('scale', 'unknown'),
                            'confidence': segment.get('original_confidence', segment['confidence'])
                        })
                    else:
                        merged_segments.append(current_segment)
                        current_segment = segment
                
                merged_segments.append(current_segment)
                
                # è¿‡æ»¤ä½ç½®ä¿¡åº¦æ®µ
                filtered_segments = [seg for seg in merged_segments if seg['confidence'] >= 0.15]
                
                fused_timeline[instrument]['segments'] = filtered_segments
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                if filtered_segments:
                    total_duration = sum(seg['end'] - seg['start'] for seg in filtered_segments)
                    confidences = [seg['confidence'] for seg in filtered_segments]
                    
                    fused_timeline[instrument]['total_duration'] = total_duration
                    fused_timeline[instrument]['max_confidence'] = max(confidences)
                    fused_timeline[instrument]['average_confidence'] = np.mean(confidences)
        
        return fused_timeline
    
    def _segments_overlap(self, seg1, seg2, gap_tolerance=0.5):
        """åˆ¤æ–­ä¸¤ä¸ªæ—¶é—´æ®µæ˜¯å¦é‡å """
        return seg1['end'] + gap_tolerance >= seg2['start'] and seg2['end'] + gap_tolerance >= seg1['start']
    
    def _extract_features(self, audio, sr, target_shape=(128, 130)):
        """æå–éŸ³é¢‘ç‰¹å¾"""
        try:
            mel_spec = librosa.feature.melspectrogram(
                y=audio, sr=sr, n_mels=128, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            # è°ƒæ•´å½¢çŠ¶
            if log_mel.shape[1] < target_shape[1]:
                log_mel = np.pad(log_mel, ((0, 0), (0, target_shape[1] - log_mel.shape[1])), mode='constant')
            else:
                log_mel = log_mel[:, :target_shape[1]]
                
            return log_mel
        except Exception as e:
            print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
            return None
    
    def _predict_single_window(self, features):
        """é¢„æµ‹å•ä¸ªçª—å£"""
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)
        input_tensor = input_tensor.to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
        
        probs = probabilities.cpu().numpy()[0]
        results = {}
        
        for idx, prob in enumerate(probs):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            results[instrument] = float(prob)
        
        return results
    
    def _process_timeline_results(self, predictions, timestamps, window_size, threshold):
        """å¤„ç†æ—¶é—´çº¿ç»“æœ"""
        timeline = {}
        
        for instrument in self.label_encoder.classes_:
            timeline[instrument] = {
                'segments': [],
                'total_duration': 0.0,
                'max_confidence': 0.0,
                'average_confidence': 0.0
            }
        
        for timestamp, pred_dict in zip(timestamps, predictions):
            if pred_dict:
                best_instrument, best_confidence = max(pred_dict.items(), key=lambda x: x[1])
                if best_confidence >= threshold:
                    timeline[best_instrument]['segments'].append({
                        'start': timestamp,
                        'end': timestamp + window_size,
                        'confidence': best_confidence
                    })
        
        # åˆå¹¶è¿ç»­æ—¶é—´æ®µ
        for instrument in timeline.keys():
            segments = timeline[instrument]['segments']
            if segments:
                segments.sort(key=lambda x: x['start'])
                merged_segments = []
                current_segment = segments[0]
                
                for segment in segments[1:]:
                    if segment['start'] <= current_segment['end'] + 0.5:
                        current_segment['end'] = max(current_segment['end'], segment['end'])
                        current_segment['confidence'] = max(current_segment['confidence'], segment['confidence'])
                    else:
                        merged_segments.append(current_segment)
                        current_segment = segment
                
                merged_segments.append(current_segment)
                timeline[instrument]['segments'] = merged_segments
                
                # è®¡ç®—ç»Ÿè®¡
                total_duration = sum(seg['end'] - seg['start'] for seg in merged_segments)
                confidences = [seg['confidence'] for seg in merged_segments]
                
                timeline[instrument]['total_duration'] = total_duration
                timeline[instrument]['max_confidence'] = max(confidences) if confidences else 0
                timeline[instrument]['average_confidence'] = np.mean(confidences) if confidences else 0
        
        return timeline
```

## test_timeline.py

```py
import os
import sys
import torch
import joblib
import librosa
import numpy as np
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.config import Config
from src.model_builder import create_improved_classifier
from src.advanced_models import create_advanced_classifier, create_simplified_classifier
from src.time_series_analyzer import TimeSeriesAnalyzer

def load_model_with_fix(model_type, device):
    """åŠ è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹å¹¶ä¿®å¤è¾“å…¥å½¢çŠ¶é—®é¢˜"""
    # åŠ è½½æ ‡ç­¾ç¼–ç å™¨
    label_encoder_path = os.path.join(project_root, "model", f"model_{model_type}_label_encoder.pkl")
    if not os.path.exists(label_encoder_path):
        print(f"é”™è¯¯: æ ‡ç­¾ç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨ - {label_encoder_path}")
        return None, None
    
    label_encoder = joblib.load(label_encoder_path)
    
    # åŠ è½½æ¨¡å‹
    model_path = os.path.join(project_root, "model", f"model_{model_type}.pth")
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ - {model_path}")
        return None, None
    
    checkpoint = torch.load(model_path, map_location=device)
    
    # ä¿®å¤è¾“å…¥å½¢çŠ¶
    input_shape = (1, 128, 130)  # (channels, height, width)
    num_classes = len(label_encoder.classes_)
    
    print(f"ä½¿ç”¨è¾“å…¥å½¢çŠ¶: {input_shape}")
    print(f"ç±»åˆ«æ•°é‡: {num_classes}")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºå¯¹åº”çš„æ¨¡å‹
    if model_type == 'basic':
        model = create_improved_classifier(input_shape, num_classes)
    elif model_type == 'advanced':
        model = create_advanced_classifier(input_shape, num_classes)
    elif model_type == 'simplified':
        model = create_simplified_classifier(input_shape, num_classes)
    else:
        print(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
        return None, None
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    
    return model, label_encoder

def ensure_feature_shape(features, target_shape=(128, 130)):
    """ç¡®ä¿ç‰¹å¾å½¢çŠ¶ä¸€è‡´"""
    if features.shape != target_shape:
        # è°ƒæ•´åˆ°ç›®æ ‡å½¢çŠ¶
        if features.shape[1] < target_shape[1]:
            # å¡«å……
            pad_width = target_shape[1] - features.shape[1]
            features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')
        elif features.shape[1] > target_shape[1]:
            # æˆªæ–­
            features = features[:, :target_shape[1]]
    
    return features

class FixedTimeSeriesAnalyzer(TimeSeriesAnalyzer):
    """ä¿®å¤çš„æ—¶é—´åºåˆ—åˆ†æå™¨"""
    
    def __init__(self, model, label_encoder, device, model_type):
        super().__init__(model, label_encoder, device)
        self.model_type = model_type
    
    def _extract_features(self, audio, sr):
        """æå–éŸ³é¢‘ç‰¹å¾"""
        try:
            # æå–Melé¢‘è°±å›¾
            mel_spec = librosa.feature.melspectrogram(
                y=audio, sr=sr, n_mels=128, fmax=8000, 
                n_fft=2048, hop_length=512
            )
            log_mel = librosa.power_to_db(mel_spec)
            
            # æ ‡å‡†åŒ–
            log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
            
            # ç¡®ä¿ç‰¹å¾å°ºå¯¸ä¸€è‡´
            log_mel = ensure_feature_shape(log_mel, (128, 130))
            
            return log_mel
            
        except Exception as e:
            print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
            return None
    
    def _predict_single_window(self, features):
        """é¢„æµ‹å•ä¸ªçª—å£"""
        # ç¡®ä¿ç‰¹å¾å½¢çŠ¶æ­£ç¡®
        features = ensure_feature_shape(features, (128, 130))
        
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼: (1, 1, 128, 130)
        input_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)
        input_tensor = input_tensor.to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
        
        # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        probs = probabilities.cpu().numpy()[0]
        results = {}
        
        for idx, prob in enumerate(probs):
            instrument = self.label_encoder.inverse_transform([idx])[0]
            results[instrument] = float(prob)
        
        return results
    
    def generate_report(self, timeline, audio_duration):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        from src.instrument_mapper import InstrumentMapper
        
        # ä¹å™¨åç§°æ˜ å°„å­—å…¸
        instrument_names = {
            'cel': 'Cello',
            'cla': 'Clarinet', 
            'flu': 'Flute',
            'gac': 'Acoustic Guitar',
            'gel': 'Electric Guitar',
            'org': 'Organ',
            'pia': 'Piano',
            'sax': 'Saxophone',
            'tru': 'Trumpet',
            'vio': 'Violin',
            'voi': 'Voice'
        }
        
        print(f"\n{'='*60}")
        print(f"      {self.model_type}æ¨¡å‹ - Instrument Timeline Analysis Report")
        print(f"{'='*60}")
        
        # ç»Ÿè®¡æ´»è·ƒä¹å™¨
        active_instruments = []
        for instrument, data in timeline.items():
            if data['segments']:
                english_name = instrument_names.get(instrument, instrument)
                active_instruments.append((english_name, data['total_duration'], instrument))
        
        # æŒ‰æŒç»­æ—¶é—´æ’åº
        active_instruments.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ“Š Active Instruments Statistics ({len(active_instruments)} instruments):")
        print("-" * 60)
        
        for english_name, duration, original_name in active_instruments:
            percentage = (duration / audio_duration) * 100
            max_conf = timeline[original_name]['max_confidence']
            avg_conf = timeline[original_name]['average_confidence']
            segment_count = len(timeline[original_name]['segments'])
            
            print(f"ğŸµ {english_name:15s} | {duration:6.1f}s ({percentage:5.1f}%) | "
                f"Max Confidence: {max_conf:.3f} | Segments: {segment_count}")
        
        return active_instruments

def test_timeline_with_model(model_type, audio_path):
    """ä½¿ç”¨æŒ‡å®šæ¨¡å‹æµ‹è¯•æ—¶é—´çº¿åˆ†æ"""
    print(f"\n{'='*50}")
    print(f"ä½¿ç”¨ {model_type} æ¨¡å‹è¿›è¡Œæ—¶é—´çº¿åˆ†æ")
    print(f"{'='*50}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åŠ è½½æ¨¡å‹å’Œæ ‡ç­¾ç¼–ç å™¨
    model, label_encoder = load_model_with_fix(model_type, device)
    if model is None or label_encoder is None:
        return None
    
    print(f"ä¹å™¨ç±»åˆ«: {list(label_encoder.classes_)}")
    print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
    
    # åˆ›å»ºæ—¶é—´åºåˆ—åˆ†æå™¨
    analyzer = FixedTimeSeriesAnalyzer(model, label_encoder, device, model_type)
    
    # åˆ†æéŸ³é¢‘æ—¶é—´çº¿
    print("\nå¼€å§‹æ—¶é—´çº¿åˆ†æ...")
    timeline = analyzer.analyze_audio_timeline(
        audio_path, 
        window_size=1.5,      # å‡å°çª—å£å¤§å°ï¼Œæé«˜æ—¶é—´åˆ†è¾¨ç‡
        hop_size=0.3,         # å‡å°è·³è·ƒæ­¥é•¿ï¼Œå¢åŠ é‡‡æ ·å¯†åº¦
        threshold=0.15         # é™ä½é˜ˆå€¼ï¼Œæ•æ‰æ›´å¤šå¼±ä¿¡å·
    )
    
    # è·å–éŸ³é¢‘æ—¶é•¿ç”¨äºå¯è§†åŒ–
    y, sr = librosa.load(audio_path, sr=22050)
    audio_duration = len(y) / sr
    
    # ç”ŸæˆæŠ¥å‘Š
    active_instruments = analyzer.generate_report(timeline, audio_duration)
    
    # å¯è§†åŒ–æ—¶é—´çº¿
    output_path = os.path.join(Config.OUTPUT_DIR, f"instrument_timeline_{model_type}.png")
    analyzer.visualize_timeline(timeline, audio_duration, output_path)
    
    print(f"\nâœ… {model_type}æ¨¡å‹åˆ†æå®Œæˆ!")
    print(f"ğŸ“Š å‘ç°äº† {len(active_instruments)} ç§æ´»è·ƒä¹å™¨")
    print(f"ğŸ“ æ—¶é—´çº¿å›¾å·²ä¿å­˜: {output_path}")
    
    return active_instruments

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ¨¡å‹æ—¶é—´çº¿åˆ†æ')
    parser.add_argument('--model-type', type=str, default='all', 
                       choices=['basic', 'simplified', 'advanced', 'all'],
                       help='è¦æµ‹è¯•çš„æ¨¡å‹ç±»å‹')
    parser.add_argument('--audio-path', type=str, 
                       default=os.path.join(project_root, "music", "3.flac"),
                       help='æµ‹è¯•éŸ³é¢‘è·¯å¾„')
    
    args = parser.parse_args()
    
    print("=== AIéŸ³é¢‘åˆ†æä¸è‡ªåŠ¨æ‰’è°±ç³»ç»Ÿ - å¤šæ¨¡å‹æ—¶é—´çº¿åˆ†æ ===")
    
    # 1. åˆå§‹åŒ–é…ç½®
    Config.create_directories()
    
    # 2. æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.audio_path):
        print(f"é”™è¯¯: æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ - {args.audio_path}")
        print("è¯·ç¡®ä¿éŸ³é¢‘æ–‡ä»¶å­˜åœ¨")
        return
    
    if args.model_type == 'all':
        # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
        model_types = ['basic', 'simplified', 'advanced']
        all_results = {}
        
        for model_type in model_types:
            result = test_timeline_with_model(model_type, args.audio_path)
            if result:
                all_results[model_type] = result
        
        # è¾“å‡ºæ¯”è¾ƒç»“æœ
        print(f"\n{'='*60}")
        print("æ—¶é—´çº¿åˆ†ææ¨¡å‹æ¯”è¾ƒç»“æœ")
        print(f"{'='*60}")
        
        for model_type, instruments in all_results.items():
            print(f"\n{model_type:>10}æ¨¡å‹: æ£€æµ‹åˆ° {len(instruments)} ç§ä¹å™¨")
            for i, (name, duration, _) in enumerate(instruments[:3]):  # æ˜¾ç¤ºå‰3ç§
                print(f"            {i+1}. {name}: {duration:.1f}s")
    
    else:
        # æµ‹è¯•å•ä¸ªæ¨¡å‹
        test_timeline_with_model(args.model_type, args.audio_path)

if __name__ == "__main__":
    main()
```



